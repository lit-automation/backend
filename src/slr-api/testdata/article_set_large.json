[
    {
        "title": "‚Ä¶ of the slopes and intercepts from log-log correlations of measured gas-particle paritioning and vapor pressure‚ÄîI. theory and analysis of available data",
        "abstract": "Gas-particle partitioning is examined using a partitioning constant Kp = (F/TSP)/A, where F (ng m‚àí3) and A (ng m‚àí3) are the particulate-assiociated and and concentrations, respectively, and TSP is the total suspended particulate matter level (Œºg m‚àí3). At a given temperature and for a given sample of particulate matter, compound-dependent values of Kp tend to be correlated with the sub-cooled liquid vapor pressure (pL0, toor according to log Kp = mr log pL0+bm. Theory predicts that br values should be somewhat similar, and that mr values should be near ‚àí1. This is supported by field and laboratory work. However, there is still noticeable variability in reported mr and br values, even when obtained by the same researchers sampling in the same location. Three possible thermodynamic sources of variability include variability in the compound-to-compound differences in the thermodynamics of adsorption, event-to-event variability in the specific surface area of the aerosol and event-to-event variability in the ambient temperature. Non-thermodynamic sources of variability include sorption of gaseous analytes to the filters used in differentiating between F and A, the presence of non-exchangeable component in the measured F values, within-event adsorption/desorption kinetics, within-event changes in contaminant levels, and within-event changes in temperature. Each of these sources of variability operate in their own way to cause variability in mr and br. In general, one can expect there to be a correlation in the obseved mr and br of the form br = msmr+bs. For the study of Yamasaki et al. (1982, Envir. Sci. Technol. 16, 189‚Äì194), one obtains ms = 5.77 and bs = ‚àí2.18, with r2 = 0.91. In the presence of such a correlation, one can expect that all log (F/TSP)/A vs log pL0 plot will tend to intersect at the same (x,y) poitn given by (‚àíms, bs. Exisiting field and laboratory data show this tendency.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/096016869290039N",
        "year": "1992"
    },
    {
        "title": "‚ÄúSecure‚Äù Log-Linear and Logistic Regression Analysis of Distributed Databases",
        "abstract": "The machine learning community has focused on confidentiality problems associated with statistical analyses that ‚Äúintegrate‚Äù data stored in multiple, distributed databases where there are barriers to simply integrating the databases. This paper discusses various techniques which can be used to perform statistical analysis for categorical data, especially in the form of log-linear analysis and logistic regression over partitioned databases, while limiting confidentiality concerns. We show how ideas from the current literature that focus on ‚Äúsecure‚Äù summations and secure regression analysis can be adapted or generalized to the categorical data setting.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11930242_24",
        "year": "2006"
    },
    {
        "title": "A $$O(\\log n)$$ Distributed Algorithm to Construct Routing Structures for Pub/Sub Systems",
        "abstract": "The Industrial Internet of Things relies on event-driven services that run on wireless networks using low power protocols. The loose coupling and the inherent scalability make publish/subscribe systems an ideal candidate for such systems. This work introduces a new routing structure for such systems and an efficient distributed algorithm to build this structure. This routing structure supports all features of PSVR, a recently introduced publish/subscribe Middleware for IIoT applications. Provided the density of the underlying communication graph is sufficiently high, each node can be reached using at most ùëÇ(logùëõ)O(log‚Å°n) hops. The algorithm is analyzed for random graphs and we prove that w.h.p.¬† the data structure can be built in ùëÇ(logùëõ)O(log‚Å°n) synchronous rounds.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-03232-6_5",
        "year": "2018"
    },
    {
        "title": "A Binocular Stereo Algorithm for Log-Polar Foveated Systems",
        "abstract": "Foveation and stereopsis are important features on active vision systems. The former provides a wide field of view and high foveal resolution with low amounts of data, while the latter contributes to the acquisition of close range depth cues. The log-polar sampling has been proposed as an approximation to the foveated representation of the primate visual system. Although the huge amount of stereo algorithms proposed in the literature for conventional imaging geometries, very few are shown to work with foveated images sampled according to the log-polar transformation. In this paper we present a method to extract dense disparity maps in real-time from a pair of log-mapped images, with direct application to active vision systems.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-36181-2_13",
        "year": "2002"
    },
    {
        "title": "A breadth-first algorithm for mining frequent patterns from event logs",
        "abstract": "Today, event logs contain vast amounts of data that can easily overwhelm a human. Therefore, the mining of frequent patterns from event logs is an important system and network management task. This paper discusses the properties of event log data, analyses the suitability of popular mining algorithms for processing event log data, and proposes an efficient algorithm for mining frequent patterns from event logs.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-540-30179-0_27",
        "year": "2004"
    },
    {
        "title": "A Clustering Model Based on Matrix Approximation with Applications to Cluster System Log Files",
        "abstract": "In system management applications, to perform automated analysis of the historical data across multiple components when problems occur, we need to cluster the log messages with disparate formats to automatically infer the common set of semantic situations and obtain a brief description for each situation. In this paper, we propose a clustering model where the problem of clustering is formulated as matrix approximations and the clustering objective is minimizing the approximation error between the original data matrix and the reconstructed matrix based on the cluster structures. The model explicitly characterizes the data and feature memberships and thus enables the descriptions of each cluster. We present a two-side spectral relaxation optimization procedure for the clustering model. We also establish the connections between our clustering model with existing approaches. Experimental results show the effectiveness of the proposed approach.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/11564096_62",
        "year": "2005"
    },
    {
        "title": "A collaborative BCI system based on P300 signals as a new tool for life log indexing",
        "abstract": "Analyses on single-trial ElectroEncephaloGram (EEG) have been investigated toward realizing real-time Brain-Computer Interface (BCI). In general, the information transfer rates of current BCI systems with single-trial EEG are generally lower than those with averaging EEG. In recent years, the concept of collaborative EEG analyses has been proposed with the purpose of improving the BCI performances with collaborative single-trial EEGs of individuals. In this paper, a collaborative BCI system is developed based on P300 evoked potentials. The collaborative BCI system with Global Positioning System (GPS) can be operated with three subjects by using three wearable EEG recording devices and wireless communications. As the results, the P300 waveforms could be observed and it was found that the collaborative P300 analyses could improve the BCI performances than individual P300 analyses. The result of this study is to be applied to P300-based big data mining and life log indexing, in particular, in outdoor environment.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6974360/",
        "year": "2014"
    },
    {
        "title": "A collaborative intrusion detection system using log server and neural networks",
        "abstract": "With the explosive rapid expansion of computer use during the past few years, security has become a crucial issue for modern computer systems. Today, there are so many intrusion detection systems (IDS) on the Internet. A variety of intrusion detection techniques and tools exist in the computer security community. We can easily download, install and configure them to our needs. But there is a potential problem involved with intrusion detection systems that are installed locally on the machines to be monitored. If the system being monitored is compromised, it is quite likely that the intruder will alter the system logs and the intrusion logs while the intrusion remains undetected. In this project KIT-I, we adopt remote logging server (RLS) mechanism, which is used to backup the log files to the server. Taking into account security, we make use of the function of SSL of Java and certificate authority (CA) based on key management. Furthermore, neural networks are applied in our project to detect the intrusion activities.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/1626666/",
        "year": "2005"
    },
    {
        "title": "A Comparative Evaluation of Log-Based Process Performance Analysis Techniques",
        "abstract": "Process mining has gained traction over the past decade and an impressive body of research has resulted in the introduction of a variety of process mining approaches measuring process performance. Having this set of techniques available, organizations might find it difficult to identify which approach is best suited considering context, performance indicator, and data availability. In light of this challenge, this paper aims at introducing a framework for categorizing and selecting performance analysis approaches based on existing research. We start from a systematic literature review for identifying the existing works discussing how to measure process performance based on information retrieved from event logs. Then, the proposed framework is built starting from the information retrieved from these studies taking into consideration different aspects of performance analysis.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-93931-5_27",
        "year": "2018"
    },
    {
        "title": "A comparative study of log-only and in-place update based temporal object database systems",
        "abstract": "No abstract available.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=354858",
        "year": "2000"
    },
    {
        "title": "A Comparative Study on the Influence between Interaction and Performance in Postgraduate In-Class and Distance Learning Courses Based on the Analysis of LMS Logs",
        "abstract": "Learning Management Systems‚Äô use has been rapidly increasing during the last ten years, mainly in online distance learning courses but also in in-class courses. In parallel, technological advances have made it possible to track and store all the activity taking place in the LMS, and therefore to register the participation and interaction of students. This paper addresses two key questions: a) Is student interaction in the LMS an indicator of the final academic performance in a course?; and b) Is this interaction carried out in a different way in distance and in-class education, with different final results?. In order to answer this question, different types of interaction have been classified and extracted from Moodle LMS activity record logs during two years in one master program with online distance learning and in-class learning modalities at the Universidad Polit√©cnica de Madrid. The results show partial or no evidence of influence between interaction indicators and academic performance. The last section of this study covers a discussion of results and implications.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-13166-0_44",
        "year": "2010"
    },
    {
        "title": "A Comparative Study on Web Log Clustering Approaches",
        "abstract": "In the area of Web Usage Mining, the widely used data mining algorithm is the Clustering analysis. Hence we analyzed the most recent work in the field of Web Logs Clustering and made comparison based on their merits and drawbacks with respect to specific application areas. In this work we discuss three different approaches on Web Logs Clustering, namely, Temporal Cluster Migration, Fuzzy Clustering and PSO based Clustering and we conclude on the most efficient algorithm based on the results of experiments conducted with various web log files.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-29219-4_47",
        "year": "2012"
    },
    {
        "title": "A comparative transaction log analysis of two computing collections",
        "abstract": "Transaction logs are invaluable sources of _ne-grained infor- mation about users‚Äô search behavior. This paper compares the searching behavior of users across two WWW-accessible digital libraries: the New Zealand Digital Library‚Äôs Computer Science Technical Reports collection (CSTR), and the Karlsruhe Computer Science Bibliographies (CSBIB) collection. Since the two collections are designed to support the same type of users . researchers/students in computer science . a comparative log analysis is likely to uncover common searching preferences for that user group. The two collections di_er in their content, however; the CSTR indexes a full text collection, while the CSBIB is primarily a bibliographic database. Di_erences in searching behavior between the two systems may indicate the e_ect of di_ering search facilities and content type.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/3-540-45268-0_53",
        "year": "2000"
    },
    {
        "title": "A comparison of numerical mode-matching techniques for the analysis of well-logging tools",
        "abstract": "Two numerical mode-matching (NMM) techniques are compared for the analysis of logging-while-drilling (LWD) tools within borehole crossing in geophysical formations. The logging tools as well as the borehole and the surrounding geophysical formation are modeled as a stratified medium in cylindrical coordinates. One NMM technique perform a vertical expansion of the fields and a mode-matching is used to incorporate the radial stratifications. In the second NMM formulation, the fields are first expanded in radial direction and subsequently matched along each axial boundary. We present validation results and a preliminary analysis of the accuracy of these two NMM approaches to model typical well-logging tools used for hydrocarbon exploration.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7916280/",
        "year": "2017"
    },
    {
        "title": "A Constraint Optimisation Model for Analysis of Telecommunication Protocol Logs",
        "abstract": "Testing a telecommunication protocol often requires protocol log analysis. A protocol log is a sequence of messages with timestamps. Protocol log analysis involves checking that the content of messages and timestamps are correct with respect to the protocol specification. We model a protocol specification using constraint programming (MiniZinc), and we present an approach where a constraint solver is used to perform protocol log analysis. Our case study is the Public Warning System service, which is a part of the Long Term Evolution (LTE) 4G standard. We were able to analyse logs containing more than 3000 messages with more than 4000 errors.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-21215-9_9",
        "year": "2015"
    },
    {
        "title": "A data logging system using optical sensing techniques to determine foraging strategies of cottontail rabbits",
        "abstract": "Adaptive foraging decisions yield information on how animals view their environment. These decisions allow researchers to know how foragers rank habitats. We used a data logging system using infrared break-beam circuitry to evaluate these decisions. The data from this system revealed differences in behaviors between two species of cottontail rabbits in outdoor enclosures where obtaining food incurs predation costs (probability of mortality).",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/594947/",
        "year": "1997"
    },
    {
        "title": "A Data Mining Approach to Learning Probabilistic User Behavior Models from Database Access Log",
        "abstract": "The problem of user behavior modeling arises in many fields of computer science and software engineering. In this paper we investigate a data mining approach for learning probabilistic user behavior models from the database usage logs. We propose a procedure for translating database traces into representation suitable for applying data mining methods. However, most existing data mining methods rely on the order of actions and ignore time intervals between actions. To avoid this problem we propose novel method based on combination of decision tree classification algorithm and empirical time-dependent feature map, motivated by potential functions theory. The performance of the proposed method was experimentally evaluated on real-world data. The comparison with existing state-of-the-art data mining methods has confirmed outstanding performance of our method in predictive user behavior modeling and has demonstrated competitive results in anomaly detection.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-70621-2_26",
        "year": "2008"
    },
    {
        "title": "A Database Redo Log System Based on Virtual Memory Disk",
        "abstract": "Redo log of database must be written to permanence storage like disks. When database is heavily loaded, the crowded redo log writing queue will become a performance bottleneck. In this paper, the operation principle of database redo log is analyzed. It is found that if redo log is stored on virtual memory disk directly, the database will demonstrate better performance. In addition, the reliability of redo log system based on virtual memory disk is analyzed. At the end of the paper, a contrastive performance measurement result is given.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/11758532_86",
        "year": "2006"
    },
    {
        "title": "A debugging scheme for fine-grain threads on massively parallel processors with a small amount of log information ‚ÄîReplay and race detection‚Äî",
        "abstract": "Concurrent programs often exhibit nondeterministic behavior because execution order of concurrent events may involve some arbitrariness. Such indeterminacy makes it difficult to find the sources of program errors, We propose a debugging scheme for fine-grain parallel programs on massively parallel processors. It facilitates (1) replay of a specific execution with a small amount of log information, provided that the intra-node scheduling policy employed is deterministic and known, and (2) by using scalar timestamps, it also detects ‚Äúrace‚Äù conditions where message arrival order causes indeterminacy. We evaluate its performance through a prototype debugging system for a concurrent object-oriented language ABCL/f on a multicomputer AP1000+ with 32‚Äì1024 nodes.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/BFb0023057",
        "year": "1996"
    },
    {
        "title": "A Design of Web Log Integration Framework Using NoSQL",
        "abstract": "Webservice is a software technology as the representative method of information communication currently used to create a dynamic system environment that is configured to fulfill its users‚Äô needs. Therefore, analyzing log data that occurred at provision is being used as the significant basic data in webservice research. Thanks to development of Cloud computing technology, it has resulted in centralized points from which data is generated and data enlargement. A research is now implemented to create information from collecting, processing and converting flood of data and to obtain the new various items of information. Against this backdrop, it is justified that collection, storage and analysis of web log data in the existing conventional RDBMS system may be inadequate to process the enlarged log data. This research propose a framework which to integrate web log for storage using HBase, a repository of the Cloud computing- based NoSQL. In addition, data validation must be completed in the pre-process when collecting web log. The validated log is stored in the modeling structure in which takes features of web log into account. According to the results, introduction of NoSQL system is found to integrate the enlargement of log data in more efficient manner. By comparisons with the existing RDBMS in terms of data processing performance, it was proved that the NoSQL- based database had a superior performance.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-55032-4_44",
        "year": "2014"
    },
    {
        "title": "A Developer's Guide to Audit Logging",
        "abstract": "Modern enterprises centrally monitor their systems by collecting logs using audit reduction tools that can search, sort, and alert. The author describes how developers can support such monitoring by writing logging mechanisms that account for the strengths and weaknesses of audit reduction tools.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7118074/",
        "year": "2015"
    },
    {
        "title": "A Digital Diary Making System Based on User Life-Log",
        "abstract": "A common digital diary system is a software technology that proactively suggests contents of interest to users based on various kinds of context information. It provides benefits to users and meets their satisfaction. This research was motivated by our interest in understanding the criteria for measuring the success of a diary making system from users‚Äô point of view. Even though existing work has introduced a wide range of criteria such as users‚Äô biological information, picture, movie, etc. In this paper, we propose a digital diary making system which aimed at measuring the user emotion from their life-log data (daily-life photos). We can get those life-log data from user‚Äôs smartphone storage. The final product of digital diary includes feeling, time, and physical location information.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-51969-2_17",
        "year": "2016"
    },
    {
        "title": "A Distributed Data Mining System Framework for Mobile Internet Access Log Based on Hadoop",
        "abstract": "Because of the popularity of mobile phone and the development of mobile network, mobile data is growing explosively. Mobile data mining is more and more of attention. But single node-based data mining platform has been unable to store and analysis the massive data. According to cloud computing technology, we preset a distributed data mining framework based on Hadoop. Then, we present the implementation of this system framework and process mobile internet access log on the Hadoop cluster. Comparative tests will show that this distributed system framework is significantly efficient for processing huge scale dataset.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-48247-6_23",
        "year": "2015"
    },
    {
        "title": "A dynamic improved apriori algorithm and its experiments in web log mining",
        "abstract": "Apriori algorithm is an influential data mining algorithm which can mining the frequent sets of Boolean association rules. But its efficiency is not high and cannot do dynamic mining, for these reasons a new association rules algorithm which is suitable for dynamic database mining was proposed. Furthermore, the new algorithm is applied to the web log mining. Compared with original algorithm, experiments show that the performance of the new algorithm is improved to some extent.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6234032/",
        "year": "2012"
    },
    {
        "title": "A Family of Jacobians Suitable for Discrete Log Cryptosystems",
        "abstract": "We investigate the jacobians of the hyperelliptic curves v2+ v = u2g+1 over finite fields, and discuss which are likely to have ‚Äúalmost prime‚Äù order.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/0-387-34799-2_8",
        "year": "1990"
    },
    {
        "title": "A File Recommendation Method Based on Task Workflow Patterns Using File-Access Logs",
        "abstract": "In recent years, office workers spend much time and effort searching for the documents required for their jobs. To reduce these costs, we propose a new method for recommending files and operations on them. Existing technologies for recommendation, such as collaborative filtering, suffer from two problems. First, they can only work with documents that have been accessed in the past, so that they cannot recommend when only newly generated documents are inputted. Second, they cannot easily handle sequences involving similar or differently ordered elements because of the strict matching used in the access sequences. To solve these problems, such minor variations should be ignored. In our proposed method, we introduce the concepts of abstract files as groups of similar files used for a similar purpose, abstract tasks as groups of similar tasks, and frequent abstract workflows grouped from similar workflows, which are sequences of abstract tasks. In experiments using real file-access logs, we confirmed that our proposed method could extract workflow patterns with longer sequences and higher support-count values, which are more suitable as recommendations.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-40173-2_33",
        "year": "2013"
    },
    {
        "title": "A First Look at the CT Landscape: Certificate Transparency Logs in Practice",
        "abstract": "Many of today‚Äôs web-based services rely heavily on secure end-to-end connections. The ‚Äútrust‚Äù that these services require builds upon TLS/SSL. Unfortunately, TLS/SSL is highly vulnerable to compromised Certificate Authorities (CAs) and the certificates they generate. Certificate Transparency (CT) provides a way to monitor and audit certificates and certificate chains, to help improve the overall network security. Using an open standard, anybody can setup CT logs, monitors, and auditors. CT is already used by Google‚Äôs Chrome browser for validation of Extended Validation (EV) certificates, Mozilla is drafting their own CT policies to be enforced, and public CT logs have proven valuable in identifying rogue certificates. In this paper we present the first large-scale characterization of the CT landscape. Our characterization uses both active and passive measurements and highlights similarities and differences in public CT logs, their usage, and the certificates they include. We also provide insights into how the certificates in these logs relate to the certificates and keys observed in regular web traffic.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-54328-4_7",
        "year": "2017"
    },
    {
        "title": "A framework for application of tree-structured data mining to process log analysis",
        "abstract": "Many data mining and simulation based algorithms have been applied in the process mining field; nevertheless they mainly focus on the process discovery and conformance checking tasks. Even though the event logs are increasingly represented in semi-structured format using XML-based templates, commonly used XML mining techniques have not been explored. In this paper, we investigate the application of tree mining techniques and propose a general framework, within which a wider range of structure aware data mining techniques can be applied. Decision tree learning and frequent pattern mining are used as a case in point in the experiments on publicly available real dataset. The results indicate the promising properties of the proposed framework in adding to the available set of tools for process log analysis by enabling (i) direct data mining of tree-structured process logs (ii) extraction of informative knowledge patterns and (iii) frequent pattern mining at lower minimum support thresholds.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-32639-4_52",
        "year": "2012"
    },
    {
        "title": "A Framework Supporting the Analysis of Process Logs Stored in Either Relational or NoSQL DBMSs",
        "abstract": "The issue of devising efficient and effective solutions for supporting the analysis of process logs has recently received great attention from the research community, as effectively accomplishing any business process management task requires understanding the behavior of the processes. In this paper, we propose a new framework supporting the analysis of process logs, exhibiting two main features: a flexible data model (enabling an exhaustive representation of the facets of the business processes that are typically of interest for the analysis) and a graphical query language, providing a user-friendly tool for easily expressing both selection and aggregate queries over the business processes and the activities they are composed of. The framework can be easily and efficiently implemented by leveraging either ‚Äútraditional‚Äù relational DBMSs or ‚Äúinnovative‚Äù NoSQL DBMSs, such as Neo4J.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-25252-0_6",
        "year": "2015"
    },
    {
        "title": "A generic import framework for process event logs",
        "abstract": "The application of process mining techniques to real-life corporate environments has been of an ad-hoc nature so far, focused on proving the concept. One major reason for this rather slow adoption has been the complicated task of transforming real-life event log data to the MXML format used by advanced process mining tools, such as ProM. In this paper, the ProM Import Framework is presented, which has been designed to bridge this gap and to build a stable foundation for the extraction of event log data from any given PAIS implementation. Its flexible and extensible architecture, adherence to open standards, and open source availability make it a versatile contribution to the general BPI community.",
        "include": false,
        "url": "https://link.springer.com/10.1007%2F11837862_10",
        "year": "2006"
    },
    {
        "title": "A high-troughput radix-4 log-MAP decoder with low complexity LLR architecture",
        "abstract": "The throughput of turbo decoder is limited by the recursion architecture. In this paper, an improved radix-4 recursion architecture is presented. In order to decrease the critical path delay, a hybrid 4-inputs addition/subtraction structure is employed. Moreover, we present a modified trace-back architecture to decrease the hardware complexity of the log-likelihood ratios (LLR) architecture. The area of the proposed MAP decoder is 0.58 mm 2 on UMC 0.13 mum standard cell technology and under the worst case a maximum throughput of 600 Mbps can be achieved.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5158137/",
        "year": "2009"
    },
    {
        "title": "A Holistic Approach to Log Data Analysis in High-Performance Computing Systems: The Case of IBM Blue Gene/Q",
        "abstract": "The complexity and cost of managing high-performance computing infrastructures are on the rise. Automating management and repair through predictive models to minimize human interventions is an attempt to increase system availability and contain these costs. Building predictive models that are accurate enough to be useful in automatic management cannot be based on restricted log data from subsystems but requires a holistic approach to data analysis from disparate sources. Here we provide a detailed multi-scale characterization study based on four datasets reporting power consumption, temperature, workload, and hardware/software events for an IBM Blue Gene/Q installation. We show that the system runs a rich parallel workload, with low correlation among its components in terms of temperature and power, but higher correlation in terms of events. As expected, power and temperature correlate strongly, while events display negative correlations with load and power. Power and workload show moderate correlations, and only at the scale of components. The aim of the study is a systematic, integrated characterization of the computing infrastructure and discovery of correlation sources and levels to serve as basis for future predictive modeling efforts.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-27308-2_51",
        "year": "2015"
    },
    {
        "title": "A Lazy Log-Keeping Mechanism for Comprehensive Global Garbage Detection on Amadeus",
        "abstract": "Global Garbage Detection (GGD) in object-oriented distributed systems requires that each application process maintains some information in support of GGD. Maintaining this information is known as log-keeping. In this paper we describe a low-overhead, log-keeping mechanism which proceeds lazily and avoids race conditions while nevertheless maintaining enough information for comprehensive GGD to take place.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4471-1009-5_12",
        "year": "1996"
    },
    {
        "title": "A life log system that recognizes the objects in a pocket",
        "abstract": "A novel approach has been developed for recognizing objects in pockets and for recording the events related to the objects. Information on putting an object into or taking it out of a pocket is closely related to user contexts. For example, when a house key is taken out from a pocket, the owner of the key is likely just getting home. We implemented a objects-in-pocket recognition device, which has a pair of infrared sensors arranged in a matrix, and life log software to obtain the time stamp of events happening. We evaluated whether or not the system could deal with one of five objects (a smartphone, ticket, hand, key, and lip balm) using template matching. When one registered object (the smartphone, ticket, or key) was put in the pocket, our system recognized the object correctly 91% of the time on average. We also evaluated our system in one action scenario. With our system's time stamps, user could easily remember what he took on that day and when he used the items.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2735788",
        "year": "2015"
    },
    {
        "title": "A Linguistic Approach Towards Intrusion Detection in Actual Proxy Logs",
        "abstract": "Modern malware imitates benign http traffic to evade detection. To detect unseen malicious traffic, a linguistic-based detection method for proxy logs has been proposed. This method uses Paragraph Vector to extract features automatically. To generate discriminative feature representation, a balanced corpus is required. In actual proxy logs, benign traffic is dominant, and occupies malicious feature representation. Therefore, the previous method does not perform accuracy in practical environment.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-01950-1_42",
        "year": "2018"
    },
    {
        "title": "A Log Analysis Study of 10 Years of eBook Consumption in Academic Library Collections",
        "abstract": "Even though libraries have been offering eBooks for more than a decade, very little is known about eBook access and consumption in academic library collections. This paper addresses this gap with a log analysis study of eBook access at the library of the University of Waikato. This in-depth analysis covers a period spanning 10 years of eBook use at this university. We draw conclusions about the use of eBooks at this institution and compare the results with other published studies of eBook usage at tertiary institutes.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-12823-8_18",
        "year": "2014"
    },
    {
        "title": "A Log Analyzer Agent for Intrusion Detection in a Multi-Agent System",
        "abstract": "In this work, the design and implementation of a log analyzer agent is described. This agent is conceived to act as a part of a multi-agent Intrusion Detection System. The agent analyzes log files of services, applications or operating systems contrasting every log line with a set of security rules defined by experts. These rules can be created using a new easy to use XML-based format founded on an object-oriented model. Whenever a security match is found, the agent sends a security report to the next level of the multi-agent system using the IDMEF (Intrusion Detection Message Exchange Format) and the IDXP (Intrusion Detection Exchange Protocol).",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-15387-7_21",
        "year": "2010"
    },
    {
        "title": "A Log Regression Seasonality Based Approach for Time Series Decomposition Prediction in System Resources",
        "abstract": "It has been challenging to predict data in terms of monitoring information technology (IT) resources. In order to obtain the quality and performance of products, changes can be detected and monitored setting up a fixed threshold value based on statistics and operation experiences. Monitoring data by a fixed threshold value may not work properly during none busy hours in exceptional situations whereas a usage change during busy hours can be detected. It is because it cannot reflect the trend of resource usage seasonality as a function of time. The technique based on Time Series Decomposition (TSD) can provide the one with appropriate methodology so that problems can be recognized and diagnosed and the correction can be made ahead of time by detecting a subtle status change of devices in massive IT resources. In this paper, we propose three approaches to predict data such as Intelligent Threshold, Abnormal Pattern Detection, time prediction of reaching target value; the appropriate trend detection of Time Series, optimal seasonality detection and technique using Log Regression Seasonality. The experimental data collected here exhibit that it can reflect the change over time to the prediction data improving its accuracy compared to existing TSD technique.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-981-10-0281-6_118",
        "year": "2015"
    },
    {
        "title": "A Log-Based Mining System for Network Node Correlation",
        "abstract": "In the field of network security, people become aware of the importance of study on the connectivity between network nodes. Based on analyzing the connectivity, this paper introduces a conception of network node correlation (NNC) and designs a novel log-based NNC mining system which adopts a typical distributed architecture based on agent. By means of bayesian network, this system can accurately and effectively mine high-level NNCs on application layer. The mined results can provide useful information for some security fields such as network risk assessment, vulnerability and intrusion detection, and virus propagation.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/1-84628-352-3_17",
        "year": "2006"
    },
    {
        "title": "A Logging Technique Based on Transaction Types for Real-Time Databases",
        "abstract": "Real-time Main Memory DataBase systems (RTMMDB) have been recently studied to meet stringent timing and reliability constraints that exist in application areas such as telecommunications, automated control of medical patient monitoring, and computer integrated manufacturing. In these applications, transactions may be aborted if they missed their deadlines, and temporal data may become invalid before being used. Due to the volatility of main memory, the database may be lost upon a system failure caused by software errors, hardware errors, or power outage. Logging is one of the activities an RTMMDB must perform during normal operation to guarantee its recoverability from a system or transaction failure.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-1-4615-6069-2_20",
        "year": "1997"
    },
    {
        "title": "Characterizing logging practices in open-source software",
        "abstract": "Software logging is a conventional programming practice. As its efficacy is often important for users and developers to understand what have happened in production run, yet software logging is often done in an arbitrarily manner. So far, there have been little study for understanding logging practices in real world software. This paper makes the first attempt (to the best of our knowledge) to provide quantitative characteristic study of the current log messages within four pieces of large open-source software. First, we quantitatively show that software logging is pervasive. By examining developers‚Äô own modifications to logging code in revision history, we find that they often do not make the log messages right in their first attempts, and thus need to spend significant amount of efforts to modify the log messages as after-thoughts. Our study further provides several interesting findings on where developers spend most of their efforts in modifying the log messages, which can give insights for programmers, tool developers, and language and compiler designers to improve the current logging practice. To demonstrate the benefit of our study, we built a simple checker based on one of our findings and effectively detected 138 new problematic logging code from studied software (24 of them are already confirmed and fixed by developers).",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2337236",
        "year": "2012"
    },
    {
        "title": "Characterizing sequences of user actions for access logs analysis",
        "abstract": "The paper presents new measures for characterizing sequences of user actions. They are aimed at categorizing user behavior on intranet sites. Their relevance is evaluated using different encoding and clustering algorithms. New criteria are introduced for comparing clustering methods.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/3-540-44566-8_29",
        "year": "2001"
    },
    {
        "title": "Characterizing the natural language descriptions in software logging statements",
        "abstract": "Logging is a common programming practice of great importance in modern software development, because software logs have been widely used in various software maintenance tasks. To provide high-quality logs, developers need to design the description text in logging statements carefully. Inappropriate descriptions will slow down or even mislead the maintenance process, such as postmortem analysis. However, there is currently a lack of rigorous guide and specifications on developer logging behaviors, which makes the construction of description text in logging statements a challenging problem. To fill this significant gap, in this paper, we systematically study what developers log, with focus on the usage of natural language descriptions in logging statements. We obtain 6 valuable findings by conducting source code analysis on 10 Java projects and 7 C# projects, which contain 28,532,975 LOC and 115,159 logging statements in total. Furthermore, our study demonstrates the potential of automated description text generation for logging statements by obtaining up to 49.04 BLEU-4 score and 62.1 ROUGE-L score using a simple information retrieval method. To facilitate future research in this field, the datasets have been publicly released.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3238193",
        "year": "2018"
    },
    {
        "title": "CHMM for discovering intentional process model from event logs by considering sequence of activities",
        "abstract": "An intentional process model is known to analyze processes deeply and provide recommendations for the upcoming processes. Nevertheless, the discovery of intentions is a difficult task because the intentions are not recorded in the event log, but they encourage the executable activities in the event log. Map Miner is the latest algorithm to depict the intentional process model. A disadvantage of this algorithm is the inability to determine strategies that contain same activities with the different sequence with other strategies. This disadvantage leads failure on the intentional process model. This research proposes an algorithm for discovering an intentional process model by considering the sequence of activities and CHMM (Coupled Hidden Markov Model). The probabilities and states of CHMM are utilized for the formation of the intentional process model. The experiment shows that the proposed algorithm with considering the sequence of activities gets an appropriate intentional process model. It also demonstrates that an obtained intentional process model using proposed algorithm gets the better validity than an intentional process model using Map Miner Method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8239194/",
        "year": "2017"
    },
    {
        "title": "ChoiceLog: Life Log System Based on Choices for Supporting Decision-Making",
        "abstract": "When we are faced with choices, we often make decisions based on past experiences where we faced similar choices. If we could refer these choices, we would be able to make better decisions. In this paper, we propose a new life log system called ‚ÄúChoiceLog‚Äù. ChoiceLog can record the options that user choose and the options that they reject. Further, it can display recorded data via searches and notifications in order to support decision-making. We implemented ChoiceLog as an iOS application and conducted two evaluations to estimate its usefulness. From the results of the evaluations, we found that ChoiceLog could support decision-making processes in cases where there were a large number of choice selection logs. Moreover, for long-term use, we are planning to make it possible to filter notification from ChoiceLog based on date, time, and preset or user-customizable tags.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07863-2_18",
        "year": "2014"
    },
    {
        "title": "Classifying Traces of Event Logs on the Basis of Security Risks",
        "abstract": "In the context of security risk analysis, we address the problem of classifying log traces describing business process executions. Specifically, on the basis of some (possibly incomplete) knowledge of the process structures and of the patterns representing unsecure behaviors, we classify each trace as instance of some process and/or as potential security breach. This classification is addressed in the challenging setting where each event has not a unique interpretation in terms of the activity that has generated it, but it can correspond to more activities. In our framework, the event/activity mapping is encoded probabilistically, and the models describing the processes and the security breaches are expressed in terms of precedence/causality rules over the activities. Each trace is classified on the basis of the conformance of its possible interpretations, generated by a Monte Carlo mechanism, to the security-breach models and/or the process models. The framework has been experimentally proved to be efficient and effective.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39315-5_8",
        "year": "2016"
    },
    {
        "title": "CLHQS: Hierarchical Query Suggestion by Mining Clickthrough Log",
        "abstract": "Most commercial search engines provide query suggestion in a ranked list for more effective search. However, a ranked list may not be an ideal way to satisfy users‚Äô various information demands. In this paper, we propose a novel query suggestion method named CLHQS (Clickthrough-Log based Hierarchical Query Suggestion). It organizes the suggested queries into a well-structured hierarchy. Users can easily generalize, extend or specialize their queries within the hierarchy. The query hierarchy is mined from the clickthrough log data in the following way. First, we generate a candidate set through the query-url graph analysis. Second, the pair-wise relationships are inspected for each pair of candidate queries. Finally, we construct the suggested query hierarchy using these relationships. Experiments on a real-world clickthrough log validate the effectiveness of our proposed CLHQS approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-01307-2_78",
        "year": "2009"
    },
    {
        "title": "Client Churn Prediction with Call Log Analysis",
        "abstract": "Client churn prediction is a classic business problem of retaining customers. Recently, machine learning algorithms have been applied to predict client churn and have shown promising performance comparing to traditional methods. Despite of its success, existing machine learning approach mainly focus on structured data such as demographic and transactional data, while unstructured data, such as emails and phone calls, have been largely overlooked. In this work, we propose to improve existing churn prediction models by analysing customer characteristics and behaviours from unstructured data, particularly, audio calls. To be specific, we developed a text mining model combined with gradient boosting tree to predict client churn. We collected and conducted extensive experiments on 900 thousand audio calls from 200 thousand customers, and experimental results show that our approach can significantly improve the previous model by exploiting the additional unstructured data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-91458-9_47",
        "year": "2018"
    },
    {
        "title": "Clinical accuracy of the respiratory tumor tracking system of the cyberknife: assessment by analysis of log files",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0360301609000212",
        "year": "2009"
    },
    {
        "title": "Combined log system",
        "abstract": "Busy Internet archives generate large logs for each access method being used. These raw log files can be difficult to process and to search. This paper describes a system for reading these growing logs, a combined log file format into which they are re-written and a system that automates this building and integration for multiple access methods. Automated summarizing of the information is also provided giving statistics on accesses by user, site, path-name and date/time amongst others.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/016975529500013W",
        "year": "1995"
    },
    {
        "title": "Combined mining of Web server logs and web contents for classifying user navigation patterns and predicting users' future requests",
        "abstract": "We present a study of the automatic classification of web user navigation patterns and propose a novel approach to classifying user navigation patterns and predicting users‚Äô future requests. The approach is based on the combined mining of Web server logs and the contents of the retrieved web pages. The textual content of web pages is captured through extraction of character N-grams, which are combined with Web server log files to derive user navigation profiles. The approach is implemented as an experimental system, and its performance is evaluated based on two tasks: classification and prediction. The system achieves the classification accuracy of nearly 70% and the prediction accuracy of about 65%, which is about 20% higher than the classification accuracy by mining Web server logs alone. This approach may be used to facilitate better web personalization and website organization.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0169023X06000954",
        "year": "2007"
    },
    {
        "title": "Combining Model- and Example-Driven Classification to Detect Security Breaches in Activity-Unaware Logs",
        "abstract": "Current approaches to the security-oriented classification of process log traces can be split into two categories: (i) example-driven methods, that induce a classifier from annotated example traces; (ii) model-driven methods, based on checking the conformance of each test trace to security-breach models defined by experts. These categories are orthogonal and use separate information sources (i.e. annotated traces and a-priori breach models). However, as these sources often coexist in real applications, both kinds of methods could be exploited synergistically. Unfortunately, when the log traces consist of (low-level) events with no reference to the activities of the breach models, combining (i) and (ii) is not straightforward. In this setting, to complement the partial views of insecure process-execution patterns that an example-driven and a model-driven methods capture separately, we devise an abstract classification framework where the predictions provided by these methods separately are combined, according to a meta-classification scheme, into an overall one that benefits from all the background information available. The reasonability of this solution is backed by experiments performed on a case study, showing that the accuracy of the example-driven (resp., model-driven) classifier decreases appreciably when the given example data (resp., breach models) do not describe exhaustively insecure process behaviors.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-02671-4_10",
        "year": "2018"
    },
    {
        "title": "Communication Network Anomaly Detection Based on Log File Analysis",
        "abstract": "Communication network today are becoming larger and increasingly complex. Failure in communication systems will cause loss of critical data and even economic losses. Therefore, detecting failures and diagnosing their root-cause in a timely manner is essential. Fast and accurate detection of these failures can accelerate problem determination, and thereby improve system reliability. Today log files have been paid attention on system and network failure detection, but it is still a challenging task to build an efficient model to detect anomaly from log files. To this effect, we propose a novel approach, which aims to detect frequent patterns from log files to build the normal profile, and then to identify the anomalous behaviour in log files. The experimental results demonstrate that our approach is an efficient way for anomaly detection with high accuracy and few false positives.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-11740-9_23",
        "year": "2014"
    },
    {
        "title": "Compact_Eddy: A compact, low consumption remotely controlled eddy covariance logging system",
        "abstract": "Compact_Eddy is a handheld computer-based logging system developed for eddy covariance installations. It is flexible to support different types of sonic anemometers, it has storing capability on internal memory, and data processing capability. The software is written in the Visual Basic .NET platform, entirely based on standard Windows operating systems thus allowing easy installation and configuration for a wide range of handheld PCs. The software computes half hourly statistics and fluxes with a simplified procedure, stores binary data with efficient memory usage and can be configured to send periodically, to a remote control station, system status information and basic computations using a GSM modem. The use of a handheld computer resulted in an average real conditions power consumption of 2.9¬†W, very low if compared with other standard logging devices, allowing important savings especially when systems are powered with solar panels in remote areas. Compact_Eddy has been installed and tested in three locations, with very different environmental conditions ranging from eastern European mountain forests to African semi-desertic areas. The field results demonstrated that the system operated reliably, resulting to be a good low-cost alternative to laptop computers and dataloggers.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0168169908001713",
        "year": "2008"
    },
    {
        "title": "Comparative analysis of methods for the log boundaries isolation",
        "abstract": "The scrutiny of boundaries isolation methods is presented in this paper. The newly developed algorithms, based on regression analysis and integral projection are compared with Hough transform in order to analyze their effectiveness for the specific problem of moving logs control. The comparative analysis of the methods was carried out on the database of images obtained from video sequence of real industrial process by the criteria of accuracy and operation speed. Results of the test show that the line-by-line scanning method with posterior LOWESS regression analysis has the best accuracy. However, the best appropriate for the implementation in the real-time control systems based on machine vision technology is consecutive line selection method due to its reasonable accuracy and impressive performance.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7347792/",
        "year": "2015"
    },
    {
        "title": "Comparative Study of Pattern Mining Techniques for Network Management System Logs for Convergent Network",
        "abstract": "The concept of Pattern Mining has obtained significant focus in Telecommunications Network Management Systems (NMS). A large volume of work has been dedicated to this field and valuable progress has been observed. Both sequential and structured pattern mining techniques were applied to NMS. In particular NMS logs (Performance and Alarm) pose several interesting issues for pattern mining, and it can help in various NMS activities such as alarm correlation, alarm associations, self-healing or pro-active fault management. In this paper, we present an overview of the different pattern mining techniques used in NMSs, compare them and present the most beneficial ones to NMS for Radio over Fiber (RoF) like convergent networks.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-27872-3_16",
        "year": "2012"
    },
    {
        "title": "Comparing Web Logs: Sensitivity Analysis and Two Types of Cross-Analysis",
        "abstract": "Different Web log studies calculate the same metrics using different search engines logs sampled during different observation periods and processed under different values of two controllable variables peculiar to the Web log analysis: a client discriminator used to exclude clients who are agents and a temporal cut-off used to segment logged client transactions into temporal sessions. How much are the results dependent on these variables? We analyze the sensitivity of the results to two controllable variables. The sensitivity analysis shows significant varying of the metrics values depending on these variables. In particular, the metrics varies up to 30-50% on the commonly assigned values. So the differences caused by controllable variables are of the same order of magnitude as the differences between the metrics reported in different studies. Thus, the direct comparison of the reported results is an unreliable approach leading to artifactual conclusions. To overcome the method-dependency of the direct comparison of the reported results we introduce and use a cross-analysis technique of the direct comparison of logs. Besides, we propose an alternative easy-accessible comparison of the reported metrics, which corrects the reported values accordingly to the controllable variables used in the studies.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11880592_39",
        "year": "2006"
    },
    {
        "title": "Comparison of dietary information logging methods for obesity prevention system",
        "abstract": "The worldwide prevalence of obesity is considered as a serious issue because obesity is one of the main causes of diabetes, heart disease, and cancer. Obesity is associated with various habits of our daily life, e.g. short sleep duration, high alcohol intake, and nonparticipation in physical exercise. Therefore, changing these habits in order to reduce body weight is recognized as an effective solution. There are some websites in effort to monitor obesity, which verify the usefulness of web-based system for health-care support. We, the authors, have been developing an obesity prevention system that helps a user to change his lifestyle with a website and health care devices. The system detects whether the user has the habits that generally considered as risk factors of obesity. After the detection, this system analyzes personal factors of weight gain, and informs it to the user of this system for changing his habits effectively. Since calorie intake is an important factor of weight gain, we have developed 3 different types of dietary information logging methods. Moreover, we evaluated and compared these methods by the experiments that we conducted. According to the results, we found that simple method with a piece of data is enough to detect calorie intake, instead of detailed method with a lot of data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6427364/",
        "year": "2012"
    },
    {
        "title": "Computer Log Anomaly Detection Using Frequent Episodes",
        "abstract": "In this paper, we propose a set of algorithms to automate the detection of anomalous frequent episodes. The algorithms make use of the hierarchy and frequency of episodes present in an examined sequence of log data and in a history preceding it. The algorithms identify changes in a set of frequent episodes and their frequencies. We evaluate the algorithms and describe tests made using live computer system log data.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4419-0221-4_49",
        "year": "2009"
    },
    {
        "title": "Computing Geographical Serving Area Based on Search Logs and Website Categorization",
        "abstract": "Knowing the geographical serving area of web resources is very important for many web applications. Here serving area stands for the geographical distribution of online users who are interested in a given web site. In this paper, we proposed a set of novel methods to detect the serving area of web resources by analyzing search engine logs. We use the search logs to detect serving area in two ways. First, we extracted the user IP locations to generate the geographical distribution of users who had the same interests in a web site. Second, query terms input by users were considered as the user knowledge about a web site. To increase the confidence and to cover new sites for use in real-time applications, we also proposed a categorization system for local web sites. A novel method for detecting the serving area was proposed based on categorizing the web content. For each category, a radius was assigned according to previous logs. In our experiments, we evaluated all these three algorithms. From the results, we found that the approach based on query terms was superior to that based on IP locations, since search queries for local sites tended to include location words while the IP locations were sometimes erroneous. The approach based on categorization was efficient for sites of known categories and were useful for small sites without sufficient number of query logs.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-74469-6_79",
        "year": "2007"
    },
    {
        "title": "Configuring Logstash for Services and System Logs",
        "abstract": "In the previous chapter, you learned how to configure Filebeat to send events to a centralized log server. In Chapter 3, you learned how to get events from the Apache access_log to your ELK Stack setup.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4842-1694-1_5",
        "year": "2016"
    },
    {
        "title": "Conformance Testing: Measuring the Fit and Appropriateness of Event Logs and Process Models",
        "abstract": "Most information systems log events (e.g., transaction logs, audit trails) to audit and monitor the processes they support. At the same time, many of these processes have been explicitly modeled. For example, SAP R/3 logs events in transaction logs and there are EPCs (Event-driven Process Chains) describing the so-called reference models. These reference models describe how the system should be used. The coexistence of event logs and process models raises an interesting question: ‚ÄúDoes the event log conform to the process model and vice versa?‚Äù. This paper demonstrates that there is not a simple answer to this question. To tackle the problem, we distinguish two dimensions of conformance: fitness (the event log may be the result of the process modeled) and appropriateness (the model is a likely candidate from a structural and behavioral point of view). Different metrics have been defined and a Conformance Checker has been implemented within the ProM Framework.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11678564_15",
        "year": "2006"
    },
    {
        "title": "Coniunge et Impera: Multiple-Graph Mining for Query-Log Analysis",
        "abstract": "Query logs of search engines record a huge amount of data about the actions of the users who search for information on the Web. Hence, they contain a wealth of valuable knowledge about the users‚Äô interests and preferences, as well as the implicit feedback that Web searchers provide when they click on the results obtained for their queries.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-15880-3_17",
        "year": "2010"
    },
    {
        "title": "Constant log-MAP decoding algorithm for duo-binary turbo codes",
        "abstract": "The constant log-MAP decoding algorithm suitable for duo-binary turbo codes. Motivated by an existing algorithm approach, an efficient algorithm is presented, which is superior in terms of frame/bit error rate performance and has approximately the same computational complexity. Compared with log-MAP decoding, the proposed algorithm has negligible performance degradation, exactly as for binary turbo codes.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1642490/",
        "year": "2006"
    },
    {
        "title": "Constrained Log-Likelihood-Based Semi-supervised Linear Discriminant Analysis",
        "abstract": "A novel approach to semi-supervised learning for classical Fisher linear discriminant analysis is presented. It formulates the problem in terms of a constrained log-likelihood approach, where the semi-supervision comes in through the constraints. These constraints encode that the parameters in linear discriminant analysis fulfill particular relations involving label-dependent and label-independent quantities. In this way, the latter type of parameters, which can be estimated based on unlabeled data, impose constraints on the former. The former parameters are the class-conditional means and the average within-class covariance matrix, which are the parameters of interest in linear discriminant analysis. The constraints lead to a reduction in variability of the label-dependent estimates, resulting in a potential improvement of the semi-supervised linear discriminant over that of its regular supervised counterpart. We state upfront that some of the key insights in this contribution have been published previously in a workshop paper by the first author. The major contribution in this work is the basic observation that a semi-supervised linear discriminant analysis can be formulated in terms of a principled log-likelihood approach, where the previous solution employed an ad hoc procedure. With the current contribution, we move yet another step closer to a proper formulation of a semi-supervised version of this classical technique.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-34166-3_36",
        "year": "2012"
    },
    {
        "title": "Context awareness of social group by topic mining on visiting logs of mobile users in two dimensions",
        "abstract": "Ubiquitous computing technologies have been developed fast and various decision support systems were proposed. Consecutively, in recently, people are working on an implicit service agent of mobile phone to make people to be provided useful services without paying attentions. However, when people want to buy some products or to go somewhere, people are still going to use Internet to get preference information of group of people. They should put queries and click one or more documents. In this sense, how the implicit services could be perfectly available by only mining on the contextual data obtainable from smart phones? In this paper, we propose a model providing group preference information implicitly through topic-mining on the data obtainable from smart phones. We would introduce developed context awareness model in smart phones and apply a topic model which automatically learns a hierarchy of topics to our server. We would also show our simple experimentation to ensure the effectiveness of topic-mining and of provided implicit group preference information.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5753444/",
        "year": "2011"
    },
    {
        "title": "Contextual Anomaly Detection Using Log-Linear Tensor Factorization",
        "abstract": "This paper presents a novel approach for the detection of contextual anomalies. This approach, based on log-linear tensor factorization, considers a stream of discrete events, each representing the co-occurence of contextual elements, and detects events with low-probability. A parametric model is used to learn the joint probability of contextual elements, in which the parameters are the factors of the event tensor. An efficient method, based on Nesterov‚Äôs accelerated gradient ascent, is proposed to learn these parameters. The proposed approach is evaluated on the low-rank approximation of tensors, the prediction of future of events and the detection of events representing abnormal behaviors. Results show our method to outperform state of the art approaches for these problems.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-18032-8_13",
        "year": "2015"
    },
    {
        "title": "Controlling open source intermediaries-a web log mining approach",
        "abstract": "Open source software (OSS) has become a focal research issue in computer science. One interesting phenomenon in the OSS market is the evolution of dedicated intermediaries, which provide OSS-related services to the OSS community. Prevalently, OSS intermediaries fail to fulfil their financial and nonfinancial goals, such that many of them have vanished from the OSS market or significantly changed their business models. A major reason for this development is the lack of appropriate instruments which are able to dismantle coordination problems and failures inherent to the OSS development model. This paper proposes a controlling instrument for OSS intermediaries based on a behaviouristic approach. Since OSS intermediaries typically provide Internet based services, it is possible to observe actor behaviour by use of technical data acquisition instruments like server logging. Therefore, it seems reasonable to examine the concept of Web log mining, which provides a framework to analyse server based log files. Finally, we apply this concept to the research field of OSS intermediaries and show empirical results. These results relate to the OSS intermediary CampusSource, who provides e-learning software and complementary services",
        "include": false,
        "url": "https://ieeexplore.ieee.org/abstract/document/1372408/",
        "year": "2004"
    },
    {
        "title": "Converting Unstructured System Logs into Structured Event List for Anomaly Detection",
        "abstract": "System logs provide invaluable resources for understanding system behavior and detecting anomalies on high performance computing (HPC) systems. As HPC systems continue to grow in both scale and complexity, the sheer volume of system logs and the complex interaction among system components make the traditional manual problem diagnosis and even automated line-by-line log analysis infeasible or ineffective. In this paper, we present a System Log Event Block Detection (SLEBD) framework that identifies groups of log messages that follow certain sequence but with variations, and explore these event blocks for event-based system behavior analysis and anomaly detection. Compared with the existing approaches that analyze system logs line by line, SLEBD is capable of characterizing system behavior and identifying intricate anomalies at a higher (i.e., event) level. We evaluate the performance of SLEBD by using syslogs collected from production supercomputers. Experimental results show that our framework and mechanisms can process streaming log messages, efficiently extract event blocks and effectively detect anomalies, which enables system administrators and monitoring tools to understand and process system events in real time. Additionally, we use the identified event blocks and explore deep learning algorithms to model and classify event sequences.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3230855",
        "year": "2018"
    },
    {
        "title": "Correlation-based document clustering using web logs",
        "abstract": "A problem facing information retrieval on the web is how to effectively cluster large amounts of web documents. One approach is to cluster the documents based on information provided only by users' usage logs and not by the content of the documents. A major advantage of this approach is that the relevancy information is objectively reflected by the usage logs; frequent simultaneous visits to two seemingly unrelated documents should indicate that they are in fact closely related. In this paper, we present a recursive density based clustering algorithm that can adaptively change its parameters intelligently. Our clustering algorithm RDBC (Recursive Density Based Clustering algorithm) is based on DBSCAN, a density based algorithm that has been proven in its ability in processing very large datasets. The fact that DBSCAN does not require the pre-determination of the number of clusters and is linear in time complexity makes it particularly attractive in web page clustering. It can be shown that RDBC require the same time complexity as that of the DBSCAN algorithm. In addition, we prove both analytically and experimentally that our method yields clustering results that are superior to that of DBSCAN.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/926536/",
        "year": "2001"
    },
    {
        "title": "Covariant support region and detection algorithm based on LoG corners",
        "abstract": "Detection of local feature covariant region is a new technology of image contents and image semantic representations, and it has become an important foundation of the image recognition, learning and understanding. First, a Laplace of Gaussian corner detection method is proposed based on edge contour curves, in the meantime, a new local feature descriptor, named covariant support region, is introduced. Then, a detection algorithm of covariant support region is framed, which is covariant for rotation and scale transformation. Comparing with previous studies, the computational complexity of proposed algorithm is significantly reduced by this method. The experiments data indicate that the method proposed in this paper has good performance on higher accuracy, higher repeatability, and lower complexity.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5420847/",
        "year": "2009"
    },
    {
        "title": "Creating meaningful data from web logs for improving the impressiveness of a website by using path analysis method",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0957417408005952",
        "year": "2009"
    },
    {
        "title": "Cross-System Validation of Engagement Prediction from Log Files",
        "abstract": "Engagement is an important aspect of effective learning. Time spent using an e-Learning system is not quality time if the learner is not engaged. Tracking the student disengagement would give the possibility to intervene for motivating the learner at appropriate time. In previous research we showed the possibility to predict engagement from log files using a web-based e-Learning system. In this paper we present the results obtained from another web-based system and compare them to the previous ones. The similarity of results across systems demonstrates that our approach is system-independent and that engagement can be elicited from basic information logged by most e-Learning systems: number of pages read, time spent reading pages, number of tests/ quizzes and time spent on test/ quizzes.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-75195-3_2",
        "year": "2007"
    },
    {
        "title": "Cut-and-pick transactions for proxy log mining",
        "abstract": "Web logs collected by proxy servers, referred to as proxy logs or proxy traces, contain information about Web document accesses by many users against many Web sites. This ‚Äúmany-to-many‚Äù characteristic poses a challenge to Web log mining techniques due to the difficulty in identifying individual access transactions. This is because in a proxy log, user transactions are not clearly bounded and are sometimes interleaved with each other as well as with noise. Most previous work has used simplistic measures such as a fixed time interval as a determination method for the transaction boundaries, and has not addressed the problem of interleaving and noisy transactions. In this paper, we show that this simplistic view can lead to poor performance in building models to predict future access patterns. We present a more advanced cut-and-pick method for determining the access transactions from proxy logs, by deciding on more reasonable transaction boundaries and by removing noisy accesses. Our method takes advantage of the user behavior that in most transactions, the same user typically visits multiple, related Web sites that form clusters. These clusters can be discovered by our algorithm based on the connectivity among Web sites. By using real-world proxy logs, we experimentally show that this cut-and-pick method can produce more accurate transactions that result in Web-access prediction models with higher accuracy.",
        "include": true,
        "url": "https://link.springer.com/10.1007/3-540-45876-X_8",
        "year": "2002"
    },
    {
        "title": "Data Improvement to Enable Process Mining on Integrated Non-log Data Sources",
        "abstract": "Process models derived using Process Mining (PM) are often very complex due to Data Quality Issues (DQIs). Some of those DQIs arise from integration of different data sources or the transformation of non-process oriented data, hence are structural and can be abstracted from the domain. Activity Sequencing and Activity Hierarchy are two concepts for improving certain DQIs in order to improve PM outcomes. The approaches are evaluated by showing the improvement of derived process models using a simplified real world scenario with simulated data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-53856-8_62",
        "year": "2013"
    },
    {
        "title": "Data Mining Algorithm of Browsing Pattern Based on Web Log",
        "abstract": "An Web log contains a large number of user browsing information, so how to effectively mine it for user browsing pattern is an important research subject. Based on the analysis of the problems in the current mining algorithm of the user browsing pattern, and combining the characteristics of the existing fast association rules mining algorithm, this paper adds the sequential constraint and the time factor, and puts forward a browsing pattern mining algorithm TBPM which is based on the temporal constraint. It also designs incremental updating algorithm based on the temporal frequent item set algorithm TBPM. At last, it makes a comparison with the related work of the class Apriori algorithm, and the experimental results on the actual data have verified the effectiveness of this algorithm.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6137642/",
        "year": "2011"
    },
    {
        "title": "Data mining and analysis in depth. case study of Qafqaz University HTTP server log analysis",
        "abstract": "The Internet Services, Web and Mobile Applications, Pervasive Communication widely available today meeting many of our needs and stimulating production of tremendous amounts of data. Over 90% of this information is unstructured, what means data does not have predefined structure and model. Generally, unstructured data is useless unless applying data mining or data extraction techniques. At the same time, just in case if we are able to process and understand data, this data worth anything, otherwise it becomes useless. Although, small part of this huge amount is structured (logs) or semi-structured (email, website), it is difficult to process and manage this data without advanced data analytics techniques. This paper provides an example of applying Data Mining and Analysis techniques on the data generated by HTTP Server Logs. Experimental results show that proposed analysis approach based on Regular Expressions is highly efficient and flexible. Results of such analysis are highly beneficial for any company which concerns about efficiency of their Internet-presence giving them important information based on the real data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7035947/",
        "year": "2014"
    },
    {
        "title": "Data Preparation of Web Log Files for Marketing Aspects Analyses",
        "abstract": "This article deals with several aspects of a marketing-oriented analysis of web log files. It discusses their preprocessing and possible ways to enrich the raw data that can be gained from a web log file in order to facilitate a later use in different analyses. Further, we look at the question which requirements a good web log analysis software needs to meet and offer an overview over current and future analysis practices including their advantages and disadvantages.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11790853_11",
        "year": "2006"
    },
    {
        "title": "Data Transformation and Semantic Log Purging for Process Mining",
        "abstract": "Existing process mining approaches are able to tolerate a certain degree of noise in the process log. However, processes that contain infrequent paths, multiple (nested) parallel branches, or have been changed in an ad-hoc manner, still pose major challenges. For such cases, process mining typically returns ‚Äúspaghetti-models‚Äù, that are hardly usable even as a starting point for process (re-)design. In this paper, we address these challenges by introducing data transformation and pre-processing steps that improve and ensure the quality of mined models for existing process mining approaches. We propose the concept of semantic log purging, the cleaning of logs based on domain specific constraints utilizing semantic knowledge which typically complements processes. Furthermore we demonstrate the feasibility and effectiveness of the approach based on a case study in the higher education domain. We think that semantic log purging will enable process mining to yield better results, thus giving process (re-)designers a valuable tool.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-31095-9_16",
        "year": "2012"
    },
    {
        "title": "Data-Driven Model-Based Detection of Malicious Insiders via Physical Access Logs",
        "abstract": "The risk posed by insider threats has usually been approached by analyzing the behavior of users solely in the cyber domain. In this paper, we show the viability of using physical movement logs, collected via a building access control system, together with an understanding of the layout of the building housing the system‚Äôs assets, to detect malicious insider behavior that manifests itself in the physical domain. In particular, we propose a systematic framework that uses contextual knowledge about the system and its users, learned from historical data gathered from a building access control system, to select suitable models for representing movement behavior. We then explore the online usage of the learned models, together with knowledge about the layout of the building being monitored, to detect malicious insider behavior. Finally, we show the effectiveness of the developed framework using real-life data traces of user movement in railway transit stations.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-66335-7_17",
        "year": "2017"
    },
    {
        "title": "Data-Driven Process Discovery - Revealing Conditional Infrequent Behavior from Event Logs",
        "abstract": "Process discovery methods automatically infer process models from event logs. Often, event logs contain so-called noise, e.g., infrequent outliers or recording errors, which obscure the main behavior of the process. Existing methods filter this noise based on the frequency of event labels: infrequent paths and activities are excluded. However, infrequent behavior may reveal important insights into the process. Thus, not all infrequent behavior should be considered as noise. This paper proposes the Data-aware Heuristic Miner (DHM), a process discovery method that uses the data attributes to distinguish infrequent paths from random noise by using classification techniques. Data- and control-flow of the process are discovered together. We show that the DHM is, to some degree, robust against random noise and reveals data-driven decisions, which are filtered by other discovery methods. The DHM has been successfully tested on several real-life event logs, two of which we present in this paper.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-59536-8_34",
        "year": "2017"
    },
    {
        "title": "Dataset Analysis of Proxy Logs Detecting to Curb Propagations in Network Attacks",
        "abstract": "The exponential growth of Internet has brought a monolithic change in the level of malicious attacks, leading to the emergence proxy servers, which indeed have proven to apotheosis hiding place for the Internet intruders. The collected logs of these proxy servers contain portentous information, and its dissection can help in analyzing the deviation of abnormal activities form the normal ones. How to figure out their network of networks, identify possible offenders, and strike the heartland of their safe haven has become an upcoming challenge for universal law enforcement agents. This paper considers exactly what kind of elements should be explored once an offensive behavior has been noticed in proxy logs. It scrutinizes (i) the Time Stamp gap of sequential records (ii) the parameters of digital action (iii) the appearance of special parameters (iv) the patterns in the log files.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-69304-8_24",
        "year": "2008"
    },
    {
        "title": "Decentralized log event correlation architecture",
        "abstract": "In our rapidly evolving societies, every corporate is trying to improve its competitiveness by refactoring and improving some - if not all - of its industrial software infrastructure. This goes from mainframe applications that actually handle the company's profit generating material, to the internal desktop applications used to manage those application servers. These applications often have extended activity logging features that notify the administrators of events those programs encounter at runtime. Unfortunately, the standalone nature of the event logging sources renders difficult the correlation of log events. In this setting, \"continuous queries\" developed in the database area offer a deal of opportunity to query such evolving logs. This paper describe an approach that \"adapt and employ continues queries\" for distributed log event correlation. Our aims cope with problems facing the present log event management systems.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1643919",
        "year": "2009"
    },
    {
        "title": "Declarative Process Mining: Reducing Discovered Models Complexity by Pre-Processing Event Logs",
        "abstract": "The discovery of declarative process models by mining event logs aims to represent flexible or unstructured processes, making them visible to business and improving their manageability. Although promising, the declarative perspective may still produce models that are hard to understand, both due to their size and to the high number of restrictions of the process activities. This work presents an approach to reduce declarative model complexity by aggregating activities according to inclusion and hierarchy semantic relations. The approach was evaluated through a case study with an artificial event log and its results showed complexity reduction on the resulting hierarchical model.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-10172-9_28",
        "year": "2014"
    },
    {
        "title": "Decoding efficiency of the MAP and the max-log MAP algorithm as a strategy in anomaly-based intrusion detection systems",
        "abstract": "Hidden Markov Methodology, with particular care to the parameter estimation and the training phase, represents a powerful finite state machine, suitable in various recognition problems. This paper investigated the capabilities of this methodology in anomaly-based intrusion detection. The model training is performed using ML criterion, based on the gradient method. Since the attacks recognition is considered as a decoding problem, the MAP and the max log MAP algorithms combined with gradient based method were applied. The comparison between these two decoding algorithms as a strategy in anomalybased IDS is represented as well.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1330633",
        "year": "2007"
    },
    {
        "title": "Decoding of serial concatenated convolutional codes using Log-MAP delta algorithm",
        "abstract": "In this paper, we study application of the novel reduced implementation complexity decoding algorithm - Log-MAP Delta to decoding serial concatenated convolutional codes (SCCC). We present BER performance results in an additive white Gaussian noise (AWGN) channel and compare the new algorithm against optimal Log-MAP and low complexity Max-Log-MAP decoding. Results show that the Log-MAP Delta algorithm performs with only a slight degradation compared to the Log-MAP while bringing substantial complexity savings.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6292786/",
        "year": "2012"
    },
    {
        "title": "Deducing Case IDs for Unlabeled Event Logs",
        "abstract": "Event logs are invaluable sources of knowledge about the actual execution of processes. A large number of techniques to mine, check conformance and analyze performance have been developed based on logs. All these techniques require at least case ID, activity ID and the timestamp to be in the log. If one of those is missing, these techniques cannot be applied. Real life logs are rarely originating from a centrally orchestrated process execution. Thus, case ID might be missing, known as unlabeled log. This requires a manual preprocessing of the log to assign case ID to events in the log.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-42887-1_20",
        "year": "2016"
    },
    {
        "title": "DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning",
        "abstract": "Anomaly detection is a critical step towards building a secure and trustworthy system. The primary purpose of a system log is to record system states and significant events at various critical points to help debug system failures and perform root cause analysis. Such log data is universally available in nearly all computer systems. Log data is an important and valuable resource for understanding system status and performance issues; therefore, the various system logs are naturally excellent source of information for online monitoring and anomaly detection. We propose DeepLog, a deep neural network model utilizing Long Short-Term Memory (LSTM), to model a system log as a natural language sequence. This allows DeepLog to automatically learn log patterns from normal execution, and detect anomalies when log patterns deviate from the model trained from log data under normal execution. In addition, we demonstrate how to incrementally update the DeepLog model in an online fashion so that it can adapt to new log patterns over time. Furthermore, DeepLog constructs workflows from the underlying system log so that once an anomaly is detected, users can diagnose the detected anomaly and perform root cause analysis effectively. Extensive experimental evaluations over large log data have shown that DeepLog has outperformed other existing log-based anomaly detection methods based on traditional data mining methodologies.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3134015",
        "year": "2017"
    },
    {
        "title": "Degradation in FH-MFSK mobile radio system capacity due to Rayleigh fading and log-normal shadowing",
        "abstract": "A frequency-hopped multilevel frequency-shift-keying (FH-MFSK) system has been proposed for digital mobile radio communications. The performance of the system is evaluated by studying average probability of error caused by transmission impairments. The degradation in performance due to Rayleigh fading and log-normal shadowing environments is determined. With perfect transmission, where the degradation in the system performance is due to mutual interference between users only, the system can accommodate up to 209 simultaneous users at an average bit error rate of 10/sup -3/. The system capacity decreases to 110 users as a result of additive white Gaussian noise (AWGN), mutual interference, frequency-selective Rayleigh fading, and log-normal shadowing with normalized area mean of 20 dB and standard deviation of 6 dB.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/16539/",
        "year": "1988"
    },
    {
        "title": "Design and evaluation of printed log periodic dipole antenna for an L band electrically steerable array system",
        "abstract": "RADAR systems that use a phased array and an electronically scanned array of antennas are extensively used in terrestrial applications on account of their vastly improved directive gain over single-element antennas. The effective beam pattern in such cases is a combination of the elemental pattern and the array factor. In this paper, the design and evaluation of a printed log-periodic dipole antenna, that operates in the L-band, is being discussed. The antenna was fabricated on an FR-4 substrate using printed circuit technology; tested and characterized in terms of its S-parameters, return loss, and insertion loss. The radiation pattern in the azimuth and elevation planes has also been determined. The antenna is found to have a directive pattern, and is linearly polarized with a wide bandwidth. The antenna is being designed for use in the front-end of a RADAR system for terrestrial surveillance applications such as landslide detection, and monitoring the productivity of metallicore mines.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7032669/",
        "year": "2014"
    },
    {
        "title": "Design and implementation of data integrity check of subway log collection system",
        "abstract": "Analyzing subway operation logs can help the management of the subway. In order to make the analysis results accurate, it is important to ensure the integrity of the collected subway logs. This paper proposes a subway log collection system based on Flume in which the integrity of the collected logs is checked. This paper describes the integrity check module in which HBase is used to store the large volume of logs. Through the preliminary processing of collected data and the design of HBase tables, it implemented the integrity check of collected data. After the integrity check, missing data is found and retransmitted for the following analysis.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7509820/",
        "year": "2016"
    },
    {
        "title": "Design of a Communication System that Can Predict Situations of an Absentee Using Its Behavior Log",
        "abstract": "We designed a communication system that can predict the current situation of an absentee on the basis of its behavior log and provide some communication tools suitable for the situation to a visitor. An operative evaluation revealed that the proposed system could predict user situations with high accuracy. Further, it can output the expected values in short terms by calculating the feedback values to the possibility table with our Bayesian network. Furthermore, the proposed system has realized the communication suitable for a situation and for information sharing including prediction by using the current information and communication technology.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-42007-3_81",
        "year": "2016"
    },
    {
        "title": "Design of a Log Management Infrastructure Using Meta-Network Analysis",
        "abstract": "The need for compliance or organization specific requirements is often guiding the implementation of a log management infrastructure. On a large scale infrastructure the log data are stored in various places, where analysts or administrators need to perform specific analysis tasks. In this work we propose a method for validating the design of the log collector part of the infrastructure, ensuring that each log collector has at its disposal the necessary log data for performing the desired analysis tasks. This is achieved by modeling the infrastructure as an organization and by applying social network analysis concepts and metrics that are used to analyze the structure and performance of real organizations. An example case study, demonstrating the workings of the method and the interpretation of the results, on a simulated infrastructure is also presented.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-44341-6_7",
        "year": "2016"
    },
    {
        "title": "Design of ARMA digital filters with arbitrary log magnitude function by WLS techniques",
        "abstract": "A technique is proposed for designing autoregressive-moving average (ARMA) digital filters to have an arbitrary log magnitude frequency response. The technique is based on an iterative weighted least-squares (WLS) approach in the frequency domain. A weight updating procedure is introduced to obtain a least-squares approximation to the given log magnitude function using the WLS approach. Filter coefficients are efficiently calculated using a fast recursive algorithm for a set of linear equations derived from the WLS problem. The technique is also extended to equiripple approximation with a minor modification of the weight updating procedure.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/196873/",
        "year": "1988"
    },
    {
        "title": "Design of Wireless Logging Instrument System for Monitoring Oil Drilling Platform",
        "abstract": "In order to achieve more effective monitoring and to solve the heavy workload issues of installation and removal on the existing wired-system logging site, thus guarantee the safety in normal operation, a wireless logging instrument system is designed to monitor the oil drilling platform. This paper introduces the overall architecture of the wireless system, and develops the hardware system - in which embedded software is designed - based on analog-digital conversion device, field-programmable gate array chip, and microprocessor. Combined with wireless sensor technology, analog measurements, and encoding quadruple frequency and phase-detection techniques, the wireless nodes in the system can detect and preprocess analog signals in the frequency range from 0.1 to 200 Hz and encoder signals of 90¬∞ phase difference, then output data through wireless transmitter module to upper monitoring center. Experimental results show that this system can realize a real-time wireless communication and monitoring for oil drilling platform stably. Moreover, it has features such as coding flexibility, high precision, good reliability, high integration, small size, and low-power consumption. Thus, the system provides a promising solution of data collection and analysis for the drilling platform.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7008480/",
        "year": "2015"
    },
    {
        "title": "Design, implementation and validation of a graphical user interface for analysis of patient monitor event logs",
        "abstract": "With the emergence of networks and sophisticated programming languages, it has become possible over the years to accumulate and translate large amounts of data in a short time period. And as biomedical engineering departments gain more active roles in the hospital, the opportunity to use such data to improve the effectiveness and safety of patient care increases. The patient monitoring network at Hartford Hospital lends itself to data elements such as patient alarms and indicators that can be used to track a patient's medical record or history. These error logs are created on GE Clinical Information Center (CIC) computers, but such logs remain there without being analyzed until there is a need to investigate suspected device malfunctions or operator errors. These data log files were previously accessed through a manual process using DOS commands to export the data from the CIC to a floppy disk for further manual analysis. Thus, with the large amounts of patient data, there is a need to create an application that allows engineers and clinicians to make better, faster data-driven decisions. The proposed developments to existing utilities provide an excellent opportunity to utilize biomedical engineering principles and skills in the clinical environment.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1300055/",
        "year": "2004"
    },
    {
        "title": "Designing a fast log-tracing scheme for targeted attack prevention",
        "abstract": "In this paper, we design a fast log-tracing scheme for preventing targeted attacks to enterprise information networks. In these attacks, confidential data leak through application gateways. In order to detect such leakage, a network management server collects multiple logs. Then a gateway traces them to check whether the forwarding data is confidential or not. In the conventional basic scheme, this check will require long processing time if log volume becomes large. In our proposed scheme, at first, multiple logs are preprocessed offline to form a black list. A gateway checks a file to be forwarded online using this black list. The evaluation results show that the tracing time can be shortened to one severalth by means of our proposed scheme.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7217111/",
        "year": "2015"
    },
    {
        "title": "Designing and Detecting Trapdoors for Discrete Log Cryptosystems",
        "abstract": "Using a number field sieve, discrete logarithms modulo primes of special forms can be found faster than standard primes. This has raised concerns about trapdoors in discrete log cryptosystems, such as the Digital Signature Standard. This paper discusses the practical impact of these trapdoors, and how to avoid them.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-48071-4_5",
        "year": "1993"
    },
    {
        "title": "Detecting Absurd Conversations from Intelligent Assistant Logs by Exploiting User Feedback Utterances",
        "abstract": "Intelligent assistants, such as Siri, are expected to converse comprehensibly with users. To facilitate improvement of their conversational ability, we have developed a method that detects absurd conversations recorded in intelligent assistant logs by identifying user feedback utterances that indicate users' favorable and unfavorable evaluations of intelligent assistant responses; e.g., \"great!\" is favorable, whereas \"what are you talking about?\" is unfavorable. Assuming that absurd/comprehensible conversations tend to be followed by unfavorable/favorable utterances, our method extracts some absurd/comprehensible conversations from the log to train a conversation classifier that sorts all the conversations recorded in the log as either absurd or not. The challenge is that user feedback utterances are often ambiguous; e.g., a user may give an unfavorable utterance (e.g., \"don't be silly!\") to a comprehensible conversation in which the intelligent assistant was attempting to make a joke. An utterance classifier is thus used to score the feedback utterances in accordance with how unambiguously they indicate absurdity. Experiments showed that our method significantly outperformed methods that lacked a conversation and/or utterance classifier, indicating the effectiveness of the two classifiers. Our method only requires user feedback utterances, which would be independent of domains. Experiments focused on chitchat, web search, and weather domains indicated that our method is likely domain-independent.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=3185992",
        "year": "2018"
    },
    {
        "title": "Detecting and Resolving Process Model Differences in the Absence of a Change Log",
        "abstract": "Business-driven development favors the construction of process models at different abstraction levels and by different people. As a consequence, there is a demand for consolidating different versions of process models by detecting and resolving differences. Existing approaches rely on the existence of a change log which logs the changes when changing a process model. However, in several scenarios such a change log does not exist and differences must be identified by comparing process models before and after changes have been made. In this paper, we present our approach to detecting and resolving differences between process models, in the absence of a change log. It is based on computing differences and deriving change operations for resolving differences, thereby providing a foundation for variant and version management in these cases.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-85758-7_19",
        "year": "2008"
    },
    {
        "title": "Detecting and Tracking Topics and Events from Web Search Logs",
        "abstract": "Recent years have witnessed increased efforts on detecting topics and events from Web search logs, since this kind of data not only capture web content but also reflect the users‚Äô activities. However, the majority of existing work is focused on exploiting clustering techniques for topic and event detection. Due to the huge size and the evolving nature of Web data, existing clustering approaches are limited to meet the real-time demand. To that end, in this article, we propose a method called LETD to detect evolving topics in a timely manner. Also, we design the techniques to extract events from topics and to infer the evolving relationship among the events. For topic detection, we first provide a measurement to select the important URLs, which are most likely to describe a real-life topic. Then, starting from these selected URLs, we exploit the local expansion method to find other topic-related URLs. Moreover, in the LETD framework, we design algorithms based on Random Walk and Markov Random Fields (MRF), respectively. Because the LETD method exploits a divide-and-conquer strategy to process the data, it is more efficient than existing methods based on clustering techniques. To better illustrate the LETD framework, we develop a demo system StoryTeller which can discover hot topics and events, infer the evolving relationships among events, and visualize information in a storytelling way. This demo system can provide a global view of the topic development and help users target the interesting events more conveniently. Finally, experimental results on real-world Microsoft click-through data have shown that StoryTeller can find real-life hot topics and meaningful evolving relationships among events, and has also demonstrated the efficiency and effectiveness of the LETD method.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2382440",
        "year": "2012"
    },
    {
        "title": "Detecting Anomalous Behavior in DBMS Logs",
        "abstract": "It is argued that anomaly-based techniques can be used to detect anomalous DBMS queries by insiders. An experiment is described whereby an n-gram model is used to capture normal query patterns in a log of SQL queries from a synthetic banking application system. Preliminary results demonstrate that n-grams do capture the short-term correlations inherent in the application.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-54876-0_12",
        "year": "2017"
    },
    {
        "title": "Detecting Devastating Diseases in Search Logs",
        "abstract": "Web search queries can offer a unique population-scale window onto streams of evidence that are useful for detecting the emergence of health conditions. We explore the promise of harnessing behavioral signals in search logs to provide advance warning about the presence of devastating diseases such as pancreatic cancer. Pancreatic cancer is often diagnosed too late to be treated effectively as the cancer has usually metastasized by the time of diagnosis. Symptoms of the early stages of the illness are often subtle and nonspecific. We identify searchers who issue credible, first-person diagnostic queries for pancreatic cancer and we learn models from prior search histories that predict which searchers will later input such queries. We show that we can infer the likelihood of seeing the rise of diagnostic queries months before they appear and characterize the tradeoff between predictivity and false positive rate. The findings highlight the potential of harnessing search logs for the early detection of pancreatic cancer and more generally for harnessing search systems to reduce health risks for individuals.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2939722",
        "year": "2016"
    },
    {
        "title": "Detecting epidemic tendency by mining search logs",
        "abstract": "We consider the problem of detecting epidemic tendency by mining search logs. We propose an algorithm based on click-through information to select epidemic related queries/terms. We adopt linear regression to model epidemic occurrences and frequencies of epidemic related terms (ERTs) in search logs. The results show our algorithm is effective in finding ERTs which obtain a high correlation value with epidemic occurrences. We also find the proposed method performs better when combining different ERTs than using single ERT.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1772840",
        "year": "2010"
    },
    {
        "title": "Detecting high log-densities: an O(n¬º) approximation for densest k-subgraph",
        "abstract": "In the Densest k-Subgraph problem, given a graph G and a parameter k, one needs to find a subgraph of G induced on k vertices that contains the largest number of edges. There is a significant gap between the best known upper and lower bounds for this problem. It is NP-hard, and does not have a PTAS unless NP has subexponential time algorithms. On the other hand, the current best known algorithm of Feige, Kortsarz and Peleg, gives an approximation ratio of n1/3 - c for some fixed c\u003e0 (later estimated at around c= 1/90).We present an algorithm that for every Œµ\u003e 0 approximates the Densest k-Subgraph problem within a ratio of n¬º + Œµ in time nO(1/Œµ). If allowed to run for time nO(log n), the algorithm achieves an approximation ratio of O(n¬º). Our algorithm is inspired by studying an average-case version of the problem where the goal is to distinguish random graphs from random graphs with planted dense subgraphs -- the approximation ratio we achieve for the general case matches the \"distinguishing ratio\" we obtain for this planted problem.At a high level, our algorithms involve cleverly counting appropriately defined trees of constant size in G, and using these counts to identify the vertices of the dense subgraph. We say that a graph G(V,E) has log-density Œ± if its average degree is Œò(|V|Œ±). The algorithmic core of our result is a procedure to output a k-subgraph of 'nontrivial' density whenever the log-density of the densest k-subgraph is larger than the log-density of the host graph.We outline an extension to our approximation algorithm which achieves an O(n¬º -Œµ)-approximation in O(2nO(Œµ)) time. We also show that, for certain parameter ranges, eigenvalue and SDP based techniques can outperform our basic distinguishing algorithm for random instances (in polynomial time), though without improving upon the O(n¬º) guarantee overall.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1806719",
        "year": "2010"
    },
    {
        "title": "Detecting Hot Events from Web Search Logs",
        "abstract": "Detecting events from web resources is a challenging task, attracting many attentions in recent years. Web search log is an important data source for event detection because the information it contains reflects users‚Äô activities and interestingness to various real world events. There are three major issues for event detection from web search logs: effectiveness, efficiency and the organization of detected events. In this paper, we develop a novel Topic and Event Detection method, TED, to address these issues. We first divide the whole data into topics for efficiency consideration, and then incorporate link information, temporal information and query content to ensure the quality of detected events. Finally, events detected are organized through the proposed interestingness measure as well as topics they belong to. Experiments are conducted on a commercial search engine log. The results demonstrate that our method can effectively and efficiently detect hot events and give a meaningful organization of them.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-14246-8_41",
        "year": "2010"
    },
    {
        "title": "Detecting implicit dependencies between tasks from event logs",
        "abstract": "Process mining aims at extracting information from event logs to capture the business process as it is being executed. In spite of many researchers‚Äô persistent efforts, there are still some challenging problems to be solved. In this paper, we focus on mining non-free-choice constructs, where the process models are represented in Petri nets. In fact, there are totally two kinds of causal dependencies between tasks, i.e., explicit and implicit ones. Implicit dependency is very hard to mine by current mining approaches. Thus we propose three theorems to detect implicit dependency between tasks and give their proofs. The experimental results show that our approach is powerful enough to mine process models with non-free-choice constructs.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/11610113_52",
        "year": "2006"
    },
    {
        "title": "Detecting Insider Information Theft Using Features from File Access Logs",
        "abstract": "Access control is a necessary, but often insufficient, mechanism for protecting sensitive resources. In some scenarios, the cost of anticipating information needs and specifying precise access control policies is prohibitive. For this reason, many organizations provide employees with excessive access to some resources, such as file or source code repositories. This allows the organization to maximize the benefit employees get from access to troves of information, but exposes the organization to excessive risk. In this work we investigate how to build profiles of normal user activity on file repositories for uses in anomaly detection, insider threats, and risk mitigation. We illustrate how information derived from other users‚Äô activity and the structure of the filesystem hierarchy can be used to detect abnormal access patterns. We evaluate our methods on real access logs from a commercial source code repository on tasks of user identification and users seeking to leak resources by accessing more than they have a need for.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-11212-1_22",
        "year": "2014"
    },
    {
        "title": "Detecting Multitasking Work and Negative Routines from Computer Logs",
        "abstract": "Multitasking on digital media has a negative effect on mental health and concentration. At the same time, the negative effects of computer usage are not immediately obvious to most people. We suggest that people can improve their daily experience on the computer if they pay closer attention to their multitasking activities. To this end, we have constructed a system that detects multitasking work and periodic negative multitasking routines from computer logs. We created two indicators: relax rate and multitasking rate. The relax rate is defined on the basis of heart rate variability information and the multitasking rate is derived from how often users switch their computer windows. We analyze whether users‚Äô multitasking is negative or not and whether or not negative multitasking is part of a periodic routine. We logged the computer activities and heart rate data of one participant for six days.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-40397-7_52",
        "year": "2016"
    },
    {
        "title": "Detecting Privacy Information Abuse by Android Apps from API Call Logs",
        "abstract": "In these years, the use of smartphones is spreading. Android is the most major smartphone OS in the world, and there are a lot of third-party application stores for Android. Such third-party stores make it easy to install third-party applications. However, these applications may access and obtain privacy information, in addition to their major functions. There is a survey showing that most users do not take good care of the settings about how their privacy information is handled by applications. Thus, privacy information abuse by authorized application is becoming a serious problem. In this paper, we propose a method to detect applications that access privacy information unrelated to their functionalities by analyzing API call logs, which can reveal the activities of the application. In order to record API call logs, we modified the Android source code, and run the rebuilt system on an emulator. We analyzed applications‚Äô API call logs with a statistical method, based on the frequency of privacy information accessing and network activities.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-97916-8_10",
        "year": "2018"
    },
    {
        "title": "Detecting Process Concept Drifts from Event Logs",
        "abstract": "Traditional process discovery algorithms assume processes to be in a steady state. However, process models tend to be dynamic due to various factors, which has brought challenges such as change point detection, change localization and change process discovery. Existing techniques to identify change points are sensitive to parameters and the accuracy is not satisfactory. This paper proposes a novel approach to deal with such concept drift phenomenon. Event logs can be characterized by the relationships between activities, which motivates us to transform a log into a relation matrix. By detecting the always and never intervals in each row of the relation matrix, we obtain candidate change points for each relation. Finally, all the candidate change points are combined into an overall result. The approach is also able to localize the changes between different phases. Experiments on synthetic logs show that our approach is accurate and performs better than the state of the art in detecting sudden drift.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-69462-7_33",
        "year": "2017"
    },
    {
        "title": "Detecting Web Crawlers from Web Server Access Logs with Data Mining Classifiers",
        "abstract": "In this study, we introduce two novel features: the consecutive sequential request ratio and standard deviation of page request depth, for improving the accuracy of malicious and non-malicious web crawler classification from static web server access logs with traditional data mining classifiers. In the first experiment we evaluate the new features on the classification of known well-behaved web crawlers and human visitors. In the second experiment we evaluate the new features on the classification of malicious web crawlers, unknown visitors, well-behaved crawlers and human visitors. The classification performance is evaluated in terms of classification accuracy, and F1 score. The experimental results demonstrate the potential of the two new features to improve the accuracy of data mining classifiers in identifying malicious and well-behaved web crawler sessions.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-21916-0_52",
        "year": "2011"
    },
    {
        "title": "Detecting wi-fi base station behavior inappropriate for positioning method in participatory sensing logs",
        "abstract": "Recently mobile base stations are getting increased, which is considered harmful for the Wi-Fi positioning methods. In this paper, three approaches for detecting Wi-Fi base station behaviors inappropriate for Wi-Fi signature sampling are introduced and their performance evaluations are presented. First approach is for outdoor environment using GPS or Wi-Fi, second for indoor environment using Wi-Fi and accelerometers and last for the first contact stations using the Bayesian estimation method. Bayesian estimation is fine for stationary stations but much severe for mobile stations.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2495987",
        "year": "2013"
    },
    {
        "title": "Detection and Interactive Repair of Event Ordering Imperfection in Process Logs",
        "abstract": "Many forms of data analysis require timestamp information to order the occurrences of events. The process mining discipline uses historical records of process executions, called event logs, to derive insights into business process behaviours and performance. Events in event logs must be ordered, typically achieved using timestamps. The importance of timestamp information means that it needs to be of high quality. To the best of our knowledge, no (semi-)automated support exists for detecting and repairing ordering-related imperfection issues in event logs. We describe a set of timestamp-based indicators for detecting event ordering imperfection issues in a log and our approach to repairing identified issues using domain knowledge. Lastly, we evaluate our approach implemented in the open-source process mining framework, ProM, using two publicly available logs.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-91563-0_17",
        "year": "2018"
    },
    {
        "title": "Detection and segmentation of clustered objects by using iterative classification, segmentation, and Gaussian mixture models and application to wood log detection",
        "abstract": "There have recently been advances in the area of fully automatic detection of clustered objects in color images. State of the art methods combine detection with segmentation. In this paper we show that these methods can be significantly improved by introducing a new iterative classification, statistical modeling, and segmentation procedure. The proposed method used a detect-and-merge algorithm, which iteratively finds and validates new objects and subsequently updates the statistical model, while converging in very few iterations.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-319-11752-2_28",
        "year": "2014"
    },
    {
        "title": "Detection of fixed targets embedded in log-normal clutter",
        "abstract": "Measurements of radar clutter using high-resolution systems show that clutter echoes cannot be modelled as Gaussian-distributed. A log-normal distribution is assumed and single-pulse detection performances are evaluated according to the Marcum-Swerling approach as well as the signal losses with respect to the Gaussian case.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4251663/",
        "year": "1985"
    },
    {
        "title": "Detection of Fractures Based on Directional Filtering in Image Logging",
        "abstract": "According to the feature of the direction of fractures in image logging, a method of fracture detection in image logging based on directional filtering is presented in this paper. The fracture image is filtered by the corresponding filter based on the local direction of fractures, which enhances the fracture traces in the direction of fractures in image logging. The final fracture traces are extracted by the combination of the results of both directional filtering and the threshold segmentation. Experimental results show this method effectively detect the fractures in image logging.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6478495/",
        "year": "2012"
    },
    {
        "title": "Detection of geological structure using gamma logs for autonomous mining",
        "abstract": "This work is motivated by the need to develop new perception and modeling capabilities to support a fully autonomous, remotely operated mine. The application differs from most existing robotics research in that it requires a detailed world model of the sub-surface geological structure. This in-ground geological information is then used to drive many of the planning and control decisions made on a mine site. This paper formulates a method for automatically detecting in-ground geological boundaries using geophysical logging sensors and a supervised learning algorithm. The algorithm uses Gaussian Processes (GPs) and a single length scale squared exponential covariance function. The approach is demonstrated on data from a producing iron-ore mine in Australia. Our results show that two separate distinctive geological boundaries can be automatically identified with an accuracy of over 99 percent. The alternative approach to automatic detection involves manual examination of these data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5980489/",
        "year": "2011"
    },
    {
        "title": "Detection of selective logging and regrowth with CBERS CCD imagery",
        "abstract": "We used imagery from the China-Brazil Earth Resources Satellite (CBERS1) to detect selective logging in Mato Grosso state, Brazil. An automated procedure using optical data was developed to determine which areas are under active, or recent, selective logging. We were able to detect selective logging using CBERS1 CCD images.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1026448/",
        "year": "2002"
    },
    {
        "title": "Detector log video amplifier with 60 dB logging range",
        "abstract": "Presents an extended range detector log video amplifier for the 2-18 GHz range usable in RWR and ESM receivers. Beside details of the logging and summing amplifier, experimental results of log linearity in the -50 dBm to +10 dBm input power range and response to short 50 ns pulses are given.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1017847/",
        "year": "2002"
    },
    {
        "title": "Determining characteristics of successful recommendations from log data: a case study",
        "abstract": "Academic research in recommender systems largely focuses on the problem of predicting the relevance of (long-tail) items that the individual user presumably does not know yet. Many real-world systems however also recommend items that users have inspected in the past, items that are popular at the moment, and items currently on sale. In this work we investigate the value of including such items in recommendation lists based on an analysis of the web logs of a large online retailer. An examination of the features of successful item suggestions reveals that the chances of a recommendation leading to a purchase increase when the item is recently trending, on sale, or was recently viewed by the user. Offline simulation experiments furthermore show that considering those success factors that were identified from log data in the ranking algorithms can help to increase the prediction accuracy of recommender systems.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=3019757",
        "year": "2017"
    },
    {
        "title": "Determining Factors Behind the PageRank Log-Log Plot",
        "abstract": "We study the relation between PageRank and other parameters of information networks such as in-degree, out-degree, and the fraction of dangling nodes. We model this relation through a stochastic equation inspired by the original definition of PageRank. Further, we use the theory of regular variation to prove that PageRank and in-degree follow power laws with the same exponent. The difference between these two power laws is in a multiplicative constant, which depends mainly on the fraction of dangling nodes, average in-degree, the power law exponent, and the damping factor. The out-degree distribution has a minor effect, which we explicitly quantify. Finally, we propose a ranking scheme which does not depend on out-degrees.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-77004-6_9",
        "year": "2007"
    },
    {
        "title": "Determining the Optimal Session Interval for Transaction Log Analysis of an Online Library Catalogue",
        "abstract": "Transaction log analysis at the level of a session is commonly used as a means of understanding user-system interactions. A key practical issue in the process of conducting session level analysis is the segmentation of the logs into appropriate user sessions (i.e., sessionisation). Methods based on time intervals are frequently used as a simple and convenient means of carrying out this segmentation task. However, little work has been carried out to determine whether the commonly applied 30-minute period is appropriate, particularly for the analysis of search logs from library catalogues. Comparison of a range session intervals with human judgements demonstrate that the overall accuracy of session segmentation is relatively constant for session intervals between 26 to 57¬†min. However, a session interval of between 25 and 30¬†min minimises the chances of one error type (incorrect collation or incorrect segmentation) predominating.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-30671-1_56",
        "year": "2016"
    },
    {
        "title": "Determining the Visibility of a Planar Set of Line Segments in ${\\mathcal{O}(n\\log\\log n)}$ Time",
        "abstract": "The visibility of a planar set of n disjoint line segments, arising from the scanline approach to rendering three-dimensional scenes, is one of the classic problems in computer graphics. In order to solve the problem quickly, many authors proposed binary space partitioning (BSP) as a preprocessing, possibly breaking up the input line segments so that visibility is determined in time linear in the number of resulting segments. T√≥th [Discrete \u0026 Comput. Geometry 30,1 pp. 3‚Äì16, 2003] demonstrated that a BSP may result in \\mathit{\\Omega}(n \\log n / \\log \\log n) line segments. We demonstrate that the time and space complexities of the problem are \\mathit{\\Theta}(n\\log n) and \\mathit{\\Theta}(n) respectively, under the algebraic RAM model of computation. Introducing a more realistic model, a RAM with arbitrary-precision rational arithmetics, a deterministic algorithm is given that solves the problem directly, without the need of preprocessing, in \\mathcal{O}(n\\log\\log n) time and \\mathcal{O}(n) space, regardless of the precision of the input data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-74477-1_5",
        "year": "2007"
    },
    {
        "title": "Determining WWW user agents from server access log",
        "abstract": "It is a common practice to analyze an access log of an WWW server to obtain data, such as user agent vendor percentage, supplying information more suitably, or to measure information exposure rate (mainly for advertisement effectiveness). HTTP does have an \"User-Agent\" header to let user agents supply their product names for these kinds of use, but in fact relying solely on this tends to be inaccurate. This paper presents some considerations needed for log analysis to determine the user agent in a more precise way, and application to an actual access log resulted in 2 percent difference compared to conventional method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/884534/",
        "year": "2000"
    },
    {
        "title": "Developing an Error Logging Framework for Ruby on Rails Application Using AOP",
        "abstract": "A framework for detecting and recording the flaws that happen during the usage of web applications is designed and a library functionality to perform this is discussed in this paper. The recorded information can be stored at different levels of detail, commonly called the logging levels. For some modules more than others, it may be required to store more detailed information about any error that arises during its usage according to its importance. A Web Application also needs to print the stack trace containing the error information on the web page when an error occurs for the user to understand the nature of the error. When dealing with legacy web applications, it is difficult to insert code. The proposed and designed framework is tested with a web application called Kic Kart.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6755103/",
        "year": "2014"
    },
    {
        "title": "Developing password security system by using artificial neural networks in user log in systems",
        "abstract": "By the developing technology, the virtual world is progressing every day to serve us and we are more being on the social media by this services. We can easily share about our daily lives from our social media accounts. Thanks to mobile banking, even in our houses we can go on banking. And we all believe its safety by using some key combinations that we think they are private. However today all the systems that we use, are providing security by comparing the passwords without differanciating the users. It means that this way is not blocking some malicious people that try to learn our passwords and get enter our systems. It was designed a structure, is unique for each individual in the system that we developed. This system knows the users' keyboard style while typing key combination. Even if someone else tries to log in with your password, system recognizes your personal keyboard style and does not allow to access unless this person is you. In that way no one other than user him/herself can access the system. In this work, user entry system is developed by machine learning by based. Artificial neural networks used in developing this system. The way of user's password entry style is learned, and during login process these stiles are compared besides password matching. Since the user log in system recognizes us that also makes an emotional connection between people and computers.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7483682/",
        "year": "2016"
    },
    {
        "title": "Development of a Blended Learning System for Engineering Students Studying Intellectual Property Law and Access Log Analysis of the System",
        "abstract": "In this paper, we analyzed access logs to this blended learning system. The blended learning system is divided into a login section, a menu section, a news section, a video viewing section, an exercise section, and a pdf output section. Therefore, we analyzed relationship between (a) access frequency, (b) video viewing time, (c) exercise system use time, and (d) pdf output count and students‚Äô achievement, respectively. The main results were as follows. (1) Of the 30 students who took the final exam, 22 students accessed the system one or more times. (2) The total access count was 81 times, the total video viewing time was 79¬†h 56¬†min 51¬†s, the exercise system use time was 22¬†h 20¬†min 58¬†s, the number of pdf output was 83 times (278 sheets). (3) Achievement superiors used all of the (a), (b), (c) and (d) more times or more hours than the achievement subordinate. (4) In the basic problems, there was a positive correlation between the students‚Äô achievement and video viewing, exercise system utilization time, respectively. (5) In the applied problems, there was a high positive correlation between the students‚Äô achievement and the use time of the exercise system. (6) In the problem with a low percentage of correct answers, a high positive correlation was found between the score and the use time of exercise system. From the above results, it seems that both video and exercise system are useful for acquiring basic knowledge, and exercise system is useful for developing applied abilities. In addition, it was considered that utilizing the exercise system also contributes to solving high difficulty problems.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-92046-7_21",
        "year": "2018"
    },
    {
        "title": "Development of digital log and analysis of its records in centers for the disabled",
        "abstract": "Day care and housing support services provided for people with disabilities account for 1 h to 24 h a day in welfare services. Services are provided according to goals established on a weekly or a monthly basis, and results are recorded and analyzed based on the log and plan evaluation. The level of services is evaluated objectively through day care accreditation for day care services and facility evaluation for housing facilities. Evaluation results are used as an important index to evaluate the service level of each institution. A serial process of establishment, practice, and evaluation of a service plan entails administrative documents, such as service plan, log, and evaluation sheet [1]. The existing method of preparing handwritten administrative documents is time consuming and entails difficulty in analysis.To address these problems, a method to improve the log recording system was introduced by the development of a digital log recording service, which enables recording of data through a digital log service and analysis of the recorded log. Two kinds of digital log services were developed, as shown in Figure 1: day care log for day care institutions and daily life log for housing facilities[2].The analysis of digital recorded logs covered those from January 1, 2009, the launch of its service, to April 30, 2013. Most of the daily life logs were written at 8 AM, whereas most day care logs were recorded from 4 PM to 5 PM. The recording time required was 24 minutes and 32 seconds for day care logs and 15 minutes and 3 seconds for daily life logs. Given the preparation time, such as turning on a computer, day care log writing takes nearly 30 minutes, which accounts for 6.25% of an eight-hour workday.The time required from log writing to obtaining approval from the manager accounted for 40% of X for the digital day care log; 72% of the log took less than one week to gain approval.Based on the research findings, improved digital services must be developed to assist smart devices, which can reduce the writing time through simplification of the writing form and reduction of the electronic approval time for digital day care logs.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2567473",
        "year": "2013"
    },
    {
        "title": "Dexter: faster troubleshooting of misconfiguration cases using system logs",
        "abstract": "Misconfigurations in the storage systems can lead to business losses due to system downtime with substantial people resources invested into troubleshooting. Hence, faster troubleshooting of software misconfigurations has been critically important for the customers as well as the vendors.This paper introduces a framework and a tool called Dexter, which embraces the recent trend of viewing systems as data to derive the troubleshooting clues. Dexter provides quick insights into the problem root cause and possible resolution by solely using the storage system logs. This differentiates Dexter from other previously known approaches which complement log analysis with source code analysis, execution traces etc.. Furthermore, Dexter analyzes command history logs from the sick system after it has been healed and predicts the exact command(s) which resolved the problem. Dexter's approach is simple and can be applied to other software systems with diagnostic logs for immediate problem detection without any pre-trained models.Evaluation on 600 real customer support cases shows 90% accuracy in root causing and over 65% accuracy in finding an exact resolution for the misconfiguration problem. Results show up to 60% noise reduction in system logs and at least 10x savings in case resolution times, bringing down the troubleshooting times from days to minutes at times. Dexter runs 24x7 in the NetApp's¬Æ support data center.The paper also presents insights from study on thousands of real customer support cases over thousands of deployed systems over the period of 1.5 years. These investigations uncover facts that cause potential delays in customer case resolutions and influence Dexter's design.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3078484",
        "year": "2017"
    },
    {
        "title": "Diagnostics analysis for log-Birnbaum‚ÄìSaunders regression models",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0167947306002908",
        "year": "2007"
    },
    {
        "title": "Dietary and Health Information Logging System for Lifestyle-Related Diseases",
        "abstract": "In this paper, we propose a wearable dietary and health information logging system. We've developed the system by using a cellular phone, a sphygmomanometer, a body composition monitor, and a calorie consumption meter. We also have conducted an experiment with a subject and succeeded to get his dietary and health information continuously for two months.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4097772/",
        "year": "2006"
    },
    {
        "title": "DigLA‚ÄìA Digsby log analysis tool to identify forensic artifacts",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1742287612000837",
        "year": "2013"
    },
    {
        "title": "Direct determination of the gamma-ray logging system response function in field boreholes",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0016714280900307",
        "year": "1980"
    },
    {
        "title": "Directed Acyclic Graph Extraction from Event Logs",
        "abstract": "The usage of probabilistic models in business process mining enables analysis of business processes in a more efficient manner. Although, the Bayesian belief network is one of the most common probabilistic models, possibilities to use it in business process mining are still not widely researched. Existing process mining approaches are incapable to extract directed acyclic graphs for representing Bayesian networks. This paper presents an approach for extraction of directed acyclic graph from event logs. The results obtained during the experiment show that the proposed approach is feasible and may be applied in practice.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-11958-8_14",
        "year": "2014"
    },
    {
        "title": "Discovering and Tracking Organizational Structures in Event Logs",
        "abstract": "The goal of process mining is to extract process-related information by observing events recorded in event logs. An event is an activity initiated or completed by a resource at a certain time point. Organizational mining is a subfield of process mining that focuses on the organizational perspective of a business process. It considers the resource attribute and derives a profile that characterizes the behavior of a resource in a specific business process. By relating resources associated with correlated profiles, it is possible to define a social network. This paper focuses on the idea of performing organizational mining of event logs via social network mining. It presents a framework that resorts to a stream representation of an event log. It adapts the time-based window model to process this stream, so that window-based social resource networks can be constructed, in order to represent interactions between resources operating at the data window level. Finally, it integrates specific algorithms, in order to discover (overlapping) communities of resources and track the evolution of these communities over consecutive windows. This paper applies the defined framework to two real event¬†logs.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39315-5_4",
        "year": "2016"
    },
    {
        "title": "Discovering Block-Structured Process Models from Event Logs - A Constructive Approach",
        "abstract": "Process discovery is the problem of, given a log of observed behaviour, finding a process model that ‚Äòbest‚Äô¬†describes this behaviour. A large variety of process discovery algorithms has been proposed. However, no existing algorithm guarantees to return a fitting model (i.e., able to reproduce all observed behaviour) that is sound (free of deadlocks and other anomalies) in finite time. We present an extensible framework to discover from any given log a set of block-structured process models that are sound and fit the observed behaviour. In addition we characterise the minimal information required in the log to rediscover a particular process model. We then provide a polynomial-time algorithm for discovering a sound, fitting, block-structured model from any given log; we give sufficient conditions on the log for which our algorithm returns a model that is language-equivalent to the process model underlying the log, including unseen behaviour. The technique is implemented in a prototypical tool.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-38697-8_17",
        "year": "2013"
    },
    {
        "title": "Discovering Block-Structured Process Models from Event Logs Containing Infrequent Behaviour",
        "abstract": "Given an event log describing observed behaviour, process discovery aims to find a process model that ‚Äòbest‚Äô¬†describes this behaviour. A large variety of process discovery algorithms has been proposed. However, no existing algorithm returns a sound model in all cases (free of deadlocks and other anomalies), handles infrequent behaviour well and finishes quickly. We present a technique able to cope with infrequent behaviour and large event logs, while ensuring soundness. The technique has been implemented in ProM and we compare the technique with existing approaches in terms of quality and performance.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-06257-0_6",
        "year": "2014"
    },
    {
        "title": "Discovering block-structured process models from event logs-a constructive approach",
        "abstract": "Process discovery is the problem of, given a log of observed behaviour, finding a process model that ‚Äòbest‚Äô¬†describes this behaviour. A large variety of process discovery algorithms has been proposed. However, no existing algorithm guarantees to return a fitting model (i.e., able to reproduce all observed behaviour) that is sound (free of deadlocks and other anomalies) in finite time. We present an extensible framework to discover from any given log a set of block-structured process models that are sound and fit the observed behaviour. In addition we characterise the minimal information required in the log to rediscover a particular process model. We then provide a polynomial-time algorithm for discovering a sound, fitting, block-structured model from any given log; we give sufficient conditions on the log for which our algorithm returns a model that is language-equivalent to the process model underlying the log, including unseen behaviour. The technique is implemented in a prototypical tool.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-38697-8_17",
        "year": "2013"
    },
    {
        "title": "Discovering Block-Structured Process Models from Incomplete Event Logs",
        "abstract": "One of the main challenges in process mining is to discover a process model describing observed behaviour in the best possible manner. Since event logs only contain example behaviour and one cannot assume to have seen all possible process executions, process discovery techniques need to be able to handle incompleteness. In this paper, we study the effects of such incomplete logs on process discovery. We analyse the impact of incompleteness of logs on behavioural relations, which are abstractions often used by process discovery techniques. We introduce probabilistic behavioural relations that are less sensitive to incompleteness, and exploit these relations to provide a more robust process discovery algorithm. We prove this algorithm to be able to rediscover a model of the original system. Furthermore, we show in experiments that our approach even rediscovers models from incomplete event logs that are much smaller than required by other process discovery algorithms.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07734-5_6",
        "year": "2014"
    },
    {
        "title": "Discovering Decision Models from Event Logs",
        "abstract": "Enterprise business process management is directly affected by how effectively it designs and coordinates decision making. To ensure optimal process executions, decision management should incorporate decision logic documentation and implementation. To achieve the separation of concerns principle, the OMG group proposes to use Decision Model and Notation (DMN) in combination with Business Process Model and Notation (BPMN). However, often in practice, decision logic is either explicitly encoded in process models through control flow structures, or it is implicitly contained in process execution logs. Our work proposes an approach of semi-automatic derivation of DMN decision models from process event logs with the help of decision tree classification. The approach is demonstrated by an example of a loan application in a bank.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39426-8_19",
        "year": "2016"
    },
    {
        "title": "Discovering Pattern-Based Mediator Services from Communication Logs",
        "abstract": "Process discovery is a technique for deriving a conceptual high-level process model from the execution logs of a running implementation. The technique is particularly useful when no high-level model is available or in case of significant gaps between process documentation and implementation. The discovered model makes the implementation accessible to various kinds of analysis for functional and non-functional properties. In this paper we extend process discovery to mediator services (or adapters) which adapt the messaging protocols of 2 or more otherwise incompatible services. We propose a technique that takes as input logs of communication behaviors‚Äîone log for each service connected to the adapter‚Äîand a library of high-level data transformation rules relevant for the domain of the adapter, and then returns an operational adapter model describing the control-flow and the data flow of the adapter in terms of Coloured Petri Nets ‚Äì if such model exists. We discuss benefits and limitations of this idea and evaluate it with a prototype implementation on industrial size models.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-06859-6_11",
        "year": "2014"
    },
    {
        "title": "Discovering Petri Nets from Event Logs",
        "abstract": "As information systems are becoming more and more intertwined with the operational processes they support, multitudes of events are recorded by today‚Äôs information systems. The goal of process mining is to use such event data to extract process related information, e.g., to automatically discover a process model by observing events recorded by some system or to check the conformance of a given model by comparing it with reality. In this article, we focus on process discovery, i.e., extracting a process model from an event log. We focus on Petri nets as a representation language, because of the concurrent and unstructured nature of real-life processes. The goal is to introduce several approaches to discover Petri nets from event data (notably the Œ±-algorithm, state-based regions, and language-based regions). Moreover, important requirements for process discovery are discussed. For example, process mining is only meaningful if one can deal with incompleteness (only a fraction of all possible behavior is observed) and noise (one would like to abstract from infrequent random behavior). These requirements reveal significant challenges for future research in this domain.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-38143-0_10",
        "year": "2013"
    },
    {
        "title": "Discovering process models from unlabelled event logs",
        "abstract": "Existing process mining techniques are able to discover process models from event logs where each event is known to have been produced by a given process instance. In this paper we remove this restriction and address the problem of discovering the process model when the event log is provided as an unlabelled stream of events. Using a probabilistic approach, it is possible to estimate the model by means of an iterative Expectaction‚ÄìMaximization procedure. The same procedure can be used to find the case id in unlabelled event logs. A series of experiments show how the proposed technique performs under varying conditions and in the presence of certain workflow patterns. Results are presented for a running example based on a technical support process.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-03848-8_11",
        "year": "2009"
    },
    {
        "title": "Discovering Queues from Event Logs with Varying Levels of Information",
        "abstract": "Detecting and measuring resource queues is central to business process optimization. Queue mining techniques allow for the identification of bottlenecks and other process inefficiencies, based on event data. This work focuses on the discovery of resource queues. In particular, we investigate the impact of available information in an event log on the ability to accurately discover queue lengths, i.e. the number of cases waiting for an activity. Full queueing information, i.e. timestamps of enqueueing and exiting the queue, makes queue discovery trivial. However, often we see only the completions of activities. Therefore, we focus our analysis on logs with partial information, such as missing enqueueing times or missing both enqueueing and service start times. The proposed discovery algorithms handle concurrency and make use of statistical methods for discovering queues under this uncertainty. We evaluate the techniques using real-life event logs. A thorough analysis of the empirical results provides insights into the influence of information levels in the log on the accuracy of the measurements.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-42887-1_13",
        "year": "2016"
    },
    {
        "title": "Discovering Stochastic Petri Nets with Arbitrary Delay Distributions from Event Logs",
        "abstract": "Capturing the performance of a system or business process as accurately as possible is important, as models enriched with performance information provide valuable input for analysis, operational support, and prediction. Due to their computationally nice properties, memoryless models such as exponentially distributed stochastic Petri nets have earned much attention in research and industry. However, there are cases when the memoryless property is clearly not able to capture process behavior, e.g., when dealing with fixed time-outs.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-06257-0_2",
        "year": "2014"
    },
    {
        "title": "Discovering Structured Event Logs from Unstructured Audit Trails for Workflow Mining",
        "abstract": "Workflow mining aims to find graph-based process models based on activities, emails, and various event logs recorded in computer systems. Current workflow mining techniques mainly deal with well-structured and -symbolized event logs. In most real applications where workflow management software tools are not installed, these structured and symbolized logs are not available. Instead, the artifacts of daily computer operations may be readily available. In this paper, we propose a method to map these artifacts and content-based logs to structured logs so as to bridge the gap between the unstructured logs of real life situations and the status quo of workflow mining techniques. Our method consists of two tasks: discovering workflow instances and activity types. We use a clustering method to tackle the first task and a classification method to tackle the second. We propose a method to combine these two tasks to improve the performance of two as a whole. Experimental results on simulated data show the effectiveness of our method.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-04125-9_47",
        "year": "2009"
    },
    {
        "title": "Discovering User Communities in Large Event Logs",
        "abstract": "The organizational perspective of process mining supports the discovery of social networks within organizations by analyzing event logs recorded during process execution. However, applying these social network mining techniques to real data generates very complex models that are hard to analyze and understand. In this work we present an approach to overcome these difficulties by focusing on the discovery of communities from such event logs. The clustering of users into communities allows the analysis and visualization of the social network at different levels of abstraction. The proposed approach also makes use of the concept of modularity, which provides an indication of the best division of the social network into community clusters. The approach was implemented in the ProM framework and it was successfully applied in the analysis of the emergency service of a medium-sized hospital.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-28108-2_11",
        "year": "2012"
    },
    {
        "title": "Discovering workflow transactional behavior from event-based log",
        "abstract": "Previous workflow mining works have concentrated their efforts on process behavioral aspects. Although powerful, these proposals are found lacking in functionalities and performance when used to discover transactional workflow that cannot be seen at the level of behavioral aspects of workflow. Their limitations mainly come from their incapacity to discover the transactional dependencies between process activities, or activities transactional properties. In this paper, we describe mining techniques, which are able to discover a workflow model, and to improve its transactional behavior from event logs. We propose an algorithm to discover workflow patterns and workflow termination states (WTS). Then based on the discovered control flow and set of termination states, we use a set of rules to mine the workflow transactional behavior.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-540-30468-5_3",
        "year": "2004"
    },
    {
        "title": "Discovering WWW User Interface Problems Via User Surveys and Log File Analysis",
        "abstract": "A complex multi-user web site was designed to support industrial experience projects for final year computing students at Monash University. The site was accessed by students, clients, supervisors and subject coordinators. A consistent interface was provided to aid site navigation and ease cognitive overhead, with a fixed navigation bar on all pages seen by all user types. Multiple usability surveys, including provision for free comments, indicated only moderately good usability of the site, but gave no indication as to how usability could be improved. A dedicated server log file was created to record user navigation on the site. Analysis of this log file showed a highly inefficient pattern of student navigation, in which almost every second page access was to the home page. The original site design was found to be inadvertently biased towards the set of staff users who were also its developers. This raised two interesting issues ‚Äî attempts at consistency by the inclusion of a fixed navigation bar for all users had resulted in a highly inefficient design. In addition, questionnaires to large numbers of final year computing students failed to identify this basic design flaw, although subsequent focussed interviews confirmed students experienced navigation as a problem. A new interface has been designed and is being trialled in 2003, along with a changed evaluation method and more sophisticated log file analysis.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4757-4852-9_43",
        "year": "2004"
    },
    {
        "title": "Discovery of Frequent Episodes in Event Logs",
        "abstract": "Lion‚Äôs share of process mining research focuses on the discovery of end-to-end process models describing the characteristic behavior of observed cases. The notion of a process instance (i.e., the case) plays an important role in process mining. Pattern mining techniques (such as traditional episode mining, i.e., mining collections of partially ordered events) do not consider process instances. In this paper, we present a new technique (and corresponding implementation) that discovers frequently occurring episodes in event logs, thereby exploiting the fact that events are associated with cases. Hence, the work can be positioned in-between process mining and pattern mining. Episode Discovery has its applications in, amongst others, discovering local patterns in complex processes and conformance checking based on partial orders. We also discover episode rules to predict behavior and discover correlated behaviors in processes, and apply our technique to other perspectives present in event logs. We have developed a ProM plug-in that exploits efficient algorithms for the discovery of frequent episodes and episode rules. Experimental results based on real-life event logs demonstrate the feasibility and usefulness of the approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-27243-6_1",
        "year": "2015"
    },
    {
        "title": "Discovery of Fuzzy DMN Decision Models from Event Logs",
        "abstract": "Successful business process management is highly dependent on effective decision making. The recent Decision Model and Notation (DMN) standard prescribes decisions to be documented and executed complementary to processes. However, the decision logic is often implicitly contained in event logs, and ‚Äúas-is‚Äù decision knowledge needs to be retrieved. Commonly, decision logic is represented by rules based on Boolean algebra. The formal nature of such decisions is often hard for interpretation and utilization in practice, because imprecision is intrinsic to real-life decisions. Operations research considers fuzzy logic, based on fuzzy algebra, as a tool dealing with partial knowledge. In this paper, we explore the possibility of incorporating fuzziness into DMN decision models. Further, we propose a methodology for discovering fuzzy DMN decision models from event logs. The evaluation of our approach on a use case from the banking domain shows high comprehensibility and accuracy of the output decision model.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-59536-8_39",
        "year": "2017"
    },
    {
        "title": "Discovery of user preferred access patterns from web logs",
        "abstract": "Web users' interest and preference can be revealed by such factors as access frequencies and time durations of web pages, in which time durations on a web page are denoted by a fuzzy linguistic variable. Thus, a novel concept of fuzzy preference considering the relative access frequency and time durations of a web page is proposed to be along with another concept of support considering the absolute access frequency of a web page to measure web user interest and preference. According to these two concepts, an algorithm is developed for mining user interesting/preferred access patterns from Frequent Link and Access Tree (FLaAT), which stores all user access information extracted from web logs.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6019511/",
        "year": "2011"
    },
    {
        "title": "Discovery of Web frequent patterns and user characteristics from Web access logs: a framework for dynamic Web personalization",
        "abstract": "An automatic discovery method that discovers frequent access routines for unique clients from Web access log files is presented. The proposed algorithm develops novel techniques to extract the sets of all predictive access sequences from semi-structured Web access logs. Important user access patterns are manifested through the frequent traversal paths, thus helping to understand user surfing behaviors. The predictive access routines discovered by AllFreSeq are also useful for understanding and improving Web site domain tree.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/888025/",
        "year": "2000"
    },
    {
        "title": "Discovery of Web pattern from web logs files using enhanced graph grammar approach",
        "abstract": "Searching useful information without fault from the Web becomes an increasingly difficult task, since the volume of Web data rapidly grows. With the growth rate, unexpected faults of Web service composition may occur in different levels at runtime. These faults are to be identified from Web Log files. The common causes of faults in Web services execution are rectified by fault diagnosis technique. So far, most existing approaches focus on the log content analysis but ignore the structural information and lead to poor performance. To improve the fault classification accuracy, fault classification analysis is carried out in this proposal. The Enhanced graph grammar algorithm is incorporated for identifying different types of fault categories in the form of graph structures.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8070753/",
        "year": "2017"
    },
    {
        "title": "Distinguishing humans from robots in web search logs: preliminary results using query rates and intervals",
        "abstract": "The workload on web search engines is actually multiclass, being derived from the activities of both human users and automated robots. It is important to distinguish between these two classes in order to reliably characterize human web search behavior, and to study the effect of robot activity. We suggest an approach based on a multi-dimensional characterization of search sessions, and take first steps towards implementing it by studying the interaction between the query submittal rate and the minimal interval of time between different queries.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1507512",
        "year": "2009"
    },
    {
        "title": "Do certified tropical logs fetch a market premium?: A comparative price analysis from Sabah, Malaysia",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1389934106001018",
        "year": "2007"
    },
    {
        "title": "Dynamic Aggregation to Support Pattern Discovery: A Case Study with Web Logs",
        "abstract": "Rapid growth of digital data collections is overwhelming the capabilities of humans to comprehend them without aid. The extraction of useful data from large raw data sets is something that humans do poorly. Aggregation is a technique that extracts important aspect from groups of data thus reducing the amount that the user has to deal with at one time, thereby enabling them to discover patterns, outliers, gaps, and clusters. Previous mechanisms for interactive exploration with aggregated data were either too complex to use or too limited in scope. This paper proposes a new technique for dynamic aggregation that can combine with dynamic queries to support most of the tasks involved in data manipulation.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/3-540-45650-3_42",
        "year": "2001"
    },
    {
        "title": "Dynamic parity logging disk arrays for engineering database systems",
        "abstract": "RAID (redundant arrays of inexpensive disks) has gained much attention in the recent development of fast I/O systems. Of the five levels, the traditional mirrored disk array still provides the highest I/O rate for small 'write' transfers. This is because the mirrored disk array does not have the 'small write problem' which is found in other levels of RAID. The authors propose a novel RAID architecture for fast engineering database systems, called a dynamic parity logging (DPL) disk array. A DPL disk array does not have the 'small write problem' and can provide much higher throughput than other RAID architectures. The DPL disk array also has a journalling capability, which means that some older design versions are kept for future reference. A queueing model for the DPL disk array is built. Analytical results, supported by simulation, show that the DPL disk array can provide higher 'write' throughput when compared to RAID levels 1, 4, and 5.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/627902/",
        "year": "1997"
    },
    {
        "title": "EClog: A handheld eddy covariance logging system",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0168169906000123",
        "year": "2006"
    },
    {
        "title": "Eeg measurements towards brain life-log system in outdoor environment",
        "abstract": "In this paper, we studied electroencephalogram during ambulatory conditions in outdoor environment. Five healthy subjects participated in this experiment. The task of the self-paced walking subjects was to count the number of appearances of the target auditory stimulus using oddball paradigm. We observed P300 evoked potentials in ambulatory conditions in outdoor environment as well as sitting conditions in indoor environment. Our results are encouraging and make new direction to promising novel applications of ambulatory BCIs.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-22095-1_63",
        "year": "2011"
    },
    {
        "title": "Effective web log mining and online navigational pattern prediction",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0950705113001263",
        "year": "2013"
    },
    {
        "title": "Efficient Auto-Increment Keys Generation for Distributed Log-Structured Storage Systems",
        "abstract": "Recent years, writing-intensive workloads on big data make log-structured style storage popular in distributed data storage systems, which provides both large-volume storage capacity and high-performance data updates. Rapidly generating valid keys for append records can significantly improve the data write performance of log-structured storage systems. In distributed and high concurrency environment, however, both the huge disk IO and the interaction overhead of a traditional lock manager limit the transactional throughput for generating auto-increment keys. In this paper, we design an efficient auto-increment keys generation manager (AKGM), a memory management structure that cannot only avoid disk IO but also eliminate the interaction overhead of traditional lock manager for transactions of generating auto-increment keys. We also propose a protocol called adaptive batch processing (ABP), which enables systems implementing AKGM to achieve high transactional throughput even under high contention workloads. We implement these protocols in an open-source database based on log-structured storage, and our experimental results show the superior performance of AKGM and ABP.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-02925-8_16",
        "year": "2018"
    },
    {
        "title": "Efficient Detection of Consecutive Facial Expression Apices Using Biologically Based Log-Normal Filters",
        "abstract": "The automatic extraction of the most relevant information in a video sequence made of continuous affective states is an important challenge for efficient human-machine interaction systems. In this paper a method is proposed to solve this problem based on two steps: first, the automatic segmentation of consecutive emotional segments based on the response of a set of Log-Normal filters; secondly, the automatic detection of the facial expression apices based on the estimation of the global face energy inside each emotional segment independently of the undergoing facial expression. The proposed method is fully automatic and independent from any reference image such as the neutral at the beginning of the sequence. The proposed method is the first contribution for the summary of the most important affective information present in a video sequence independently of the undergoing facial expressions. The robustness and efficiency of the proposed method to different data acquisition and facial differences has been evaluated on a large set of data (157 video sequences) taken from two benchmark databases (Hammal-Caplier and MMI databases) [1, 2] and from 20 recorded video sequences of multiple facial expressions (between three to seven facial expressions per sequence) in order to include more challenging image data in which expressions are not neatly packaged in neutral-expression-neutral.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-24028-7_54",
        "year": "2011"
    },
    {
        "title": "Efficient discovery of understandable declarative process models from event logs",
        "abstract": "Process mining techniques often reveal that real-life processes are more variable than anticipated. Although declarative process models are more suitable for less structured processes, most discovery techniques generate conventional procedural models. In this paper, we focus on discovering Declare models based on event logs. A Declare model is composed of temporal constraints. Despite the suitability of declarative process models for less structured processes, their discovery is far from trivial. Even for smaller processes there are many potential constraints. Moreover, there may be many constraints that are trivially true and that do not characterize the process well. Naively checking all possible constraints is computationally intractable and may lead to models with an excessive number of constraints. Therefore, we have developed an Apriori algorithm to reduce the search space. Moreover, we use new metrics to prune the model. As a result, we can quickly generate understandable Declare models for real-life event logs.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-31095-9_18",
        "year": "2012"
    },
    {
        "title": "Efficient Event-Driven Forward Kinematics of Open Kinematic Chains with O(Log n) Complexity",
        "abstract": "This paper presents novel event-driven forward kinematics algorithms for open kinematic chains with O(log n) complexity. This event-driven algorithm can efficiently update forward kinematics only when new sensory data comes. This will also contribute to localization of computational resources at sensitive joints to the position of the endpoint (e.g. a fingertip), like a root joint. We constructed 3 event-driven FK algorithms. We proved that the algorithms have the complexity of O(logn) for updating 1 joint angle, and O(logn) for obtaining a homogeneous transformation matrix between links. We compared the 3 algorithms with a conventional forward kinematics algorithm in the viewpoint of complexity, computation time, time-variance and algebraic structures. The results showed that the computation time is well adequate for real-time computation. Computation time is less than 2 us per 1 query, for 40,000 kinematic chains.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8461211/",
        "year": "2018"
    },
    {
        "title": "Efficient Indexing and Representation of Web Access Logs",
        "abstract": "We present a space-efficient data structure, based on the Burrows-Wheeler Transform, especially designed to handle web sequence logs, which are needed by web usage mining processes. Our index is able to process a set of operations efficiently, while at the same time maintains the original information in compressed form. Results show that web access logs can be represented using 0.85 to 1.03 times their original (plain) size, while executing most of the operations within a few tens of microseconds.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-11918-2_7",
        "year": "2014"
    },
    {
        "title": "Efficient logging of metadata using NVRAM for NAND flash based file system",
        "abstract": "The synchronous writing of metadata for flash file system generates excessive garbage. We propose the scheme for merging the writing of metadata so as to reduce the garbage of the NAND flash while ensuring file system consistency. The proposed scheme uses the non-volatile memory for synchronously logging modifications of the metadata. The last modified metadata can be recovered from a crash, after scanning logs in the non-volatile memory. The evaluation results show that the proposed scheme greatly reduced the overall application time and the number of written pages across various benchmarks, compared to the conventional flash file system.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6161940/",
        "year": "2012"
    },
    {
        "title": "Efficient Web Logs Stair-Case Technique to Improve Hit Ratios of Caching",
        "abstract": "Cache prefetching technique can improve the hit ratio and expedite users visiting speed. Predictive Web prefetching refers to the mechanism of deducing the forth coming page accesses of a client based on its past accesses.Congestion in Network remains one of the main barriers to the continuing success of the Internet. For Web users, congestion manifests itself in unacceptably long response times. One possible remedy to the latency problem is to use caching at the client, at the proxy server, or within the Internet. However, Web documents are becoming increasingly dynamic, which limits the potential benefit of caching. The performance of a Web caching system can be dramatically increased by integrating document prefetching into its design. Although prefetching reduces the response time of a requested document, it also increases the network load, as some documents will be unnecessarily prefetched.In the paper, we developed a Stair-Case prune algorithm to mine popular with their conditional probabilities from the proxy log, and stored them in the rule table. Then, according to contents and the rule table, a prediction is calculated in some precondition. After the simulation, we found that our approach has much better performance than the other ones, in terms of hit ratio.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-17881-8_17",
        "year": "2011"
    },
    {
        "title": "Empty handed? A material availability study and transaction log analysis verification",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0099133398901045",
        "year": "1998"
    },
    {
        "title": "End user searching: A Web log analysis of NAVER, a Korean Web search engine",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0740818805000083",
        "year": "2005"
    },
    {
        "title": "Enforcing Data Quality Rules for a Synchronized VM Log Audit Environment Using Transformation Mapping Techniques",
        "abstract": "In this paper we examine the transformation mapping mechanisms required in synchronizing virtual machine (VM) log audit data for the system administrator environment. We explain the formal constraints that are required by the transformation mapping process between the source and target log schemas for these VMs. We discuss the practical considerations of using these formalisms in establishing the suitable data quality rules that provides for security within these abstract domains.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-21323-6_34",
        "year": "2011"
    },
    {
        "title": "Engineering Software for the Cloud: Messaging Systems and Logging",
        "abstract": "Software business continues to expand globally, highly motivated by the reachability of the Internet and possibilities of Cloud Computing. While widely adopted, development for the cloud has some intrinsic properties to it, making it complex to any newcomer. This research is capturing those intricacies using a pattern catalog, with this paper contributing with three of those patterns: Messaging System, a message bus for abstracting service placement in a cluster and orchestrating messages between multiple services; Preemptive Logging, a design principle where services and servers continuously output relevant information to log files, making them available for later debugging failures; and Log Aggregation, a technique to aggregate logs from multiple services and servers in a centralized location, which indexes and provides them in a queryable, user friendly format. These patterns are useful for anyone designing software for the cloud, either to guide or validate their design decisions.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3147720",
        "year": "2017"
    },
    {
        "title": "Enhancement of Hybrid Concatenated Codes Using A Modified Log-MAP Algorithm",
        "abstract": "To improve the performance of hybrid concatenated convolutional codes (HCCC); a modified Log-MAP algorithm and an enhanced HCCC are introduced and demonstrated to be efficient and practical by simulation results. The new coding scheme achieves about 1.0 dB additional coding gain, compared to the general turbo coding scheme at a BER = 10-6, with a frame length of 8192-bit. The system complexity and decoding latency of the new scheme is lower than the HCCC proposed by Divsalar and Pollara (1997) within acceptable performance degradation. Since the bit error-rate of the proposed HCCC can be dramatically reduced by slightly increasing signal-to-noise ratio, the new hybrid concatenated coding scheme is very suitable for those communication environments in which high reliability is important.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4603721/",
        "year": "2008"
    },
    {
        "title": "Enhancing Source Selection for Live Queries over Linked Data via Query Log Mining",
        "abstract": "Traditionally, Linked Data query engines execute SPARQL queries over a materialised repository which on the one hand, guarantees fast query answering but on the other hand requires time and resource consuming preprocessing steps. In addition, the materialised repositories have to deal with the ongoing challenge of maintaining the index which is ‚Äì given the size of the Web ‚Äì practically unfeasible. Thus, the results for a given SPARQL query are potentially out-dated. Recent approaches address the result freshness problem by answering a given query directly over dereferenced query relevant Web documents. Our work investigate the problem of an efficient selection of query relevant sources under this context. As a part of query optimization, source selection tries to estimate the minimum number of sources accessed in order to answer a query. We propose to summarize and index sources based on frequently appearing query graph patterns mined from query logs. We verify the applicability of our approach and empirically show that our approach significantly reduces the number of relevant sources estimated while keeping the overhead low.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-29923-0_12",
        "year": "2012"
    },
    {
        "title": "Enriching web taxonomies through subject categorization of query terms from search engine logs",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0167923602000994",
        "year": "2003"
    },
    {
        "title": "Entropic Trace Estimates for Log Determinants",
        "abstract": "The scalable calculation of matrix determinants has been a bottleneck to the widespread application of many machine learning methods such as determinantal point processes, Gaussian processes, generalised Markov random fields, graph models and many others. In this work, we estimate log determinants under the framework of maximum entropy, given information in the form of moment constraints from stochastic trace estimation. The estimates demonstrate a significant improvement on state-of-the-art alternative methods, as shown on a wide variety of matrices from the SparseSuite Matrix Collection. By taking the example of a general Markov random field, we also demonstrate how this approach can significantly accelerate inference in large-scale learning methods involving the log determinant.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-71249-9_20",
        "year": "2017"
    },
    {
        "title": "ERP Event Log Preprocessing: Timestamps vs. Accounting Logic",
        "abstract": "Process mining has been gaining significant attention in academia and practice. A promising first step to apply process mining in the audit domain was taken with the mining of process instances from accounting data. However, the resulting process instances constitute graphs. Commonly, timestamp oriented event log formats require a sequential list of activities and do not support graph structures. Thus, event log based process mining techniques cannot readily be applied to accounting data. To close this gap, we present an algorithm that determines an activity sequence from accounting data. With this algorithm, mined process instance graphs can be decomposed in a way they fit into sequential event log formats. Event log based process mining techniques can then be used to construct process models. A case study demonstrates the effectiveness of the presented approach. Results reveal that the preprocessing of the event logs considerably improves the derived process models.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-38827-9_8",
        "year": "2013"
    },
    {
        "title": "ESTI-LOG PV plant monitoring system",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0927024897000512",
        "year": "1997"
    },
    {
        "title": "Estimating rates of rare events with multiple hierarchies through scalable log-linear models",
        "abstract": "We consider the problem of estimating rates of rare events for high dimensional, multivariate categorical data where several dimensions are hierarchical. Such problems are routine in several data mining applications including computational advertising, our main focus in this paper. We propose LMMH, a novel log-linear modeling method that scales to massive data applications with billions of training records and several million potential predictors in a map-reduce framework. Our method exploits correlations in aggregates observed at multiple resolutions when working with multiple hierarchies; stable estimates at coarser resolution provide informative prior information to improve estimates at finer resolutions. Other than prediction accuracy and scalability, our method has an inbuilt variable screening procedure based on a \"spike and slab prior\" that provides parsimony by removing non-informative predictors without hurting predictive accuracy. We perform large scale experiments on data from real computational advertising applications and illustrate our approach on datasets with several billion records and hundreds of millions of predictors. Extensive comparisons with other benchmark methods show significant improvements in prediction accuracy.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1835834",
        "year": "2010"
    },
    {
        "title": "Estimation Models of User Skills Based on Web Search Logs",
        "abstract": "The number of Internet users has been increasing in Japan, especially the elderly. Even though there are differences in the skills of the elderly, no effective method of personalizing the user interface to suit skill level has been proposed. To solve this problem, conventionally, questionnaires or tests are used to evaluate user skills, but users find them burdensome. In order to evaluate user skills automatically, we focus on the logs of user operations. This study uses machine learning to build models that can estimate skill level from operation logs on tablet PC. First, we investigate and identify the 6 key skills necessary for effective Web search. Second, the skill evaluation tasks and the Web search tasks are created and then performed by elderly users. During the Web search tasks, the operation logs such as screen touch behavior are gathered by the Web browser. Finally, decision tree-based estimation models of the 6 skills are built. The results confirm that models can very accurately estimate skill level.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-40542-1_9",
        "year": "2016"
    },
    {
        "title": "Estimation of Average Latent Waiting and Service Times of Activities from Event Logs",
        "abstract": "Analysis of performance is crucial in the redesign phase of business processes where bottlenecks are identified from the average waiting and service times of activities and resources in business processes. However, such averages of waiting and service times are not readily available in most event logs that only record either the start or the completion times of events in activities. The transition times between events in such logs are the only performance features that are available. This paper proposes a novel method of estimating the average latent waiting and service times from the transition times that employs the optimization of the likelihood of the probabilistic model with expectation and maximization (EM) algorithms. Our experimental results indicated that our method could estimate the average latent waiting and service times with sufficient accuracy to enable practical applications through performance analysis.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-23063-4_11",
        "year": "2015"
    },
    {
        "title": "Evaluating Web software reliability based on workload and failure data extracted from server logs",
        "abstract": "We characterize usage and problems for Web applications, evaluate their reliability, and examine the potential for reliability improvement. Based on the characteristics of Web applications and the overall Web environment, we classify Web problems and focus on the subset of source content problems. Using information about Web accesses, we derive various measurements that can characterize Web site workload at different levels of granularity and from different perspectives. These workload measurements, together with failure information extracted from recorded errors, are used to evaluate the operational reliability for source contents at a given Web site and the potential for reliability improvement. We applied this approach to the Web sites www.seas.smu.edu and www.kde.org. The results demonstrated the viability and effectiveness of our approach.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/1359769/",
        "year": "2004"
    },
    {
        "title": "Evaluation of standard approximation to log-likelihood ratio addition in the MAP algorithm, and Its application in block code (‚ÄòTurbo‚Äô) Iterative decoding algorithms",
        "abstract": "This paper examines the log-likelihood addition aspects of the Turbo Decoding of product codes. Simple Parity Check (SPC) codes are used as the Constituent Codes (CCs) of the Turbo encoder in an array code format. The general approximation used in the log-likelihood addition of the soft values of the code bits is evaluated and some results are presented. Two iterative (‚ÄúTurbo‚Äô) decoding algorithms for block codes are compared, only one of which uses the previously mentioned approximation. The first is set out in Hagenauer (1994), which uses the dual code method to implement the MAP decoder, and the second was developed in Pyndiah (1994), which uses an algorithm presented in Chase (1972). The results presented show the affect of the approximation on decoder performance. The outcome differs somewhat from that which was expected.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/BFb0024469",
        "year": "1997"
    },
    {
        "title": "Event Log Cleaning for Business Process Analytics",
        "abstract": "Event log preprocessing; Event log repair",
        "include": false,
        "url": "http://link.springer.com/referenceworkentry/10.1007/978-3-319-63962-8_87-1",
        "year": "2018"
    },
    {
        "title": "Extending Log-Based Affect Detection to a Multi-User Virtual Environment for Science",
        "abstract": "The application of educational data mining (EDM) techniques to interactive learning software is increasingly being used to broaden the range of constructs typically incorporated in student models, moving from traditional assessment of student knowledge to the assessment of engagement, affect, strategy, and metacognition. Researchers are also broadening the range of environments within which these constructs are assessed. ¬†In this study, we develop sensor-free affect detection for EcoMUVE, an immersive multi-user virtual environment that teaches middle-school students about casualty in ecosystems.¬†In this study, models were constructed for five different educationally-relevant affective states (boredom, confusion, delight, engaged concentration, and frustration). Such models allow us to examine the behaviors most closely associated with particular affective states, paving the way for the design of adaptive personalization to improve engagement and learning.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-08786-3_25",
        "year": "2014"
    },
    {
        "title": "Face recognition by subspace analysis of 2D Log-Gabor wavelets features",
        "abstract": "In this paper, we discuss a face recognition scheme by subspace analysis of 2D log-Gabor wavelets features. In which, an input face image is firstly decomposed with a set of two dimensional log-Gabor wavelets (2D-LGWs) localized with respect to spatial location, orientation and frequency. Based on complex responses of filters, local energy model (LEM) is used to represent log-Gabor features (LGFs) which are substantially effective for the task of recognition. Then, subspace modeling is performed to transform the high dimensional LGFs into more compact one to simplify the task of classification. Common nearest-neighbor (NN) based matching algorithm is adopted to classify a probe to one of classes. The superiority of the proposed scheme for face recognition is comparatively demonstrated with the traditional appearance-based methods. Moreover, performances of several leading subspace techniques, PCA, ICA and LDA, are comparatively evaluated based on LGFs representation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4731107/",
        "year": "2008"
    },
    {
        "title": "Face‚Äìiris multi-modal biometric system using multi-resolution Log-Gabor filter with spectral regression kernel discriminant analysis",
        "abstract": "A multi-modal biometric system is used to verify or identify a person by exploiting information of more than one biometric modality. It combines the strengths of the unimodal biometric system to solve their limitations. This study proposes schemes of multi-modal biometric system based on texture information extracted from face and two iris (left and right) using hybrid level of fusion. Feature extraction is the key step to get a robust recognition system. Multi-resolution two-dimensional Log-Gabor filter combined with spectral regression kernel discriminant analysis is exploited to extract features from both face and iris modalities. These features are used in the fusion and the classification process. The proposed schemes were tested using CASIA Iris Distance database in the verification mode. The experimental results show that the proposed multi-modal biometric system yields attractive performances of up to 0.24% in terms of equal error rate and outperforms the recent similar existing state-of-the-art methods.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8440882/",
        "year": "2018"
    },
    {
        "title": "Facet discovery for structured web search: a query-log mining approach",
        "abstract": "In recent years, there has been a strong trend of incorporating results from structured data sources into keyword-based web search systems such as Bing or Amazon. When presenting structured data, facets are a powerful tool for navigating, refining, and grouping the results. For a given structured data source, a fundamental problem in supporting faceted search is finding an ordered selection of attributes and values that will populate the facets. This creates two sets of challenges. First, because of the limited screen real-estate, it is important that the top facets best match the anticipated user intent. Second, the huge scale of available data to such engines demands an automated unsupervised solution.In this paper, we model the user faceted-search behavior using the intersection of web query-logs with existing structured data. Since web queries are formulated as free-text queries, a challenge in our approach is the inherent ambiguity in mapping keywords to the different possible attributes of a given entity type. We present an automated solution that elicits user preferences on attributes and values, employing different disambiguation techniques ranging from simple keyword matching, to more sophisticated probabilistic models. We demonstrate experimentally the scalability of our solution by running it on over a thousand categories of diverse entity types and measure the facet quality with a real-user study.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1989342",
        "year": "2011"
    },
    {
        "title": "Facial Expression Recognition by Independent Log-Gabor Component Analysis",
        "abstract": "Research on facial expression recognition is critical for personalized human-computer interaction (HCI). Recent advances in localized, sparse and discriminative image feature descriptors have been proven to be promising in visual recognition, both statically and dynamically, making it quite useful for facial expression recognition. In this paper we show that the independent Log-Gabor feature (IGF), a localized and sparse representation of pattern of interest, can perform conveniently and satisfactorily for facial expression recognition task. In low-level feature extraction, Log-Gabor wavelet features are extracted, then ICA is applied to produce independent image bases that reduce the redundancy, emphasize edge information, while preserving orientation and scale selection property in the image data. In high-level classification, SVM classifies the propagated independent Log-Gabor features features as discriminative components. We demonstrate our algorithm on facial expression databases for recognition tasks, showing that the proposed method is accurate and more efficient than current approaches.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-21090-7_36",
        "year": "2011"
    },
    {
        "title": "FACT-Graph in Web Log Data",
        "abstract": "This paper describes a visualization of web access log by FACT-Graph. As the increase of large web sites with complex structure, web access logs have a clue to understand visitor activities and the improvement of site structure. For web access log analysis, statistical methods and web usage mining are used to recognize trend of pages and relationships between pages. However, these methods are independently applied, and there are no instances to visualize trends and relationships at the same time. In order to resolve them, we use FACT-Graph which shows the trend and relationships between terms for time-series text data. To apply FACT-Graph, we regard pages and sessions in web access log as terms and documents. In the experiment using 9 million access log at Osaka Prefecture University, we show the feature of access log from the global view by FACT-Graph.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-23866-6_29",
        "year": "2011"
    },
    {
        "title": "Fast Initialization and Memory Management Techniques for Log-Based Flash Memory File Systems",
        "abstract": "Flash memory‚Äôs adoption in the mobile devices is increasing for various multimedia services such as audios, videos, and games. The traditional research issues such as out-place update, garbage collection, and wear-leveling are important, the fast initialization and response time issues of flash memory file system are becoming much more important than ever because flash memory capacity is rapidly increasing. In this paper, we propose a fast initialization technique and an efficient memory management technique for fast response time in log-based flash memory file systems. Our prototype is implemented based on a well-known log-based flash memory file system YAFFS2 and the performance tests were conducted by comparing our prototype with YAFFS2. The experimental results show that the proposed initialization technique reduced the initialization time of the log-based flash memory file system regardless of unmounting the file system properly. Moreover our prototype outperforms YAFFS2 in the read I/O operations and the forward/backward seek I/O operations by way of our proposed memory management technique. This technique is also able to be used to control the memory size required for address mapping in flash memory file systems.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-72685-2_21",
        "year": "2007"
    },
    {
        "title": "Faster Log Analysis and Integration of Security Incidents Using Knuth-Bendix Completion",
        "abstract": "With the rapid popularization of cloud computing, mobile devices and high speed Internet, recent security incidents have become more complicated which imposes a great burden on network administrators. In this paper we propose an integration and simplification method of log strings obtained by many kinds of computer devices: memory, socket and file. Besides, we apply reasoning strategy for term rewriting called as Knuth-Bendix completion algorithm for ensuring termination and confluent. Knuth Bendix completion includes some inference rules such as lrpo (the lexicographic recursive path ordering) and dynamic demodulation. As a result, we can achieve the reduction of generated clauses which result in faster integration of log strings. In experiment, we present the effectiveness of proposed method by showing the result of exploitation of vulnerability and malware‚Äôs behavior log.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-24958-7_4",
        "year": "2011"
    },
    {
        "title": "Feasibility analysis of big log data real time search based on Hbase and ElasticSearch",
        "abstract": "There are a lot of log events generated very day from modern enterprises. Form the pattern of these log data, enterprises can mine business value; it's even better if they can do it in real time. But managing this big log data is a big challenge because traditional technology is not powerful enough to process huge data. Luckily, Hadoop echo system provides a new way to process big data; ElasticSearch, which is based on Lucene, is the modern search engineen for cloud environment. This paper presents a real-time big data search method: First, Flume agent from the end user's machine collect log events, then ElasticSearch according to the search conditions are needed rowkey list; finally Hbase using these rowkey directly from the database to get the data, the paper-based hardware to create a virtual machine environment, the experiment proved, with the search for more log events, the search response time does not increase linearly. This article will explain Hbase based on modern distributed systems and cloud real-time search search engine ElasticSearch feasibility of large data logs.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6818154/",
        "year": "2013"
    },
    {
        "title": "File Defragmentation Scheme for a Log-Structured File System",
        "abstract": "In recent years, many researchers have focused on log-structured file systems (LFS), because it gracefully enhances the random write performance and efficiently resolves the consistency issue. However, the write policy of LFS can cause a file fragmentation problem, which degrades sequential read performance of the file system. In this paper, we analyze the relationship between file fragmentation and the sequential read performance, considering the characteristics of underlying storage devices. We also propose a novel file defragmentation scheme on LFS to effectively address the file fragmentation problem. Our scheme reorders the valid data blocks belonging to a victim segment based on the inode numbers during the cleaning process of LFS. In our experiments, our scheme eliminates file fragmentation by up to 98.5% when compared with traditional LFS.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2967622",
        "year": "2016"
    },
    {
        "title": "Filling the Gaps of Development Logs and Bug Issue Data",
        "abstract": "It has been suggested that the data from bug repositories is not always in sync or complete compared to the logs detailing the actions of developers on source code.In this paper, we trace two sources of information relative to software bugs: the change logs of the actions of developers and the issues reported as bugs. The aim is to identify and quantify the discrepancies between the two sources in recording and storing the developer logs relative to bugs.Focussing on the databases produced by two mining software repository tools, CVSAnalY and Bicho, we use part of the SZZ algorithm to identify bugs and to compare how the \"defects-fixing changes\" are recorded in the two databases. We use a working example to show how to do so.The results indicate that there is a significant amount of information, not in sync when tracing bugs in the two databases. We, therefore, propose an automatic approach to re-align the two databases, so that the collected information is mirrored and in sync.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2641592",
        "year": "2014"
    },
    {
        "title": "Finding Anomalies in SCADA Logs Using Rare Sequential Pattern Mining",
        "abstract": "Pattern mining is a branch of data mining used to discover hidden patterns or correlations among data. We use rare sequential pattern mining to find anomalies in critical infrastructure control networks such as supervisory control and data acquisition (SCADA) networks. As anomalous events occur rarely in a system and SCADA systems‚Äô topology and actions do not change often, we argue that some anomalies can be detected using rare sequential pattern mining. This anomaly detection would be useful for intrusion detection or erroneous behaviour of a system. Although research into rare itemsets mining previously exists, neither research into rare sequential pattern mining nor its applicability to SCADA system anomaly detection has previously been completed. Moreover, since there is no consideration to events order, the applicability to intrusion detection in SCADA is minimal. By ensuring the events‚Äô order is maintained, in this paper, we propose a novel Rare Sequential Pattern Mining (RSPM) technique which is a useful anomaly detection system for SCADA. We compared our algorithm with a rare itemset mining algorithm and found anomalous events in SCADA logs.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-46298-1_32",
        "year": "2016"
    },
    {
        "title": "Finding Critical Thresholds for Defining Bursts in Event Logs",
        "abstract": "A burst, i.e., an unusally high frequency of occurrence of an event in a time-window, is interesting in many monitoring applications that give rise to temporal data as it often indicates an abnormal activity. While the problem of detecting bursts from time-series data has been well addressed, the question of what choice of thresholds, on the number of events as well as on the window size, makes a window ‚Äúunusally bursty‚Äù remains a relevant one. We consider the problem of finding critical values of both these thresholds. Since for most applications, we hardly have any apriori idea of what combination of thresholds is critical, the range of possible values for either threshold can be very large. We formulate finding the combination of critical thresholds as a two-dimensional search problem and design efficient deteministic and randomized divide-and-conquer heuristics. For the deterministic heuristic, we show that under some weak assumptions, the computational overhead is logarithmic in the sizes of the ranges. Under identical assumptions, the expected computational overhead of the randomized heuristic in the worst case is also logarithmic. Using data obtained from logs of medical equipment, we conduct extensive simulations that reinforce our theoretical results, and show that on average, the randomized heuristic beats its deteministic counterpart in practice.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-37574-3_4",
        "year": "2013"
    },
    {
        "title": "Finding generalized path patterns for web log data mining",
        "abstract": "Conducting data mining on logs of web servers involves the determination of frequently occurring access sequences. We examine the problem of finding traversal patterns from web logs by considering the fact that irrelevant accesses to web documents may be interleaved within access patterns due to navigational purposes. We define a general type of pattern that takes into account this fact and also, we present a level-wise algorithm for the determination of these patterns, which is based on the underlying structure of the web site. The performance of the algorithm and its sensitivity to several parameters is examined experimentally with synthetic data.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/3-540-44472-6_17",
        "year": "2000"
    },
    {
        "title": "Finding Process Variants in Event Logs",
        "abstract": "The analysis of event data is particularly challenging when there is a lot of variability. Existing approaches can detect variants in very specific settings (e.g., changes of control-flow over time), or do not use statistical testing to decide whether a variant is relevant or not. In this paper, we introduce an unsupervised and generic technique to detect significant variants in event logs by applying existing, well-proven data mining techniques for recursive partitioning driven by conditional inference over event attributes. The approach has been fully implemented and is freely available as a ProM plugin. Finally, we validated our approach by applying it to a real-life event log obtained from a multinational Spanish telecommunications and broadband company, obtaining valuable insights directly from the event data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-69462-7_4",
        "year": "2017"
    },
    {
        "title": "Firewall log analysis and dynamic rule re-ordering in firewall policy anomaly management framework",
        "abstract": "Today, there are more many ways to communicate than there were just a few years ago and among them, internet plays a major role. Firewalls are essential for a secure network communication to ensure that only trusted packets are transferred between the private and public network. In firewall, security policy is implemented based on the rules defined by the network administrator; that decides which packets can be allowed to an organization's private network. Manual definition of rules often results in anomalies in the policy. Therefore, an effective anomaly detection and resolution approach is needed. After resolving these conflicts, the rules can be re-ordered dynamically that improves the efficiency of the anomaly management framework. With firewall log analysis, frequently used rules can be set as primitive rules, to which more security can be added.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6823553/",
        "year": "2013"
    },
    {
        "title": "Flow-based identification of botnet traffic by mining multiple log files",
        "abstract": "Botnet detection and disruption has been a major research topic in recent years. One effective technique for botnet detection is to identify Command and Control (C\u0026C) traffic, which is sent from a C\u0026C center to infected, hosts (bots) to control the bots. If this traffic can be detected, both the C\u0026C center and the bots it controls can be detected, and the botnet can be disrupted. We propose a multiple log-file based temporal correlation technique for detecting C\u0026C traffic. Our main assumption is that bots respond much faster than humans. By temporally correlating two host-based log files, we are able to detect this property and thereby detect bot activity in a host machine. In our experiments we apply this technique to log files produced by tcpdump and exedump, which record all incoming and outgoing network packets, and the start times of application executions at the host machine, respectively. We apply data mining to extract relevant features from these log files and detect C\u0026C traffic. Our experimental results validate our assumption and show better overall performance when compared to other recently published techniques.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/4784437/",
        "year": "2008"
    },
    {
        "title": "Flow-based Worm Detection using Correlated Honeypot Logs",
        "abstract": "Attack detection in high-speed networks is a hot research topic. While the performance of packet oriented signature-based approaches is questionable, flow-based anomaly detection shows high false positive rates. We tried to combine both techniques. In this paper, we study the applicability of flow-based attack detection. We installed a lab environment consisting of a monitoring infrastructure and a well-controlled honeypot. Using correlated honeypot logs and flow signatures, we created a first set of attack pattern. The evaluation of the approach was done within our university network. On the positive side, we were able to prove the successful detection of worm attacks. Problems can occur if incomplete monitoring data is used.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5755487/",
        "year": "2007"
    },
    {
        "title": "Frame rate distributed computing for log-polar images with a novel real-time operating system on a general purpose platform",
        "abstract": "This paper presents our results in using ART, a real-time extension for Linux, in the context of a visual attention model implementation using log-polar images. ART allows periodic computing in the user space, suitable for frame-rate image processing, and motor control at 200 Hz. Integrated with PredN (parallel real-time event and data-driven network), a development tool for complex robotic applications, it provides a low cost and user-friendly environment to realize distributed image processing in real-time.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/949813/",
        "year": "2001"
    },
    {
        "title": "Frequency Domain Analysis of Large-Scale Proxy Logs for Botnet Traffic Detection",
        "abstract": "Botnets have become one of the most significant cyber threats over the last decade. The diffusion of the \"Internet of Things\" and its for-profit exploitation, contributed to botnets spread and sophistication, thus providing real, efficient and profitable criminal cyber-services. Recent research on botnet detection focuses on traffic pattern-based detection, and on analyzing the network traffic generated by the infected hosts, in order to find behavioral patterns independent from the specific payloads, architectures and protocols. In this paper we address the periodic behavioral patterns of infected hosts communicating with their Command-and-Control servers. The main novelty introduced is related to the traffic analysis in the frequency domain without using the well-known Fast Fourier Transform. Moreover, the mentioned analysis is performed through the exploitation of the proxy logs, easily deployable on almost every real-world scenario, from enterprise networks to mobile devices.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2947634",
        "year": "2016"
    },
    {
        "title": "From cookies to cooks: insights on dietary patterns via analysis of web usage logs",
        "abstract": "Nutrition is a key factor in people's overall health. Hence, understanding the nature and dynamics of population-wide dietary preferences over time and space can be valuable in public health. To date, studies have leveraged small samples of participants via food intake logs or treatment data. We propose a complementary source of population data on nutrition obtained via Web logs. Our main contribution is a spatiotemporal analysis of population-wide dietary preferences through the lens of logs gathered by a widely distributed Web-browser add-on, using the access volume of recipes that users seek via search as a proxy for actual food consumption. We discover that variation in dietary preferences as expressed via recipe access has two main periodic components, one yearly and the other weekly, and that there exist characteristic regional differences in terms of diet within the United States. In a second study, we identify users who show evidence of having made an acute decision to lose weight. We characterize the shifts in interests that they express in their search queries and focus on changes in their recipe queries in particular. Last, we correlate nutritional time series obtained from recipe queries with time-aligned data on hospital admissions, aimed at understanding how behavioral data captured in Web logs might be harnessed to identify potential relationships between diet and acute health problems. In this preliminary study, we focus on patterns of sodium identified in recipes over time and patterns of admission for congestive heart failure, a chronic illness that can be exacerbated by increases in sodium intake.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2488510",
        "year": "2013"
    },
    {
        "title": "From Relational Database to Event Log: Decisions with Quality Impact",
        "abstract": "This paper addresses the topic of ‚ÄòRemediation approaches for event log quality assurance‚Äô. The assumption of having readily minable event logs is often not fulfilled. This paper addresses, from an end-user‚Äôs perspective, the quality issues that arise when an event log needs to be built from a relational database. The decisions that are taken when building the event log, have an impact on the quality of the event log. Namely, these decisions impact the suitability of an event log for the planned analyses. The goal of this paper is to provide an overview of the decisions that impact the quality of the event log, along with a realistic running example. Based on this overview of decisions, a procedure is presented. This procedure provides guidance to build the event log in a conscious manner, taking into account all the decisions and their impact on quality. This work relates to other studies on how to build an event log from relational databases, but puts more emphasis on how the technical decisions have a direct impact on the analyses of the practitioner that will use the event log afterwards.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-74030-0_46",
        "year": "2018"
    },
    {
        "title": "Gas-water flow pattern recognition by log response in horizontal wells",
        "abstract": "Gas-water flow in horizontal development wells is complicated and changeable. In order to monitor reservoir development status and pace accurately, primary problem should be solved is to distinguish and recognize downhole gas-water flow patterns. Multi-phase flow simulative device, which is similar to downhole status, was used to conduct a series of measurement experiments using air and tap water as test media. Mixed flows were measured by real production logging tool (PLT) string at different inclinations and flow states in the transparent experimental well-bore, aim to study flow pattern recognition methods in horizontal gas wells by production logging information. Production logging experimental flow patterns analysis basing on experimental flow pattern observations and categorization reveals that gas-water mixed flow occurring in measurement procedure not only obey the general law of hydrodynamic but also own its unique characteristics resulted from logging procedure. But these experimental flow patterns still can be recognized from log response characteristics of spinner flowmeter intuitively. Cross-section water holdup vs. actual inlet water cut experiment relationship charts illustrate the links between flow parameters and flow patterns, thus they can be used to guide flow pattern recognition. The CAT 3D plots of phase profile, which are interpolation imaged by CATView software, compared with production logging experimental flow patterns suggest that gas-water flow patterns can be recognition even more intuitively by CAT 3D plots of phase profile. Combined with additional recognition method that mentioned above, more reliable flow pattern recognition results will be concluded for horizontal gas wells.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5658579/",
        "year": "2010"
    },
    {
        "title": "Gathering and Mining Information from Web Log Files",
        "abstract": "In this paper, a general methodology for gathering and mining information from Web log files is proposed. A series of tools to retrieve, store, and analyze the data extracted from log files have been designed and implemented. The aim is to form general methods by abstracting from the analysis of logs which use a well-defined standard format, such as the Extended Log File Format proposed by W3C. The methodology has been experimented on the Web log files of The European Library portal; the experimental analyses led to personal, technical, geographical and temporal findings about the usage and traffic load. Considerations about a more accurate tracking of users and users profiles, and a better management of crawler accesses using authentication are presented.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-77088-6_10",
        "year": "2007"
    },
    {
        "title": "Gaze Tracing in a Bounded Log-Spherical Space for Artificial Attention Systems",
        "abstract": "Human gaze is one of the most important cue for social robotics due to its embedded intention information. Discovering the location or the object that an interlocutor is staring at, gives the machine some insight to perform the correct attentional behaviour. This work presents a fast voxel traversal algorithm for estimating the potential locations that a human is gazing. Given a 3D occupancy map in log-spherical coordinates and the gaze vector, we evaluate the regions that are relevant for attention by computing the set of intersected voxels between an arbitrary gaze ray in the 3D space and a log-spherical bounded section defined by ùúå‚àà(ùúåùëöùëñùëõ,ùúåùëöùëéùë•),ùúÉ‚àà(ùúÉùëöùëñùëõ,ùúÉùëöùëéùë•),ùúô‚àà(ùúôùëöùëñùëõ,ùúôùëöùëéùë•)œÅ‚àà(œÅmin,œÅmax),Œ∏‚àà(Œ∏min,Œ∏max),œï‚àà(œïmin,œïmax). The first intersected voxel is computed in closed form and the rest are obtained by binary search guaranteeing no repetitions in the intersected set. The proposed method is motivated and validated within a human-robot interaction application: gaze tracing for artificial attention systems.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-27149-1_32",
        "year": "2016"
    },
    {
        "title": "GBTM: Graph Based Troubleshooting Method for Handling Customer Cases Using Storage System Log",
        "abstract": "Present day computing environments consist of different bits of hardware and software that are associated with each other in a complex way. Hence, in case of failures of such system, it is very difficult to detect the exact module which has caused the problem. In such a situation, an automated technique which can pin down to (at least) a set of modules that may be responsible for the failure would be very useful for support engineers. This paper makes an important step towards that direction. We propose a graph based troubleshooting methodology exploring storage system logs (EMS) of around 4500 customer cases to troubleshoot customer problems. We provide a ranked list of modules to the support engineers which can significantly narrow down the troubleshooting process for around 95% cases with only 10% false positive rate whereas the competing baseline MonitorRank covers only 74% cases with 23% false positive rate.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-93034-3_31",
        "year": "2018"
    },
    {
        "title": "Generating C Code from LOGS Specifications",
        "abstract": "This paper introduces a tool that automatically translates a concrete form of specifications into C code linked with BSPlib. The translation tool is rigorously developed with important safety properties proved. A Logs specification for Bulk-Synchronous Parallelism is a relation of an initial state, a final state and some intermediate states. Nondeterminism and parallelism correspond to disjunction and conjunction respectively. Various advanced specification commands can be derived from the basic ones. The translator checks syntax, freedom of communication interference, type consistency and communication dependencies before generating the target code. Static analysis (including both static checkings and translation) is presented in abstract interpretation. It is shown that a few laws are complete for transforming any specification into a normal form. These laws are satisfied by the abstract functions. We demonstrate the actual effects of the abstract functions by applying them on the normal form. The approach has been implemented using an object-oriented language.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11560647_13",
        "year": "2005"
    },
    {
        "title": "Generating Event Logs Through the Simulation of Declare Models",
        "abstract": "In the process mining field, several techniques have been developed during the last years, for the discovery of declarative process models from event logs. This type of models describes processes on the basis of temporal constraints. Every behavior that does not violate such constraints is allowed, and such characteristic has proven to be suitable for representing highly flexible processes. One way to test a process discovery technique is to generate an event log by simulating a process model, and then verify that the process discovered from such a log matches the original one. For this reason, a tool for generating event logs starting from declarative process models becomes vital for the evaluation of declarative process discovery techniques. In this paper, we present an approach for the automated generation of event logs, starting from process models that are based on Declare, one of the most used declarative modeling languages in the process mining literature. Our framework bases upon the translation of Declare constraints into regular expressions and on the utilization of Finite State Automata for the simulation. An evaluation of the implemented tool is presented, showing its effectiveness in both the generation of new logs and the replication of the behavior of existing ones. The presented evaluation also shows the capability of the tool of generating very large logs in a reasonably small amount of time, and its integration with state-of-the-art Declare modeling and discovery tools.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-24626-0_2",
        "year": "2015"
    },
    {
        "title": "Generating Event Logs with Workload-Dependent Speeds from Simulation Models",
        "abstract": "Both simulation and process mining can be used to analyze operational business processes. Simulation is model-driven and very useful because different scenarios can be explored by changing the model‚Äôs parameters. Process mining is driven by event data. This allows detailed analysis of the observed behavior showing actual bottlenecks, deviations, and other performance-related problems. Both techniques tend to focus on the control-flow and do not analyze resource behavior in a detailed manner. In this paper, we focus on workload-dependent processing speeds because of the well-known phenomenon that people perform best at a certain stress level. For example, the ‚ÄúYerkes-Dodson Law of Arousal‚Äù states that people will take more time to execute an activity if there is little work to do. This paper shows how workload-dependent processing speeds can be incorporated in a simulation model and learned from event logs. We also show how event logs with workload-dependent behavior can be generated through simulation. Experiments show that it is crucial to incorporate such phenomena. Moreover, we advocate an amalgamation of simulation and process mining techniques to better understand, model, and improve real-life business processes.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-31069-0_31",
        "year": "2012"
    },
    {
        "title": "Geo-identification of web users through logs using ELK stack",
        "abstract": "With the Internet penetration rate going higher, huge amount of log files are being generated, which contains hidden information having enormous business value. To unlock the hidden returns, log management system helps in making business decisions. Although, a lot of log management exist but they either fail to scale or are costly. Here efforts have been made to solve the shortcomings of prevailing log analyzer tools and this paper demonstrates the working of ELK ecosystem i.e. Elasticsearch, Logstash and Kibana clubbed together to efficiently analyze the log files and provide an interactive and easily understandable insights. Log management systems built on ELK stack are desired to analyze large log data sets while making the whole computation process easy to monitor through an interactive interface. Being from open source community ELK stack has many useful features for log analysis. Elasticsearch is used as Indexing, storage and retrieval engine. Logstash acts as a Log input slicer and dicer and output writer while Kibana performs Data visualization using dashboards. By implementing ELK ecosystem we have efficiently geo-identify the website users traffic using logs.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7508191/",
        "year": "2016"
    },
    {
        "title": "Geo-spatial and log-linear analysis of pedestrian and bicyclist crashes involving school-aged children",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0022437507001053",
        "year": "2007"
    },
    {
        "title": "Global Analysis of Log Likelihood Criterion",
        "abstract": "This paper introduces and investigates a gradient flow of the log likelihood function restricted on the isospectral submanifold and proves that the flow globally converges to diagonal matrices for almost all positive definite initial matrices. This result shows that the log likelihood function does not have any spurious stable fixed point and ensures the global convergence of ICA algorithms based on the log likelihood function.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11679363_101",
        "year": "2006"
    },
    {
        "title": "GPS Log Mining Method for Tourism Activity Analysis",
        "abstract": "Recently, tourism activity analysis has been required in order to construct effective tourism policy and strategy for personal tour which became the mainstream of Japan. In conventional tourism activity analysis, the information for analysis, i.e., tourism activity information, is collected on each tourist by using questionnaire, and then the analysis is carried out on the basis of it. However, it is difficult to realize effective tourism activity analysis in conventional method because the questionnaire-based information collection has difficulty in collecting accurate and detailed tourism activity information. In this paper, we propose a GPS log mining method in order to implement effective tourism activity analysis. GPS can automatically and continuously collect the position information where the GPS receiver exists. It is likely that an effective tourism activity analysis can be realized by extracting accurate tourism activity information from the information which is collected by GPS. We confirm the effectiveness of the proposed method through the experiments using the GPS log data collected from actual tourists in Hokkaido.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-32391-0_36",
        "year": "2005"
    },
    {
        "title": "Graph structures and algorithms for query-log analysis",
        "abstract": "Query logs are repositories that record all the interactions of users with a search engine. This incredibly rich user behavior data can be modeled using appropriate graph structures. In the recent years there has been an increasing amount of literature on studying properties, models, and algorithms for query-log graphs. Understanding the structure of such graphs, modeling user querying patterns, and designing algorithms for leveraging the latent knowledge (also known as the wisdom of the crowds) in those graphs introduces new challenges in the field of graph mining. The main goal of this paper is to present the reader with an example of these graph-structures, i.e., the Query-flow graph. This representation has been shown extremely effective for modeling user querying patterns and has been extensively used for developing real time applications. Moreover we present graph-based algorithmic solutions applied in the context of problems appearing in web applications as query recommendation and user-session segmentation.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-13962-8_14",
        "year": "2010"
    },
    {
        "title": "Graph-Based Pattern Identification from Architecture Change Logs",
        "abstract": "Service-based architectures have become commonplace, creating the need to address their systematic maintenance and evolution. We investigate architecture change representation, primarily focusing on the identification of change patterns that support the potential reuse of common changes in architecture-centric evolution for service software. We propose to exploit architecture change logs - capturing traces of sequential changes - to identify patterns of change that occur over time. The changes in the log are formalised as a typed attributed graph that allows us to apply frequent sub-graph mining approaches to identify potentially reusable, usage-determined change patterns. We propose to foster the reuse of routine evolution tasks to allow an architect to follow a systematic, reuse-centered approach to architectural change execution.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-31069-0_18",
        "year": "2012"
    },
    {
        "title": "Gulf of Mexico Gas Hydrate Joint Industry Project Leg II logging-while-drilling data acquisition and analysis",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0264817211001784",
        "year": "2012"
    },
    {
        "title": "Hardware implementation of Log-MAP turbo decoder for W-CDMA Node B with CRC-aided early stopping",
        "abstract": "Based on the coding/multiplexing scenario specified for W-CDMA FDD (wideband-code division multiple access, frequency division duplex), CRC (cyclic redundancy check)-aided early stopping method is chosen for hardware implementation of the Log-MAP (logarithm-maximum a posteriori) turbo decoder for use in W-CDMA Node B. Modifications to decoder structure for efficient incorporation of the CRC-aided method are proposed. With negligible cost and no performance degradation, our modified decoder works more efficiently.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1002642/",
        "year": "2002"
    },
    {
        "title": "Harnessing pseudonyms with implicit attributes for privacy-respecting mission log analysis",
        "abstract": "Many applications in the area of collaborative work can be enhanced by tracking users regularly. Consider a future emergency management application, in which mobile first responders are continuously tracked in order to support a better coordination of the rescue missions and to create a mission log. However, continuous tracking of individuals and storing the data for later use is often in conflict with individual privacy preferences. Therefore, it is a challenge to deal with conflicting traceability and privacy protection requirements. A common way to implement some kind of privacy protection is to use pseudonyms instead of fixed IDs for each user. However, in order to build a multilateral secure and acceptable solution, a more complex system design w.r.t. to pseudonym linkability is required, that also allows third parties to analyze the logs for organizational and legal reasons. In this paper, we present our approach to deal with this issue: we propose to encode additional information into pseudonyms that are used in location tracking systems and stored in data logs. Our concept comprises both access rights for the user herself and implicit attributes that may be verified by third parties in a privacy-respecting manner. We introduce the cryptographic constructions, which employ cryptographically secure pseudorandom number generators, threshold cryptography and techniques for securely evaluating encrypted data. Moreover, in this paper, we sketch a practical application example in the area of emergency mission log analysis and discuss the main security properties of our concepts.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/abstract/document/5370929/",
        "year": "2009"
    },
    {
        "title": "Hierarchical Conformance Checking of Process Models Based on Event Logs",
        "abstract": "Process mining techniques aim to extract knowledge from event logs. Conformance checking is one of the hard problems in process mining: it aims to diagnose and quantify the mismatch between observed and modeled behavior. Precise conformance checking implies solving complex optimization problems and is therefore computationally challenging for real-life event logs. In this paper a technique to apply hierarchical conformance checking is presented, based on a state-of-the-art algorithm for deriving the subprocesses structure underlying a process model. Hierarchical conformance checking allows us to decompose problems that would otherwise be intractable. Moreover, users can navigate through conformance results and zoom into parts of the model that have a poor conformance. The technique has been implemented as a ProM plugin and an experimental evaluation showing the significance of the approach is provided.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-38697-8_16",
        "year": "2013"
    },
    {
        "title": "High performance logging system for embedded UNIX and GNU/Linux applications",
        "abstract": "We present a high performance logging system for embedded UNIX and GNU/Linux applications. Compared to the standard UNIX and GNU/Linux logging method, syslog, our method has two orders of magnitude lower latency and an order of magnitude higher message throughput. This speed-up is mainly due to the use of a memory-mapped file as the means of inter-process communication, fewer memory copies and the batching of output messages in the logging daemon. In addition, our logging system also accepts syslog messages, providing compatibility with existing applications. Our logging system is in production use in the Cisco UCS Virtual Interface Card.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6732232/",
        "year": "2013"
    },
    {
        "title": "High speed low complexity radix-16 Max-Log-MAP SISO decoder",
        "abstract": "At present, the main challenge for hardware implementation turbo decoders is to achieve the high data rates required by current and future communication system standards. In order to address this challenge, a low complexity radix-16 SISO decoder for the Max-Log- MAP algorithm is proposed in this paper. Based on the elimination of parallel paths in the radix-16 trellis diagram, architectural solutions to reduce the hardware complexity of the different blocks of a SISO decoder are detailed. Moreover, two complementary techniques are introduced order to overcome BER/FER performance degradation when turbo decoders based on the proposed SISO decoder are considered. Thus, a penalty lower than 0.05dB is observed for a 8 state binary turbo code with respect to a traditional radix-2 turbo decoder for 6 decoding iterations.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6463718/",
        "year": "2012"
    },
    {
        "title": "High-Volume Hypothesis Testing for Large-Scale Web Log Analysis",
        "abstract": "Time-stamped event sequence data is being generated across many domains: shopping transactions, web traffic logs, medical histories, etc. Oftentimes, analysts are interested in comparing the similarities and differences between two or more groups of event sequences to better understand processes that lead to different outcomes (e.g., a customer did or did not make a purchase). CoCo is a visual analytics tool for Cohort Comparison that combines automated high-volume hypothesis testing (HVHT) with and interactive visualization and user interface for improved exploratory data analysis. This paper covers the first case study of CoCo for large-scale web log analysis and the challenges that arise when scaling a visual analytics tool to large datasets. The direct contributions of this paper are: (1) solutions to 7 challenges of scaling a visual analytics tool to larger datasets, and (2) a case study with three real-world analysts with these solutions implemented.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2892487",
        "year": "2016"
    },
    {
        "title": "Histogram Matrix: Log File Visualization for Anomaly Detection",
        "abstract": "In today's IT environments, there is an ever increasing demand for log file analysis solutions. Log files often contain important information about possible incidents, but inspecting the often large amounts of textual data is too time-consuming and tedious a task to perform manually. To address this issue, we propose a novel log file visualization technique called Histogram Matrix (HMAT). HMAT visualizes the content of a log file in order to enable a security administrator to efficiently spot anomalies. The system uses a combination of graphical and statistical techniques and allows even non-experts to interactively search for anomalous log messages. Contrary to other approaches, our proposal does not only work on certain special kinds of log files, but instead works on almost every textual log file. Additionally, the system allows to automatically generate security events if an anomaly is detected, similar to anomaly-based intrusion detection systems. This paper introduces HMAT, demonstrates its functionality using log files from a variety of services in real environments, and identifies strengths and limitations of the technique.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/4529398/",
        "year": "2008"
    },
    {
        "title": "Histogram of Log-Gabor Magnitude Patterns for face recognition",
        "abstract": "The Gabor-based features have achieved excellent performances for face recognition on traditional face databases. However, on the recent LFW (Labeled Faces in the Wild) face database, Gabor-based features attract little attention due to their high computing complexity and feature dimension and poor performance. In this paper, we propose a Gabor-based feature termed Histogram of Gabor Magnitude Patterns (HGMP) which is very simple but effective. HGMP adopts the Bag-of-Words (BoW) image representation framework. It views the Gabor filters as codewords and the Gabor magnitudes of each point as the responses of the point to these codewords. Then the point is coded by the orientation normalization and scale non-maximum suppression of its magnitudes, which are efficient to compute. Moreover, the number of codewords is so small that the feature dimension of HGMP is very low. In addition, we analyze the advantages of log-Gabor filters to Gabor filters to serve as the codewords, and propose to replace Gabor filters with log-Gabor filters in HGMP, which produces the Histogram of Log-Gabor Magnitude Patterns (HLGMP) feature. The experimental results on LFW show that HLGMP outperforms HGMP and it achieves the state-of-the-art performance, although its computing complexity and feature dimension are very low.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6853650/",
        "year": "2014"
    },
    {
        "title": "Historical-Log Recording System for Crack Opening and Growth Based on Mechanoluminescent Flexible Sensor",
        "abstract": "Recently, there are innovative mechanoluminescent (ML) particles made available, each of which repeatedly emits light in response to small applied stresses even in elastic region. When dispersedly coated onto a structure, each particle acts as a sensitive mechanical sensor, while the two-dimensional emission pattern of the whole assembly reflects the dynamical stress distribution inside the structure and the mechanical information around the crack and defect. To use the remarkable advantage of the ML sensor in flexibility, electricity/lead-free, low-cost, and so forth, and to answer social needs for historical-log of stress/damage accumulation on social infra-structure, we investigate historical-log recording system for crack opening and fatigue crack growth, and finally succeed to record it with responding position and intensity reflecting the trace of propagating crack tip and stress intensity factor around the tip. Furthermore, crack mouse opening displacement accompanied by general traffic of bridge in use is successfully detected.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6518140/",
        "year": "2013"
    },
    {
        "title": "Holistic Processing and Exploring Event Logs",
        "abstract": "Computer systems generate large amounts of event logs related to various operational aspects (positive and negative). Extracting from them useful information (e.g. targeted at dependability and resilience issues) is a challenging problem widely discussed in the literature and still needing deeper studies. We have developed a new holistic approach using enhanced event classification (based on original text mining algorithms) combined with multidimensional statistical analysis of various properties in vocabulary (words, phrases), time, spatial, local and global correlations. It has been incorporated in the developed tools and verified on event data sets collected from different computers.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-65948-0_12",
        "year": "2017"
    },
    {
        "title": "Honeynet Systems Based on Genetic Algorithm Log Mining",
        "abstract": "This article introduces the honeynet network structure and working mechanism from the perspective of network security technology, makes an in-depth analysis of honeynet technologies, and brings in suspicious behavior directed security strategies. It Introduces honeynet systems based on genetic algorithm log detection, and makes a rather further study of data mining using genetic algorithms.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6322395/",
        "year": "2012"
    },
    {
        "title": "How are we searching the World Wide Web? A comparison of nine search engine transaction logs",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0306457304001396",
        "year": "2006"
    },
    {
        "title": "How Do People Talk with a Virtual Philosopher: Log Analysis of a Real-World Application",
        "abstract": "Conversation with computers is an important form of human computer interaction. Inappropriately designed conversational agents can easily lead to unsatisfying user experience and even frustration, and this is especially true when the application is deployed in the real world. Currently, research on casual non-task oriented systems and our understanding in how people interact with such agents are still limited. To gain more insights on this issue, we carried out both quantitative and qualitative content analysis of conversation logs collected from a real-world application, featuring a non-task oriented conversational agent as a virtual philosopher. We construct a taxonomy of user utterances to the agent and discuss a few strategies that an agent might employ to provide a better user experience.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-41106-9_16",
        "year": "2013"
    },
    {
        "title": "How k-12 students search for learning?: analysis of an educational search engine log",
        "abstract": "In this study, we analyze an educational search engine log for shedding light on K-12 students' search behavior in a learning environment. We specially focus on query, session, user and click characteristics and compare the trends to the findings in the literature for general web search engines. Our analysis helps understanding how students search with the purpose of learning in an educational vertical, and reveals new directions to improve the search performance in the education domain.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2609532",
        "year": "2014"
    },
    {
        "title": "How People Read Books Online: Mining and Visualizing Web Logs for Use Information",
        "abstract": "This paper explores how people read books online using the International Children‚Äôs Digital Library (ICDL). We analyzed usage of the ICDL in an attempt to understand how people read books from websites. We propose a definition of reading a book (in contrast to others who visit the website), and report a number of observations about the use of the library in question.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-04346-8_35",
        "year": "2009"
    },
    {
        "title": "HuMa: A Multi-layer Framework for Threat Analysis in a Heterogeneous Log Environment",
        "abstract": "The advent of massive and highly heterogeneous information systems poses major challenges to professionals responsible for IT security. The huge amount of monitoring data currently being generated means that no human being or group of human beings can cope with their analysis. Furthermore, fully automated tools still lack the ability to track the associated events in a fine-grained and reliable way. Here, we propose the HuMa framework for detailed and reliable analysis of large amounts of data for security purposes. HuMa uses a multi-analysis approach to study complex security events in a large set of logs. It is organized around three layers: the event layer, the context and attack pattern layer, and the assessment layer. We describe the framework components and the set of complementary algorithms for security assessment. We also provide an evaluation of the contribution of the context and attack pattern layer to security investigation.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-75650-9_10",
        "year": "2018"
    },
    {
        "title": "Human behavior logging support system utilizing fused pose/position sensor and behavior target sensor information",
        "abstract": "This paper proposes a behavior log creation support system realized by sensor fusion. The system is equipped with a human pose sensor and a staying room sensor as the pose/position sensors as well as a voice sensor and a PC (personal computer) utilization history sensor to detect the target of the behavior. The human pose sensor classifies human behavior into \"standing\", \"sitting\" and \"walking\". The staying room sensor records a name of a room where the user stays. The voice sensor detects the conversation, which appears when the user is with someone else. The PC utilization history sensor records not only whether a PC is used or not but also the names of the application software to serve as a detector of the behavior target during computer work. The measured data from these sensors is displayed to the user to support the creation of a behavior log, i.e. to support the user to recall and to input contents and targets of his or her behavior. The experiment of making use of the sensors and creating a behavior log proved that the recorded events of the behavior log per all events improved from 60% with no support to more than 90% with support of the system. The result quantitatively shows the capability of human behavior logging support of the system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1232675/",
        "year": "2003"
    },
    {
        "title": "Human behavior logging support system utilizing pose/position sensors and behavior target sensors",
        "abstract": "This paper proposes a behavior log creation support system utilizing human pose/position sensors and behavior target sensors. The system is equipped with a human pose sensor and a staying room sensors as the pose/position sensors as a well as a voice sensor and a PC utilization history sensor to detect the target of the behavior. The human pose sensor classifies human behaviors into \"standing\", \"sitting\", and \"walking\". The staying room sensor records a name of the room where the user stays. The voice sensor detects the conversation which appears when the user is communicating with someone else. The PC utilization history sensor records not only whether a PC is used or not but also the names of the application software to serve as a detector of the behavior target during the computer work. The measured data from these sensors is displayed to the user to support creating the behavior log, i.e. to support the user to recall and to input contents and targets of his or her behaviors. The experiment of making use of the sensors and creating the behavior log proved that the recorded events of the behavior log per all events improve from 60% with no support to more than 90% with support of the system. The result quantitatively shows the capability of human behavior logging support of the system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1248786/",
        "year": "2003"
    },
    {
        "title": "Hydrologic properties of coal beds in the Powder River Basin, Montana I. Geophysical log analysis",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0022169404005323",
        "year": "2005"
    },
    {
        "title": "A Low Disk-Bound Transaction Logging System for In-memory Distributed Data Stores",
        "abstract": "Transaction logging and snapshotting are techniques used to deliver durability to the data in in-memory data stores. Absolute durability guarantees are delivered to a system by sequentially recording the transaction logs and snapshots to a non-volatile disk. Recent advancements in database restoration techniques have given rise to lock-free fuzzy snapshots. Still the transaction log that completes the fuzzy snapshots is not lock-free. In addition to locking, the major overhead behind the transaction logging technique is the bottleneck involved in storing the logs to a persistent but slower disk. This paper concentrates on implementing an in-memory transaction logging system with a lesser disk dependency. This logging system mainly targets the distributed in-memory data stores that are transaction replicated, eventually consistent and fault tolerant to crash failures. By making logging in-memory, the performance will be improved, but during the crash fails, the state may be lost. On recovery, we restore the current state partially from the locally available fuzzy snapshot and the remaining from the non-failed nodes in the distributed replica. ZooKeeper, a distributed data store that offers distributed coordination as its major service is used to implement and test our research. On average, a 30 times write performance improvement has been achieved with this approach guaranteeing sufficient durability in replicated mode.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7776474/",
        "year": "2016"
    },
    {
        "title": "A Low-Memory Management for Log-Based File Systems on Flash Memory",
        "abstract": "Flash memory has become a popular device due to its huge-capacity, low-power consumption, non-volatility, and shock-resistance. A flash-memory storage system has replaced a hard-disk drive in many applications, especially in embedded systems. Recently, the implementation of file systems on flash-memory storage systems has become an important research topic. How to efficiently manage files and handle file accesses becomes an important issue. In this paper, we will propose a low-memory management for log-based file systems on flash memory. The experimental results show that the proposed method can provide reasonable performance under a low-memory environment.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5279648/",
        "year": "2009"
    },
    {
        "title": "A Method of Large - Scale Log Pattern Mining",
        "abstract": "With the development of the telecommunication network, more and more devices are used in the network, which has been a burden for the network operation and maintenance. At the same time, network devices generate large amounts of log data every day, recording the activities of each device in detail. As a result, the log can reflect the performance of network state, and sometimes, we can predict the occurrence of network failure based on the log. However, since the log has such features: big volume, multi-source heterogeneous and difficult to understand, people have not reasonably used it to analyze and predict network failure. Therefore, we propose a method for structuring a large number of device logs in the short term, and use the data generated from a real communication device network to verify the effect. Besides, we compare our method with the traditional log parsers, such as regular expressions, LogSig, etc. to demonstrate the efficient processing performance and accurate pattern extraction analysis for massive network device logs.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-74521-3_9",
        "year": "2018"
    },
    {
        "title": "A Mobile Log Data Analysis System Based on Multidimensional Data Visualization",
        "abstract": "The log data collected from mobile telecom services contains plenty of valuable information. The critical technical challenges to mobile log analysis include how to extract information from unformatted raw data, as well as how to visually represent and interact with the analysis results. In this paper, we demonstrate MobiLogViz, which seamlessly combines multidimensional data visualization and web usage mining techniques. By automatically processing the log data and providing coordinated views with various interactions, MobiLogViz effectively aids users in analyzing and exploring mobile log data.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-05813-9_41",
        "year": "2014"
    },
    {
        "title": "A model for website anomaly detection based on log analysis",
        "abstract": "To found security events from web logs has become an important aspect of network security. This paper proposes a website anomaly detection model based on security-log-analysis. After creating a anomaly feature sets of the model, C4.5 algorithm was used to improve feature sets, making the abnormal records in feature sets store hierarchically. Compared logs in website with the treated feature stes, the model ultimately achieves the purpose of checking website's security event fast and accurately.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7175806/",
        "year": "2014"
    },
    {
        "title": "A model to estimate intrinsic document relevance from the clickthrough logs of a web search engine",
        "abstract": "We propose a new model to interpret the clickthrough logs of a web search engine. This model is based on explicit assumptions on the user behavior. In particular, we draw conclusions on a document relevance by observing the user behavior after he examined the document and not based on whether a user clicks or not a document url. This results in a model based on intrinsic relevance, as opposed to perceived relevance. We use the model to predict document relevance and then use this as feature for a \"Learning to Rank\" machine learning algorithm. Comparing the ranking functions obtained by training the algorithm with and without the new feature we observe surprisingly good results. This is particularly notable given that the baseline we use is the heavily optimized ranking function of a leading commercial search engine. A deeper analysis shows that the new feature is particularly helpful for non navigational queries and queries with a large abandonment rate or a large average number of queries per session. This is important because these types of query is considered to be the most difficult to solve.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1718510",
        "year": "2010"
    },
    {
        "title": "A multi-dimensional quality assessment of state-of-the-art process discovery algorithms using real-life event logs",
        "abstract": "Process mining is the research domain that is dedicated to the a posteriori analysis of business process executions. The techniques developed within this research area are specifically designed to provide profound insight by exploiting the untapped reservoir of knowledge that resides within event logs of information systems. Process discovery is one specific subdomain of process mining that entails the discovery of control-flow models from such event logs. Assessing the quality of discovered process models is an essential element, both for conducting process mining research as well as for the use of process mining in practice. In this paper, a multi-dimensional quality assessment is presented in order to comprehensively evaluate process discovery techniques. In contrast to previous studies, the major contribution of this paper is the use of eight real-life event logs. For instance, we show that evaluation based on real-life event logs significantly differs from the traditional approach to assess process discovery techniques using artificial event logs. In addition, we provide an extensive overview of available process discovery techniques and we describe how discovered process models can be assessed regarding both accuracy and comprehensibility. The results of our study indicate that the HeuristicsMiner algorithm is especially suited in a real-life setting. However, it is also shown that, particularly for highly complex event logs, knowledge discovery from such data sets can become a major problem for traditional process discovery techniques.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0306437912000464",
        "year": "2012"
    },
    {
        "title": "A new log-likelihood gradient formula for continuous time stochastic systems with uncertain matrix",
        "abstract": "We use a finitely additive white noise approach to derive an explicit expression for the gradient of the log-likelihood ratio for system parameter estimation in the case of a continuous time linear dynamic stochastic system and noisy observations. Our gradient formula, includes the smoother estimates of the state, and derivatives of the system matrices, but no derivatives of the estimates or error covariances. A scheme to calculate the log-likelihood gradient without solving any Ricatti equations is described.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/411408/",
        "year": "1994"
    },
    {
        "title": "A new log-polar mapping for space variant imaging.: Application to face detection and tracking",
        "abstract": "Space-variant images (images whose resolution changes across the image) supply a powerful image representation for active vision systems. In this article a new log-polar mapping is presented. The main originality of this mapping is its great flexibility. Unlike other approaches the range of the logarithmic function used for the mapping can be fully specified (width and position). In this paper we also propose several efficient algorithms performing basic operations on these images. The originality of the proposed encoding is that log-polar pixels are a fractional part of rectangular pixels. Finally, we present some experimental results describing the use of this mapping in a face detection and tracking application.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S003132039800096X",
        "year": "1999"
    },
    {
        "title": "A New State Space Representation Method for Adaptive Log Domain Systems",
        "abstract": "In this paper, a new method for the state space representation of a system is proposed, which is based on the companion form technique. It is very important to have almost equal coefficients of state space equations since their coefficients are proportional to devices' currents or voltages, e.g. transistors' currents for log domain filters. The method gives us the opportunity to choose two arbitrary parameters (alpha, beta) to be able to obtain more balanced state space equations. This method is applied a log domain filter, which can be considered an adaptive filter since it can be electronically tuned. It is particularly useful for higher order log domain filters synthesized in the state space",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/1638150/",
        "year": "2006"
    },
    {
        "title": "A Non-visual Sensor Triggered Life Logging System Using Canonical Correlation Analysis",
        "abstract": "Life logging is one of the key service in modern life as wearable devices are forming an emerging market. Life logging has advantages to enlarge the human memory and even can help patients who suffer from memory impairment. Periodical picture taking is the simplest and the most widely used method but inefficient in both energy and memory side. In this paper, we suggest a novel capturing points decision method using the combination of visual information and non-visual information. In order to merge visual information and non-visual information, we adopted canonical correlation analysis (CCA) which is a statistic technique to find the mapping function for two different domain data into a highly correlate domain. In this way, we showed the new possibility of life logging system with minimum heuristic methodology. Moreover, we tested our approach on restricted real life situation and evaluated the result based on an image diversity measure using a determinantal point process (DPP) and showed better results than conventional methods.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7272681/",
        "year": "2015"
    },
    {
        "title": "A novel log-based relevance feedback technique in content-based image retrieval",
        "abstract": "Relevance feedback has been proposed as an important technique to boost the retrieval performance in content-based image retrieval (CBIR). However, since there exists a semantic gap between low-level features and high-level semantic concepts in CBIR, typical relevance feedback techniques need to perform a lot of rounds of feedback for achieving satisfactory results. These procedures are time-consuming and may make the users bored in the retrieval tasks. For a long-term study purpose in CBIR, we notice that the users' feedback logs can be available and employed for helping the retrieval tasks in CBIR systems. In this paper, we propose a novel scheme to study the log-based relevance feedback (LRF) technique for improving retrieval performance and reducing the semantic gap in CBIR. In order to effectively incorporate the users' feedback logs, we propose a modified support vector machine (SVM) technique called soft label support vector machine (SLSVM) to construct the LRF algorithm in CBIR. We conduct extensive experiments to evaluate the performance of our proposed algorithm. Compared with the typical approach using query expansion (QEX) technique, we demonstrate that our proposed scheme can significantly improve the retrieval performance of semantic image retrieval from detailed experiments.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1027533",
        "year": "2004"
    },
    {
        "title": "A novel telemetric logging system for recording physiological signals in unrestrained animals",
        "abstract": "A novel, dual mode telemetric logging system to monitor, transmit and record physiological waveforms (electrocardiogram ECG, electroencephalogram EEG and respiratory signals) in unrestrained animals was designed, constructed and tested. The system operating in telemetry mode allows bursts of physiological waveform data to be acquired and displayed on demand and allows parameters such as the gain of the signal conditioning amplifiers to be adjusted remotely. The system will commence logging several minutes of physiological waveforms when triggered by a command; it will continue logging regardless of the continued existence of a radio link and will subsequently download those data to a personal computer. System performance is demonstrated by physiological measurements (simultaneous ECG, EEG and/or respiration signals) made on chickens during stunning in a commercial controlled atmosphere system. This application demonstrated the acquisition of high quality physiological data in circumstances where other measurement approaches (including telemetry) would be difficult or impossible.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S016816990700052X",
        "year": "2007"
    },
    {
        "title": "A nuclear geophysical borehole logging system",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0029554X79962050",
        "year": "1979"
    },
    {
        "title": "A Passive Attack on the Privacy of Web Users Using Standard Log Information",
        "abstract": "Several active attacks on user privacy in the World Wide Web using cookies or active elements (Java, Javascript, ActiveX) are known. One goal is to identify a user in consecutive Internet session to track and to profile him (such a profile can be extended by personal information if available).",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/3-540-36467-6_14",
        "year": "2003"
    },
    {
        "title": "A personal photograph browser for life log analysis based on location, time, and person",
        "abstract": "Image browsers are important and useful applications for retrieving images from personal photograph collections. Such browsers can be a life log analysis tool to explore the events of photograph owners. This paper presents a novel photograph browser consisting of two linked views. One of the views displays photographs clustered based on their locations and times, and the other displays people clustered based on their co-occurrences in the events. Specifying a photograph, the corresponding time the picture was shot and people in the photograph are highlighted. Specifying a time, corresponding photographs and people are highlighted. Specifying a person, associated photographs are highlighted, and their corresponding times are shown. The mechanism helps users to discover interested photographs and understand the events of photograph owners. This paper presents a real scenario and user experiment, demonstrating the effectiveness of the presented browser.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1982458",
        "year": "2011"
    },
    {
        "title": "A Practical Approach for Controlling the Shape of the Radiation Pattern of a Microwave Log-Periodic Antenna for Wideband Applications [Antenna Applications Corner]",
        "abstract": "The log-periodic antenna (LPA) is a wideband frequency-independent antenna, used in many applications including direction-finding (DF) receivers. The direction finding is an important parameter measured by a direction-finding receiver in electronic support measures (ESM) systems. Amplitude-comparison direction finding (ADF) is one of the widely used angle-of-arrival measurement techniques in electronic support measures systems across the world. Many EW engineers have derived the mathematical equation for the estimation of the angle of arrival based on this popular technique. The angle of arrival accuracy using this technique depends on the number of antennas used in the configuration, channel imbalances, the beamwidth, and the shape factor of the antennas. To obtain good angle-of-arrival accuracy with the amplitude-comparison technique, the log-periodic antenna is one suitable antenna. Normally, electronic support measures systems are wide open in frequency (the bandwidths of electronic support measures receivers are a few thousands of MHz), and hence the antennas exhibit nearly the same beam shape (beamwidth and shape factor) over these wide operating frequency ranges so as to have a uniform angle-of-arrival error distribution across the frequency spectrum. However, a log-periodic antenna offers some variations in half-power beamwidth over a wide frequency spectrum at microwave frequency ranges, which is reasonably good, but is not enough to obtain uniform angle-of-arrival error patterns throughout the operating frequency range. Similarly, a uniform shape factor is also important across the bandwidth. The radiation beam of a log-periodic antenna over a wide operating frequency range thus need to be controlled. A practical solution to this problem, along with experimental results, are presented in the current paper.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6971971/",
        "year": "2014"
    },
    {
        "title": "A Predictive Location Management Scheme by Extracting the Unique Sub-patterns from the Mobility Logs",
        "abstract": "We propose a predictive scheme, where the MT stores only current day movement log and sends this log to MSC during off-peak hours. The MSC performs offline computation to find unique sub-patterns and hot cells from pattern logs. The hot cells are downloaded in the MT which sends an update when it leaves a hot cell thus enables the MSC to identify the sub-pattern to be followed next. On arrival of a call the MSC performs selective paging Analytical study shows the total location management cost using the proposed scheme is far better than distance-based location management scheme.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11947950_65",
        "year": "2006"
    },
    {
        "title": "A Preliminary Analysis of Web Usage Behaviors from Web Access Log Files",
        "abstract": "In our digital age, Internet becomes one of the most important factors in order to achieve information and knowledge. In educational institutes, Internet also plays important roles for people to promote comprehensive learning and teaching environments. However, there are always two sides of every coin. Overuse of Internet may lead to other problems. Our research objective is to investigate the Web usage behaviors from Web access log files. Data mining and statistical techniques have been employed to analyze for the purpose of descriptive and predictive aspects.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-81-322-1695-7_37",
        "year": "2014"
    },
    {
        "title": "A Probabilistic Unified Framework for Event Abstraction and Process Detection from Log Data",
        "abstract": "We consider the scenario where the executions of different business processes are traced into a log, where each trace describes a process instance as a sequence of low-level events (representing basic kinds of operations). In this context, we address a novel problem: given a description of the processes‚Äô behaviors in terms of high-level activities (instead of low-level events), and in the presence of uncertainty in the mapping between events and activities, find all the interpretations of each trace \\Phi. Specifically, an interpretation is a pair \\langle \\sigma , W \\rangle that provides a two-level ‚Äúexplanation‚Äù for \\Phi: \\sigma is a sequence of activities that may have triggered the events in \\Phi, and W is a process whose model admits \\sigma. To solve this problem, we propose a probabilistic framework representing ‚Äúconsistent‚Äù \\Phi‚Äôs interpretations, where each interpretation is associated with a probability score.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-26148-5_20",
        "year": "2015"
    },
    {
        "title": "Hypervisor Event Logs as a Source of Consistent Virtual Machine Evidence for Forensic Cloud Investigations",
        "abstract": "Cloud Computing is an emerging model of computing where users can leverage the computing infrastructure as a service stack or commodity. The security and privacy concerns of this infrastructure arising from the large co-location of tenants are, however, significant and pose considerable challenges in its widespread deployment. The current work addresses one aspect of the security problem by facilitating forensic investigations to determine if these virtual tenant spaces were maliciously violated by other tenants. It presents the design, application and limitations of a software prototype called the Virtual Machine (VM) Log Auditor that helps in detecting inconsistencies within the activity timelines for a VM history. A discussion on modeling a consistent approach is also provided.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-39256-6_7",
        "year": "2013"
    },
    {
        "title": "Identification and characterization of crawlers through analysis of web logs",
        "abstract": "Web crawlers are software programs that automatically traverse the hyperlink structure of the world-wide web in order to locate and retrieve information. In addition to crawlers from search engines, we observed many other crawlers which may gather business intelligence, confidential information or even execute attacks based on gathered information while camouflaging their identity. Therefore, it is important for a website owner to know who has crawled his site, and what they have done. In this study we have analyzed crawler patterns in web server logs, developed a methodology to identify crawlers and classified them into three categories. To evaluate our methodology we used seven test crawler scenarios. We found that approximately 53.25% of web crawler sessions were from ‚Äúknown‚Äù crawlers and 34.16% exhibit suspicious behavior.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6731972/",
        "year": "2013"
    },
    {
        "title": "Identification and evaluation of alarm logs from the alarm management system",
        "abstract": "This paper deals with the configuration of control systems implemented into the control of industrial processes. An analysis of alarm logs giving feedback for better understanding at the level of the human-machine interface was done to identify and analyze abnormal situations that affect process safety. This research supports the engineering work on the configuration and implementation of the systems, and further development in creating knowledge tools in a control operations environment according to international standards.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7501199/",
        "year": "2016"
    },
    {
        "title": "Identifying Fracture by Time-Frequency Analysis of Well Logs",
        "abstract": "It is important to precisely identify the fracture in the fractured reservoirs for exploiting gas and oil. The common method, identifying the fracture by the size and difference of dual laterolog, is affected by many factors, so the resolution of the fracture identification is not high. To solve these problems, this paper studies the principles of time-frequency analysis and Short Time Fourier transform, and then discusses how to extract the implicit information of the fracture in the dual laterolog by using the time-frequency analysis. Through theoretical modeling and practical applications, the results show that using Hamming window function and selecting 27 samples of window function we can calculate the amplitude and spectrum of the resistivity curves, which can greatly improve the ability of distinguishing fracture.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6086282/",
        "year": "2011"
    },
    {
        "title": "Identifying In-App User Actions from Mobile Web Logs",
        "abstract": "We address the problem of identifying in-app user actions from Web access logs when the content of those logs is both encrypted (through HTTPS) and also contains automated Web accesses. We find that the distribution of time gaps between HTTPS accesses can distinguish user actions from automated Web accesses generated by the apps, and we determine that it is reasonable to identify meaningful user actions within mobile Web logs by modelling this temporal feature. A real-world experiment is conducted with multiple mobile devices running some popular apps, and the results show that the proposed clustering-based method achieves good accuracy in identifying user actions, and outperforms the state-of-the-art baseline by 17.84%17.84%.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-93037-4_24",
        "year": "2018"
    },
    {
        "title": "Image watermarking using spread spectrum technique in log-2-spatio domain",
        "abstract": "In this paper, we propose to embed the watermark information in the log-2-spatio domain by means of spread spectrum technique. In log-2-spatio domain, the variance of the information is reduced significantly. This improves the efficiency and robustness of spread spectrum technique. Low intensity and mid-band regions are selected to embed the information in order to guarantee an invisible watermark as well as the robustness to JPEG compression. Simulation results show that the embedded information still survives up to the JPEG compression ratio of 14.7.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/857068/",
        "year": "2000"
    },
    {
        "title": "Implementation of Log Analysis System for Desktop Grids and Its Application to Resource Group-Based Task Scheduling",
        "abstract": "It is important that desktop grids should be aggressively deal with the dynamic properties arisen from the volatility and heterogeneity of resources. Therefore, it is required that task scheduling should positively consider the execution behavior that is characterized by an individual resource. In this paper, we implement a log analysis system which can analyze the execution behavior by utilizing actual log data of desktop grid systems. To verify the log analysis system, we conducted simulations and showed that the resource group-based task scheduling, based on the analysis of the execution behavior, offers faster turnaround time than the existing one even if few resources are used.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-27201-1_40",
        "year": "2011"
    },
    {
        "title": "Implementing advanced cleaning and end-user interpretability technologies in Web log mining",
        "abstract": "Two new approaches to Web log mining are presented. Novel advanced cleaning improves Web log mining results. Improved filtering removes pages with no links from other pages. In the data visualisation phase, technical representations of Web pages are replaced by user attractive text interpretations. Experiments with the real world problems showed that the proposed techniques significantly increase the quality and usefulness of Web log mining results.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1024660/",
        "year": "2002"
    },
    {
        "title": "Improved Artificial Negative Event Generation to Enhance Process Event Logs",
        "abstract": "Process mining is the research area that is concerned with knowledge discovery from event logs. Process mining faces notable difficulties. One is that process mining is commonly limited to the harder setting of unsupervised learning, since negative information about state transitions that were prevented from taking place (i.e. negative events) is often unavailable in real-life event logs. We propose a method to enhance process event logs with artificially generated negative events, striving towards the induction of a set of negative examples that is both correct (containing no false negative events) and complete (containing all, non-trivial negative events). Such generated sets of negative events can advantageously be applied for discovery and evaluation purposes, and in auditing and compliance settings.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-31095-9_17",
        "year": "2012"
    },
    {
        "title": "Improved morphological method in the detection of log CT image with defect",
        "abstract": "Mathematical morphology is a new subject established based on rigorous mathematical theories. In the basis of set theory, mathematical morphology is used for image processing, analysing and comprehending. It is a powerful tool in the geometric morphological analysis and description. It has become a new theory in the digital image processing field. Morphology offers a unified and powerful approach to numerous image processing problems. Mathematical morphology is a mathematic tool to analyze the image based on the structuring element. Moreover, it has deep influence on the image processing theory and technology. Edge is the basic feature in the image. It involves a lot of valuable target information of boundary, which is used for image processing, target identifying and image filtering. The mathematical morphology is an effective theory used to locate the image edge. In the paper improved multi-scale and structuring element in mathematical morphology is used to detect log CT image with defect, and the omnidirectional structure element provides a new method in log defect recognition.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4636341/",
        "year": "2008"
    },
    {
        "title": "Improved multi-scale and structuring element morphological detection in the log CT image",
        "abstract": "Mathematical morphology is a new subject established based on rigorous mathematical theories. In the basis of set theory, mathematical morphology is used for image processing, analysing and comprehending. It is a powerful tool in the geometric morphological analysis and description. It has become a new theory in the digital image processing field. Moreover, it has deep influence on the image processing theory and technology. Edge is the basic feature in the medical image. It involves a lot of valuable target information of boundary, which is used for image processing, target identifying and image filtering. The mathematical morphology is an effective theory used to locate the image edge. In the paper multi-scale and structuring element in mathematical morphology is used to detect log CT image with defect, and provides a new method in log defect recognition.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4670747/",
        "year": "2008"
    },
    {
        "title": "Improved selective logging detection with Landsat images in tropical regions",
        "abstract": "Logging is a major form of forest degradation in the tropical regions like Brazilian Amazon. It alters the tropical habitat environments and results in release of carbons as well. The traditional way of logging is through forest clearing, which converts forest to other land uses such as agriculture or rangeland. Recently a new form of forest degradation is selective logging, removing only those good quality tree species. This form of deforestation does not result in land use conversion, but degradation. Logging by means of clear-cutting can be easily detected and monitored from satellite images such as those from Landsat sensors. However, detection and monitoring selective logging is difficult with satellite images because the process only removes a small number of trees per area, resulting in subtle disturbances but substantial removal of biomass. Therefore, traditional classification technique is unable to detect and monitor this type of disturbances effectively. In order to detect selective logging, and to better understand carbon sequestrations, a continuous field ought to be used that can quantify the degree of disturbances due to selective logging, instead of using binary classification techniques. In this paper, we used signal-unmixing techniques in a spectral vegetation index domain as a continuous field measure of forest density, with which selective logging is mapped and quantified. The spectral index used is the MSAVI further modified to enhance its sensitivity to subtle forest degradations in the tropical environments in Brazilian Amazon as well as in Southeast Asia.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1026450/",
        "year": "2002"
    },
    {
        "title": "Improving Gross Count Gamma-Ray Logging in Uranium Mining With the NGRS Probe",
        "abstract": "AREVA Mines and the Nuclear Measurement Laboratory of CEA Cadarache are collaborating to improve the sensitivity and precision of uranium concentration measurement by means of gamma-ray logging. The determination of uranium concentration in boreholes is performed with the Natural Gamma Ray Sonde (NGRS) based on a NaI(Tl) scintillation detector. The total gamma count rate is converted into uranium concentration using a calibration coefficient measured in concrete blocks with known uranium concentration in the AREVA Mines calibration facility located in Bessines, France. Until now, to take into account gamma attenuation in a variety of boreholes diameters, tubing materials, diameters and thicknesses, filling fluid densities, and compositions, a semiempirical formula was used to correct the calibration coefficient measured in Bessines facility. In this paper, we propose to use Monte Carlo simulations to improve gamma attenuation corrections. To this purpose, the NGRS probe and the calibration measurements in the standard concrete blocks have been modeled with Monte Carlo N-Particles (MCNP) computer code. The calibration coefficient determined by simulation 5.3 s -1 ¬∑ ppm -1 U with 10% accuracy is in good agreement with the one measured in Bessines (and for which no uncertainty was provided), 5.2 s -1 ¬∑ ppm -1 U . The calculations indicate that the concrete blocks used for measuring the calibration coefficients measured in Bessines are underestimated by about 10%. Based on the validated MCNP model, several parametric studies have been performed. For instance, the rock density and chemical composition proved to have a limited impact on the calibration coefficient. However, gamma self-absorption in uranium leads to a nonlinear relationship between count rate and uranium concentration beyond approximately 1% of uranium weight fraction, the underestimation of the uranium content reaching more than a factor 2.5 for a 50% uranium weight fraction. Parametric studies have a...(View more)",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8278240/",
        "year": "2018"
    },
    {
        "title": "Improving Voting System Event Logs",
        "abstract": "Federal standards require that electronic voting machines log information about the voting system behavior to support post-election audits and investigations. Our study examines what additional voter interaction information should be collected to allow investigation of human factors issues of the voting systems used in an election, while at the same time preserving voter privacy. We have focused on simulating touch screen interface errors that have been hypothesized as the cause of problems in past elections, such as miscalibration and insensitivity. The preliminary data gathered indicates that event logs which record voter interaction information may allow investigators to detect the existence of interface problems in deployed voting systems. This information can be collected without compromising secret ballot rights. We believe that any voting system using a touch screen interface could benefit by logging these events.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5460383/",
        "year": "2009"
    },
    {
        "title": "In vivo attenuation estimation in human thyroid nodules using the regularized spectral log difference technique: Initial pilot study",
        "abstract": "In vivo estimation of attenuation coefficients is useful because of its potential for tissue characterization and relevance in accurate backscatter coefficient estimation. However, current methods based on spectral analysis for ultrasonic attenuation estimation suffer from a severe trade-off between estimation precision and spatial resolution. Recently, the regularized spectral log difference (RSLD) technique was proposed to extend such trade-off in attenuation coefficient slope (ACS) estimation. The RSLD technique has been tested with simulated data and physical phantoms. Therefore, the aim of this pilot study is to validate the feasibility of in vivo estimation of ACSs from thyroid nodules using the RSLD technique.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8092846/",
        "year": "2017"
    },
    {
        "title": "In-vehicle data logging system for fatigue analysis of drive shaft",
        "abstract": "The work presents a cost-effective custom-made data logging system for in-vehicle use. Based on the required bandwidth and accuracy of the torque signal a real-time data acquisition system for long-term data logging of torque signal has been designed and implemented. The system is based on a computer running Windows CE 3.0 operating system. Filtered input signal is sample by over-sampling data acquisition system. Data reduction is achieved by decimation and data compression. Three data compression methods are compared: two methods based on zero-order and first-order predictors and a piecewise linear approximation method followed by run-length and Huffman coding. Several error bands have been investigated in the data compression methods. Test result show that a system with 1 GB flash card can store over 10000 hrs of drive shaft load history data allowing signal reconstruction with satisfactory accuracy.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1317610/",
        "year": "2004"
    },
    {
        "title": "Incorporating User Behavior Patterns to Discover Workflow Models from Event Logs",
        "abstract": "We propose a novel approach to discover workflow models from event logs. The proposed approach addresses two major limitations of current process mining approaches. First, they assume either a single workflow model for the entire event log or the availability of workflow ids that can be used to group logs associated with the same workflow model together. Nonetheless, these assumptions are oversimplified as a complex system typically runs multiple workflow models, all of which share the same log system. Second, existing process mining approaches do not consider the usage patterns of workflow users. Most systems support multi-users and each user is typically associated with (or use) certain number of operation sequences, which may all follow one or several workflow models. Hence, we propose to leverage User Behavior Patterns (or UBPs) to improve the outcome of process mining. In particular, we exploit machine learning techniques to incorporate UBPs into sequence clustering for workflow model discovery. We model a UBP as a probabilistic distribution on sequences, which allows to compute the distance between a UBP and any sequence. We apply three-way matrix factorization onto a UBP-sequence distance matrix to co-cluster users and sequences. In this way, users that share similar UBPs are grouped together while the clustering of similar sequences will lead to the discovery of workflow models. An comprehensive experimental study is conducted to demonstrate the effectiveness and efficiency of the proposed approach.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6649576/",
        "year": "2013"
    },
    {
        "title": "Incremental learning of system log formats",
        "abstract": "System logs come in a large and evolving variety of formats, many of which are semi-structured and/or non-standard. As a consequence, off-the-shelf tools for processing such logs often do not exist, forcing analysts to develop their own tools, which is costly and timeconsuming. In this paper, we present an incremental algorithm that automatically infers the format of system log files. From the resulting format descriptions, we can generate a suite of data processing tools automatically. The system can handle large-scale data sources whose formats evolve over time. Furthermore, it allows analysts to modify inferred descriptions as desired and incorporates those changes in future revisions.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1740410",
        "year": "2010"
    },
    {
        "title": "Insider threat detection using log analysis and event correlation",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S1877050915004184",
        "year": "2015"
    },
    {
        "title": "Integrating event logs into KDM repositories",
        "abstract": "Business knowledge embedded in legacy information systems is a valuable asset that must be recovered and preserved when these systems are modernized. Event logs register the execution of business activities supported by existing information systems, thus they entail a key artifact to be used for recovering the actual business processes. There exists a wide variety of techniques to discover business processes by reversing event logs. Unfortunately, event logs are typically represented with particular notations such as Mining XML (MXML) rather than the recent software modernization standard Knowledge Discovery Metamodel (KDM). Process mining techniques consequently cannot be effectively reused within software modernization projects. This paper proposes an automatic technique to transform MXML event logs into event models to be integrated into KDM repositories. Its main implication is the exploitation of valuable event logs by well-proven software modernization techniques. The model transformation has been validated through a case study involving several benchmark event logs.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2231949",
        "year": "2012"
    },
    {
        "title": "Intelligent mininig for capturing processes through event logs to represent workflows using FP tree",
        "abstract": "Data mining applications require an ability to understand unfiltered data embedded in event logs. The scalability of the data, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records will determine efficiency of data mining. Contemporary workflow management systems are driven by explicit process models based on completely specified workflow designs. Creating a workflow design is a complicated time-consuming process and typically there are discrepancies between the actual workflow processes and the processes as perceived by the management. In this paper, we propose a Process Mining Architecture (PROARCH) model which involves capturing processes in a system through event logs containing information about the different processes under execution. We assume that events in logs bear timestamps. But these logs will also contain log of unformatted data which may be dirty data for our model. Hence this information needs to be filtered before further processing. After filtering, the clean data is represented in MXML format and will serve as input to our model. This MXML data is parsed into a Petri net representation. The nodes and transitions, are connected to form a workflow representation. Since the initial input logs are dirty we use FP tree approach to build our workflow model.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4658342/",
        "year": "2007"
    },
    {
        "title": "Intelligent System for Tracking and Logging the Zigzag Pantograph Motion",
        "abstract": "In this paper a system which is able to track and log data obtained by measuring the zigzag motion of the pantograph used in railway transportation is presented. The horizontally movement of the pantograph (zigzag) is sensed in real time using a monitoring camera. The system is composed by a NI MyRIO controller equipped with FPGA and Real Time technologies which communicates with a laptop as the host computer. A LabVIEW application monitors the zigzag motion of pantograph, displays it in real time and logs the information into document files in order the information to be post processed. As well, the application can save the camera images into .avi file.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8466302/",
        "year": "2018"
    },
    {
        "title": "Intelligent Web Caching for E-learning Log Data",
        "abstract": "E-learning has been a common online service to support teaching and learning in education. Universiti Teknologi Malaysia (UTM) has been using such service that is known as e-Learning@UTM since 2005. The demand for e-learning content increases dramatically every semester. The performance of e-learning servers reduces when the number of users for each semester keeps growing. Hence users often experience poor performance in accessing the e-learning contents or downloading files. Such problems are due to the problem in the performance of servers, network infrastructure and majority of users tend to access the same piece of information repetitively. Web caching has been recognized as an effective scheme to reduce service bottleneck, userspsila access latency and network traffic. Therefore this paper will discuss an alternative way to tackle these problems by implementing a log data detection tool. This tool is capable to automatically directing either to cache or not to cache the objects in a document based on the log data (number of object hits, script size of objects, and time to receive object) in e-Learning@UTM to enhance such Web access.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5071972/",
        "year": "2009"
    },
    {
        "title": "Intelligent Web Image Retrieval System Using User Log (ICCAS 2007)",
        "abstract": "E-business sites and shopping mall sites deal with a lot of image information. To find a specific image from image sources, we usually use web search engines or image database engines. But, the feature based retrieval capabilities of these systems are quite limited, especially for the web images. This Paper presents Web Image Metadata Mining System For E-business Intelligence. We propose the indexing techniques and color based image classification and representation schemes of user log. The system keeps track of user's preferences by generating user query logs and automatically adds more search information to subsequent user queries. To demonstrate the usefulness of the proposed system, some experimental results showing and precision are also explained.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4406946/",
        "year": "2007"
    },
    {
        "title": "Interactive Recovery of Requirements Traceability Links Using User Feedback and Configuration Management Logs",
        "abstract": "Traceability links between requirements and source code can assist in software maintenance tasks. There are some automatic traceability recovery methods. Most of them are similarity-based methods recovering links by comparing representation similarity between requirements and code. They cannot work well if there are some links independent of the representation similarity. Herein to cover weakness of them and improve the accuracy of recovery, we propose a method that extends the similarity-based method using two techniques: a log-based traceability recovery method using the configuration management log and a link recommendation from user feedback. These techniques are independent of the representation similarity between requirements and code. As a result of applying our method to a large enterprise system, we successfully improved both recall and precision by more than a 20 percent point in comparison with singly applying the similarity-based method (recall: 60.2% to 80.4%, precision: 41.1% to 64.8%).",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-19069-3_16",
        "year": "2015"
    },
    {
        "title": "Introducing and analysis of the Windows 8 event log for forensic purposes",
        "abstract": "All operating systems are employing some sort of logging mechanism to track and note users activities and Microsoft Windows is not an exception. Log Analysis is one of the important parts of Windows forensics process. The Windows event log system introducing in Windows NT was released with a new feature for Microsoft Windows family and since then went through several major changes and updates. The event log experienced major updated in Windows 8. This paper first introduces Windows 8 event log format and then proceeds with explaining methods for analyzing the logs for digital investigation and incident handling. The main contributions of this paper are introducing Windows8 logging service and forensic examination of it.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-319-20125-2_13",
        "year": "2015"
    },
    {
        "title": "Introduction to special issue on query log analysis: Technology and ethics",
        "abstract": "No abstract available.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1409221",
        "year": "2008"
    },
    {
        "title": "Inversion of electromagnetic logging measurements using an oblique coordinate system",
        "abstract": "Electromagnetic logging methods as known in the oil industry are methods for determining the electrical conductivity distribution around a borehole from the low frequency field measurements (either electrode or induction logging). The authors study the reconstruction of the three-dimensional (3D) conductivity around a borehole in a highly deviated formation with invasions. In most available methods, the dipping bed environment is approximated using the staircase discretization grid. In contrast, they have modeled the dipping bed environment using an oblique coordinate system. This coordinate system has a vertical axis, which is defined to coincide with the borehole axis. By using this oblique coordinate system, they have obtained some advantageous above the usual approaches. Firstly, the use of staircase discretization grid can be avoided. This means that the discretization errors can be reduced, and the authors can suffice with less discretization grid to obtain the results with the same degree of accuracy of the problem formulated in the Cartesian coordinate system. Secondly, the contrast source inversion (CSI) method based on the integral equation formulated in the oblique coordinate system allows us to include some a priori information about the conductivity distribution in a simple fashion.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/771560/",
        "year": "1999"
    },
    {
        "title": "Investigating daily temporal patterns of social media usage by cellphone log",
        "abstract": "Mobile phone not only becomes an important extended-self but also provides opportunity for researchers to record and investigate people's daily behaviors through it. The central goal of the study is to understand people's daily life routine as well as how they use cellphone to communicate and interact with others. Compared with conventional methods such as diary method or experience sampling method, the study collected, visualized and analyzed logs of social media uses of thirty-five participants' cellphones for further interview. The results show that combining one's daily movement (including speed and location features) and apps usage timeline can help to discover routine patterns. The daily moving map and timeline of uses of apps shows both spatial and temporal regular patterns. In conclusion, daily activity data collected from new technology is believed to help social scientists to discover patterns in advance to develop in-depth questions for qualitative interviews.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8203537/",
        "year": "2017"
    },
    {
        "title": "Is my event log complete? ‚Äî A probabilistic approach to process mining",
        "abstract": "Process mining is a technique for extracting process models from event logs recorded by information systems. Process mining approaches normally rely on the assumption that the log to be mined is complete. Checking log completeness is known to be a difficult issue. Except for some trivial cases, checkable criteria for log completeness are not known. We overcome this problem by taking a probabilistic point of view. In this paper, we propose a method to compute the probability that the event log is complete. Our method provides a probabilistic lower bound for log completeness for three subclasses of Petri nets, namely, workflow nets, T-workflow nets, and S-workflow nets. Furthermore, based upon the complete log obtained by our methods, we propose two specialized mining algorithms to discover T-workflow nets and S-workflow nets, respectively. We back up our theoretical work with empirical studies that show that the probabilistic bounds computed by our method are reliable.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6006848/",
        "year": "2011"
    },
    {
        "title": "IT Equipment Monitoring and Analyzing System for Forecasting and Detecting Anomalies in Log Files Utilizing Machine Learning Techniques",
        "abstract": "The ability to detect anomalies in application log files has attracted the attention of researchers over the past decade as it has become a challenging issue. Intuitively, a noticeable variation in the performance can be as a result of some natural causes (e.g., CPU workload variations and memory leaks) or from internal anomalies or errors that may cause performance failure or application crash. In here, an account of prediction and detection of the performance anomalies together with their causes has been reported. A framework for the detection of anomalies was particularly targeted onto the application log files whereby some quantity of historical data was acquired and analyzed. From the data set, a correlation analysis was demonstrated which data was then submitted for Machine-Learning (ML) forecasting and anomaly detection algorithms. The best algorithms were chosen based on accuracy and precision. In the second phase, CPU usage and memory utilization for the data points collected previously were analyzed. From the obtained results it was evident that combining the parameters for approximation aided by time-series models with ML forecasting and anomaly detection techniques provided excellent results as regards to prediction of performance anomalies. And the framework is robust enough to identify the applications causing these anomalies and abnormal behaviors.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/8465400/",
        "year": "2018"
    },
    {
        "title": "K-means Application for Anomaly Detection and Log Classification in HPC",
        "abstract": "Detecting anomalies in the flow of system logs of a high performance computing (HPC) facility is a challenging task. Although previous research has been conducted to identify nominal and abnormal phases; practical ways to provide system administrators with a reduced set of the most useful messages to identify abnormal behaviour remains a challenge. In this paper we describe an extensive study of logs classification and anomaly detection using K-means on real HPC unlabelled data extracted from the Curie supercomputer. This method involves (1) classifying logs by format, which is a valuable information for admin, then (2) build normal and abnormal classes for anomaly detection. Our methodology shows good performances for clustering and detecting abnormal logs.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-60045-1_23",
        "year": "2017"
    },
    {
        "title": "Kiosk Usage Measurement using a Software Logging Tool",
        "abstract": "Rural PC kiosks have become prominent recently as a way to impact socio-economic development through computing technology. Despite the significant backing these projects receive from governments and other large organizations, there are very few rigorous studies which measure their actual impact and utility. We have developed and deployed a software PC logging tool that allows us to gain exact quantitative insight into the usage statistics of kiosks on which the tool is installed. In field trials in Maharashtra and Uttar Pradesh, India, we collected over 120 days of software tool logging data from 13 separate kiosks, while we also questioned the kiosk operators during the same period. We show some evidence that the software based logging tool complements the existing survey based and other ethnographic approaches for data collection. We also show that the tool does a better job in gathering certain usage statistics as compared to questioning the kiosk operator",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4085546/",
        "year": "2006"
    },
    {
        "title": "Knowledge discovery in web traffic log: A case study of facebook usage in kasetsart university",
        "abstract": "Recognizing and understanding knowledge flow between user interactions in social networks are valuable for sociology, economy, political science, and marketing. In this paper, we present a methodology in order to extract information and discover knowledge from a web traffic log. Our study is based on traffic and login history logs of Kasetsart University's network during a 7-days period from March 1-7, 2011. The summarized HTTP sessions show 39,046 distinct users together with 25,894 IP addresses. We conduct a pattern analysis in six aspects: The Origin of HTTP Requests, Distribution of HTTP Requests at the level of hostname, Time spent communicating online, Overall Traffic Workload Analysis, Facebook Traffic Workload Analysis and Web Access Patterns. The results reveal many interesting patterns and knowledge from raw data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6261960/",
        "year": "2012"
    },
    {
        "title": "Knowledge extraction of the behaviour of software developers by the analysis of time recording logs",
        "abstract": "Software development project management has a poor reputation in terms of avoiding cost and schedule overruns. The cause of this situation is based on the feature of the software development process that is characterized by quickly growing complexity and change. Therefore, there are many uncertainties to define exactly the necessary time to complete a tasks according to the person's performance. In this scenario Soft-Computing techniques may offer new approaches with the aim of helping the participants of the project to manage their time, give priority to their activities and readjust the work to complete satisfactorily the project tasks. This work presents an automatic features extraction process with the aim of defining the elements involved in a software project. This knowledge is represented by means fuzzy sets and fuzzy prototypes. The source of data is the Personal Software Project time recording logs. A preliminary experiment illustrates the feasibility of this approach.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5584364/",
        "year": "2010"
    },
    {
        "title": "Large scale query log analysis of re-finding",
        "abstract": "Although Web search engines are targeted towards helping people find new information, people regularly use them to re-find Web pages they have seen before. Researchers have noted the existence of this phenomenon, but relatively little is understood about how re-finding behavior differs from the finding of new information. This paper dives deeply into the differences via analysis of three large-scale data sources: 1) query logs (queries, clicks, result impressions), 2) Web browsing logs (URL visits), and 3) a daily Web crawl (page content). It appears that people learn valuable information about the pages they find that helps them re-find what they are looking for later; compared to the initial finding query, re-finding queries are typically shorter, and rank the re-found URL higher. While many instances of re-finding probably serve as a type of bookmark for a known URL, others seem to represent the resumption of a previous task; results clicked at the end of a session are more likely than those at the beginning to be re-found during a later session, while re-finding is more likely to happen at the beginning of a session than at the end. Additionally, we observe differences in cross-session and intra-session re-finding that may indicate different types of re-finding tasks. Our findings suggest there is a rich opportunity for search engines to take advantage of re-finding behavior as a means to improve the search experience.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1718512",
        "year": "2010"
    },
    {
        "title": "Large-scale log analysis of digital reading",
        "abstract": "In this paper, we address daily reading practices of the general public in Russia analyzing 10 months of log data from the commercial ebook site Bookmate. We study different reading characteristics with ebooks, i.e. the reading volume and preferences, reading schedule, reading speed and reading style (including parallel reading patterns and book abandonment rates), with respect to reader gender, book length and genre of the book. We find that book genres impact certain reading behaviors, while gender differences or book length seem to play less of a role in ebook reading. Parallel book reading and book abandonment occur very frequently, possibly pointing towards changing reading behaviors in the ebook environment. The obtained insights demonstrate the high potential of log analysis for book reading studies.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=3017491",
        "year": "2016"
    },
    {
        "title": "Lattice decoding can achieve 1/2 log(1+SNR) on the AWGN channel using nested codes",
        "abstract": "We demonstrate that using dithering techniques the power constrained AWGN channel can be transformed into a module lattice additive noise channel, effectively increasing the SNR by one. This allows lattice decoding to be optimal for SNRs for which a pair of \"good\" nested lattices exist.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/935988/",
        "year": "2001"
    },
    {
        "title": "LBMF: Log-Bilinear Matrix Factorization for Recommender Systems",
        "abstract": "Collaborative filtering techniques have been successfully applied in recommender systems recently. In order to improve recommendation accuracy for better user experience, the review texts should be exploited due to its rich information about users‚Äô explicit preferences and items‚Äô features, which cannot be fully revealed only by rating scores. In this paper, we propose an effective algorithm called LBMF to explore review texts and rating scores simultaneously. We directly correlate user and item latent dimensions with each word in review texts and ratings in our model, so semantic word vectors can be easily learned and effectively clustered based on rating values. On the other hand, the learned semantic word vectors can justify the rating values, which can promote better learning of user and item latent vectors for rating prediction. The learned latent dimensions by our model can reasonably explain why users rated items the way they did. This revelation can promote better modeling of user profiles and item information, and enable further analysis of user behaviors. Experimental results on several real-world datasets demonstrate the efficiency and effectiveness of LBMF comparing to the state-of-the-art models.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-31753-3_40",
        "year": "2016"
    },
    {
        "title": "Learning Interactions from Web Service Logs",
        "abstract": "Web services are typically involved in various types of interaction during their lifespan. They may participate as components in more complex services (composition) or replace unavailable services (substitution). Identifying the invocations that are part of the same interaction relationship and the nature of these relationships provides support for mashup developers. In this paper, we propose a novel approach for discovering composition and substitution relationships from service logs. We introduce a technique to correlate events that are part of the same relationship. We use association rule algorithms to determine the most frequent item-sets of correlated events. We infer composition and substitution relationships from these item-sets and derive a multi-relation network of Web services. Experiments show that 80% of the interaction relationships can be learned with 70% precision.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-64471-4_22",
        "year": "2017"
    },
    {
        "title": "Leonardo Log: Copyright restrictions prevent ACM from providing the full text for this work.",
        "abstract": "No abstract available.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1178982",
        "year": "2006"
    },
    {
        "title": "LEP accelerator logging system using on-line database",
        "abstract": "The performance and efficiency of LEP depend on a multitude of factors, including particle beam characteristics, physics parameters, hardware settings, and environmental conditions. Often, these factors interact in unexpected ways and affect the machine performance. In January 1992, a project was started to create a unique logging system using an on-line database. One year's worth of data was to be kept on-line, which was estimated to be a total of 8 GByte. The systems concerned are of different nature (particle beam profile, power converter current, meteorological data, magnet temperature, lep-mode, ‚Ä¶), require sampling at different frequencies (from seconds to several hours) and are of different sample size (from 10 MByte to 1 GByte per year). Major performance criteria included rapid logging of data for useful real-time monitoring of compound measurements, and rapid retrieval and correlation of large amounts of data for efficient off-line analysis.For the database design the NIAM methodology was used as well as some interesting techniques such as tagging the rows with timeslots instead of timestamps and row packing for storage minimisation. A complex structure of servers and clients takes care of data gathering, data logging and management of all real time measurement and logging requests. Several tools have been developed to make the data correlation transparent to non-database experts.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/016890029491527X",
        "year": "1994"
    },
    {
        "title": "Leveraging Community-Generated Videos and Command Logs to Classify and Recommend Software Workflows",
        "abstract": "Users of complex software applications often rely on inefficient or suboptimal workflows because they are not aware that better methods exist. In this paper, we develop and validate a hierarchical approach combining topic modeling and frequent pattern mining to classify the workflows offered by an application, based on a corpus of community-generated videos and command logs. We then propose and evaluate a design space of four different workflow recommender algorithms, which can be used to recommend new workflows and their associated videos to software users. An expert validation of the task classification approach found that 82% of the time, experts agreed with the classifications. We also evaluate our workflow recommender algorithms, demonstrating their potential and suggesting avenues for future work.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=3173859",
        "year": "2018"
    },
    {
        "title": "Leveraging Site Search Logs to Identify Missing Content on Enterprise Webpages",
        "abstract": "Online visitors often do not find the content they were expecting on specific pages of a large enterprise website, and subsequently search for it in site‚Äôs search box. In this paper, we propose methods to leverage website search logs to identify missing or expected content on webpages on the enterprise website, while showing how several scenarios make this a non-trivial problem. We further discuss how our methods can be easily extended to address concerns arising from the identified missing content.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-56608-5_41",
        "year": "2017"
    },
    {
        "title": "Life log system based on tactile sound",
        "abstract": "In this paper, we propose a new life log system which estimates touching object based on a contact sound. A life log system records our behaviors for recommending appropriate information depending on logs. There are some devices including a camera, a microphone and a GPS for recording our actions. However, those devices record too rich information to be accepted in terms of privacy. In this research we focus on the sound when we touch/manipulate objects. A piezoelectric device on a fingernail records touching sound propagating through a fingertip. Since the recorded sound depends on touching objects, we can record what we touched. This can be used as a new life log which assures secrecy. We show that our prototype system recognizes 12 different actions with 94.4% accuracy.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-14064-8_42",
        "year": "2010"
    },
    {
        "title": "Little web log and big social event: The public discourse of internet",
        "abstract": "On the account of civic society by social scientists, the capacity of civic society has experienced a dual aggression from economic and politic power. However, the existence and development of Internet provides a new capacity for the renaissance of civic society. Especially in the age from WEB 1.0 to WEB 2.0, the public transform their role from an audience to a speaker; this transfer fosters a possibility to boost the formation of public discourse on the Internet. Public discourse as a powerful Web force, its influence extends beyond the Web and reaches the authentic society which brings latent consequences. This paper explores the formative processes of public discourse on the internet and discusses its latent consequence, moreover, as in an overexposed society; sociology should play a positive role in this formative process as a guide.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5271770/",
        "year": "2009"
    },
    {
        "title": "Log analysis for data protection accountability",
        "abstract": "Accountability is increasingly recognised as a cornerstone of data protection, notably in European regulation, but the term is frequently used in a vague sense. For accountability to bring tangible benefits, the expected properties of personal data handling logs (used as ‚Äúaccounts‚Äù) and the assumptions regarding the logging process must be defined with accuracy. In this paper, we provide a formal framework for accountability and show the correctness of the log analysis with respect to abstract traces used to specify privacy policies. We also show that compliance with respect to data protection policies can be checked based on logs free of personal data, and describe the integration of our formal framework in a global accountability process.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-319-06410-9_12",
        "year": "2014"
    },
    {
        "title": "Log analysis of Estonian internet voting 2013‚Äì2014",
        "abstract": "This paper presents analysis of Internet voting system logs of 2013 local municipal and 2014 European Parliament elections in Estonia. We study both sociodemographic characteristics of voters and technical aspects of voting. Special attention is paid to voting and verification sessions that can be considered irregular (e.g. inability to cast a valid vote or failed verifications). We observe several interesting phenomena, e.g. that older people are generally faster Internet voters and that women use the vote verification option significantly less than men.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-319-22270-7_2",
        "year": "2015"
    },
    {
        "title": "Log analysis of exploitation in cloud computing environment using automated reasoning",
        "abstract": "Recently server consolidation using virtualization leverages cloud computing. In cloud computing, we can apply centralized logging system using server consolidation. In this paper we propose a log analysis method in cloud computing environment using automated reasoning. On cloud computing providers, VM (virtual machine) monitoring is important to detect security incident. We discuss how to monitor VM, formatting and analyzing logs. Automated reasoning is more effective to retrieves information from large amount of log string. In proposed system, VM log is represented as clausal form and processed by FoL (First order Logic) theorem prover. We also present the numerical output of proposed system.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-17534-3_41",
        "year": "2010"
    },
    {
        "title": "Log Analysis of Map-Based Web Page Search on Digital City Kyoto",
        "abstract": "This paper analyzes how people use map-based user interfaces of regional information systems on the Internet. The analysis is based on the log data of InfoMap, which is supplied on the Web site of the Digital City Kyoto prototype. InfoMap provides a map-based user interface that has useful functions enabling users to choose links to Web pages using digital maps. The log data of InfoMap was recorded automatically on the Web server and enabled us to analyze access frequencies, function usages, and content selections. In this paper, the characteristics of the map-based user interface compared with the characteristics of traditional text-based search engines are explained.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-45636-8_18",
        "year": "2002"
    },
    {
        "title": "Log Analysis of Multilingual Image Searches in Flickr",
        "abstract": "In this paper, we summarize our analysis over the logs of multilingual image searches in Flickr provided to iCLEF 2008 participants. We have studied: a) correlations between the language skills of searchers in the target language and other session parameters, such as success (was the image found?), number of query refinements, etc.; b) usage of specific cross-language search facilities; and c) users perceptions on the task (questionnaire analysis).",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-04447-2_30",
        "year": "2009"
    },
    {
        "title": "Log Analysis Using Hadoop",
        "abstract": "The explosive growth of the Web toward the end of the 20th century led to web scale data, particularly log files. Suddenly, everyone who had a web site generated lots and lots of web access logs that were initially used as to debug problems with a web site. Eventually, organizations realized that web access logs were a rich source of information about their customers and potential customers. Click stream analysis offered insights into customer behavior within a web site, and search query analysis offered examples of products and services most important to customers.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4302-4864-4_13",
        "year": "2014"
    },
    {
        "title": "Log Analysis, Monitoring, and Automation",
        "abstract": "If you are a web hosting provider, setting up web servers will be a fairly repetitive task to the extent that you might want to automate the whole process of creating and configuring the website. However, if you have a few websites to manage, setting up a web server and hosting your application will often be relatively straightforward. Once you have set up the web server, the changes in the configuration will be rare and only on a need basis. A typical web administrator spends far more time maintaining the web farm than configuring it. This chapter focuses on maintaining the web server. You will learn about log gathering, analysis, monitoring, and automation.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4842-1656-9_9",
        "year": "2016"
    },
    {
        "title": "Log Based Recovery with Low Overhead for Mobile Computing Systems",
        "abstract": "The article proposes a recovery protocol for applications in mobile computing environment by combining movement based checkpointing with message logging. The focus of the scheme is to have a low overhead to the normal application execution due to the recovery scheme. The cost to locate the mobile host, the number of messages exchanged over the wireless media, and the size of recovery related data stored with a mobile host, have been reduced. We also include the correctness proof and performance analysis of our protocol in the presentation.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-19542-6_125",
        "year": "2011"
    },
    {
        "title": "Log Delta Analysis: Interpretable Differencing of Business Process Event Logs",
        "abstract": "This paper addresses the problem of explaining behavioral differences between two business process event logs. The paper presents a method that, given two event logs, returns a set of statements in natural language capturing behavior that is present or frequent in one log, while absent or infrequent in the other. This log delta analysis method allows users to diagnose differences between normal and deviant executions of a process or between two versions or variants of a process. The method relies on a novel approach to losslessly encode an event log as an event structure, combined with a frequency-enhanced technique for differencing pairs of event structures. A validation of the proposed method shows that it accurately diagnoses typical change patterns and can explain differences between normal and deviant cases in a real-life log, more compactly and precisely than previously proposed methods.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-23063-4_26",
        "year": "2015"
    },
    {
        "title": "Log File Analysis with Context-Free Grammars",
        "abstract": "Classical intrusion analysis of network log files uses statistical machine learning or regular expressions. Where statistically machine learning methods are not analytically exact, methods based on regular expressions do not reach up very far in Chomsky‚Äôs hierarchy of languages. This paper focuses on parsing traces of network traffic using context-free grammars. ‚ÄúGreen grammars‚Äù are used to describe acceptable log files while ‚Äúred grammars‚Äù are used to represent known intrusion patterns. This technique can complement or augment existing approaches by providing additional precision. Analytically, the technique is also more powerful than existing techniques that use regular expressions.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-41148-9_10",
        "year": "2013"
    },
    {
        "title": "Log File Analyzing in Intelligent Transportation Systems Development",
        "abstract": "Intelligent Transportation Systems (ITS) consist of a large number of vehicles and stop monitors, as well as operations management center and servers. Their development can be challenging due to 3rd party black-box components and limited debugging visibility. Our solution is log file analysis. We developed a tool framework called LOGDIG, which differs from related work by supporting also very complex system behaviors discovered by recurrent and backward processing of the log files. The tool was successfully used to find and fix faulty timing of bus stop monitor information in a product called ELMI.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-49094-6_40",
        "year": "2016"
    },
    {
        "title": "Log Visualization of Intrusion and Prevention Reverse Proxy Server against Web Attacks",
        "abstract": "SQL Injection Attack (SQLIA) has made to the top of the OWASP, Top 10 Web Application Security Risks in 2013 and in 2010. The explosive use of web application with very little emphasis lay on securing it make this attack becoming more popular. Various methods have been discussed and proposed as countermeasure to the attack. Unfortunately, most of them are seen to be not comprehensive enough to address any kind of issues an organization might have when it comes to hardening the web security such as technical and financial matter for instance. This study presents a way to prevent and detect intrusion through the deployment of reverse proxy with an intrusion and prevention mechanism built in against web attacks especially SQLIA. With the flexibility offered in server logging process, we obtain and analyse preferred data to visualize the type of attack based on logs information. Our graph visualization development monitors three web security aspects, i.e. the top traffic blocked attempted by IP address, number of regular expression rules violated and detect the rules of intrusion detection.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6702833/",
        "year": "2013"
    },
    {
        "title": "Log Wavelet Leaders Cumulant Based Multifractal Analysis of EVI fMRI Time Series: Evidence of Scaling in Ongoing and Evoked Brain Activity",
        "abstract": "Classical within-subject analysis in functional magnetic resonance imaging (fMRI) relies on a detection step to localize which parts of the brain are activated by a given stimulus type. This is usually achieved using model-based approaches. Here, we propose an alternative exploratory analysis. The originality of this contribution is twofold. First, we propose a synthetic, consistent, and comparative overview of the various stochastic processes and estimation procedures used to model and analyze scale invariance. Notably, it is explained how multifractal models are more versatile to adjust the scaling properties of fMRI data but require more elaborated analysis procedures. Second, we bring evidence of the existence of actual scaling in fMRI time series that are clearly disentangled from putative superimposed nonstationarities. By nature, scaling analysis requires the use of long enough signals with high frequency sampling rate. To this end, we make use of a localized 3-D echo volume imaging (EVI) technique, which has recently emerged in fMRI because it allows very fast acquisitions of successive brain volumes. High temporal resolution EVI fMRI data have been acquired both in resting state and during a slow event-related visual paradigm. A voxel-based systematic multifractal analysis has been performed over both kinds of data. Combining multifractal attribute estimates together with paired statistical tests, we observe significant scaling parameter changes between ongoing and evoked brain activity, which clearly validate an increase in long memory and suggest a global multifractality decrease effect under activation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4740319/",
        "year": "2008"
    },
    {
        "title": "Log who's playing: psychophysiological game analysis made easy through event logging",
        "abstract": "Modern psychophysiological game research faces the problem that for understanding the computer game experience, it needs to analyze game events with high temporal resolution and within the game context. This is the only way to achieve greater understanding of gameplay and the player experience with the use of psychophysiological instrumentation. This paper presents a solution to recording in-game events with the frequency and accuracy of psychophysiological recording systems, by sending out event byte codes through a parallel port to the psychophysiological signal acquisition hardware. Thus, psychophysiological data can immediately be correlated with in-game data. By employing this system for psychophysiological game experiments, researchers will be able to analyze gameplay in greater detail in future studies.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-540-88322-7_15",
        "year": "2008"
    },
    {
        "title": "Log Who‚Äôs Playing: Psychophysiological Game Analysis Made Easy through Event Logging",
        "abstract": "Modern psychophysiological game research faces the problem that for understanding the computer game experience, it needs to analyze game events with high temporal resolution and within the game context. This is the only way to achieve greater understanding of gameplay and the player experience with the use of psychophysiological instrumentation. This paper presents a solution to recording in-game events with the frequency and accuracy of psychophysiological recording systems, by sending out event byte codes through a parallel port to the psychophysiological signal acquisition hardware. Thus, psychophysiological data can immediately be correlated with in-game data. By employing this system for psychophysiological game experiments, researchers will be able to analyze gameplay in greater detail in future studies.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-88322-7_15",
        "year": "2008"
    },
    {
        "title": "Log- and Model-Based Techniques for Security-Sensitive Tackling of Obstructed Workflow Executions",
        "abstract": "Imposing access control onto workflows considerably reduces the set of users authorized to execute the workflow tasks. Further constraints (e.g. Separation of Duties) as well as unexpected unavailability of users may finally obstruct the successful workflow execution. To still complete the execution of an obstructed workflow, we envisage a hybrid approach. We first flatten the workflow and its authorizations into a Petri net and analyse for or encode the obstruction with a corresponding ‚Äúobstruction marking‚Äù. If a log is provided, we partition its traces into ‚Äúsuccessful‚Äù or ‚Äúobstructed‚Äù by replaying the log on the flattened net. An obstruction should then be solved by finding its nearest match from the list of successful traces. If no log is provided, the structural theory of Petri nets shall be used to provide a minimized Parikh vector, that may violate given firing rules, but reach a complete marking and by that, complete the workflow.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-55862-1_3",
        "year": "2017"
    },
    {
        "title": "Log-based architectures: using multicore to help software behave correctly",
        "abstract": "While application performance and power-efficiency are both important, application correctness is even more important. In other words, if the application is misbehaving, it is little consolation that it is doing so quickly or power-efficiently. In the Log-Based Architectures (LBA) project, we are focusing on a challenging source of application misbehavior: software bugs, including obscure bugs that only cause problems during security attacks. To help detect and fix software bugs, we have been exploring techniques for accelerating dynamic program monitoring tools, which we call \"lifeguards\". Lifeguards are typically written today using dynamic binary instrumentation frameworks such as Valgrind or Pin. Due to the overheads of binary instrumentation, lifeguards that require instructiongrain information typically experience 30X-100X slowdowns, and hence it is only practical to use them during explicit debug cycles. The goal in the LBA project is to reduce these overheads to the point where lifeguards can run continuously on deployed code. To accomplish this, we propose hardware mechanisms to create a dynamic log of instruction-level events in the monitored application and stream this information to one or more software lifeguards running on separate cores on the same multicore processor. In this paper, we highlight techniques and features of LBA that reduce the slowdown to just 2%--51% for sequential programs and 28%--51% for parallel programs.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1945034",
        "year": "2011"
    },
    {
        "title": "Log-Based Failure Analysis of Complex Systems: Methodology and Relevant Applications",
        "abstract": "Failure analysis is valuable to dependability engineers because it supports designing effective mitigation means, defining strategies to reduce maintenance costs, and improving system service. Event logs, which contain textual information about regular and anomalous events detected by the system under real workload conditions, represent a key source of data to conduct failure analysis. So far, event logs have been successfully used in a variety of domains. This chapter describes methodology and well-established techniques underlying log-based failure analysis. Description introduces the workflow leading to analysis results starting from the raw data in the log. Moreover, the chapter surveys relevant works in the area with the aim of highlighting main objectives and applications of log-based failure analysis. Discussion reveals benefits and limitations of logs for evaluating complex systems.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-88-470-2772-5_15",
        "year": "2013"
    },
    {
        "title": "Log-based indexing to improve web site search",
        "abstract": "Despite the success of global search engines, web site search is still problematic in its retrieval accuracy. In this paper, we try to improve the performance of the site search by combining a new source of evidence from web server logs. We propose a novel approach of using server log analysis to extract terms to build the web page index. Then, this log-based index is combined with the text-based and anchor-based index to provide a more complete view on the page content. Experiments have shown that it could improve the effectiveness of the web site search significantly.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1244186",
        "year": "2007"
    },
    {
        "title": "Log-Based Model to Enforce Data Consistency on Agnostic Fault-Tolerant Systems",
        "abstract": "Agnostic fault-tolerant systems cannot recover to a consistent state if a failure/crash occurs during a transaction. By their nature, inconsistent states are very difficult to be treated and recovered into the previous consistent state. One of the most common fault tolerance mechanisms consists in logging the system state whenever a modification takes place, and recovering the system to the system previous consistent state in the event of a failure. This principle was used to design a general recovering log-based model capable of providing data consistency on agnostic fault-tolerant systems. Our proposal describes how a logging mechanism can recover a system to a consistent state, even if a set of actions of a transaction were interrupted mid-way, due to a server crash. Two approaches of implementing the logging system are presented: on local files and on memory in a remote fault-tolerant cluster. The implementation of a proof of concept resorted to a previous proposed framework, which provides common relational features to NoSQL database management systems. Among the missing features, the previous proposed framework used in the proof of concept, was not fault-tolerant.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-62911-7_8",
        "year": "2017"
    },
    {
        "title": "Log-based personalization: the 4th web search click data (WSCD) workshop",
        "abstract": "WSCD 2014 is the fourth workshop on Web Search Click Data, following WSCD 2009, WSCD 2011 and WSCD 2012. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This series of workshops comes with new datasets based on logged user search behaviour and accompanying data mining challenges. This year the challenge and the workshop are focused on the tasks of personalization using logs.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2556207",
        "year": "2014"
    },
    {
        "title": "Log-End Cut-Area Detection in Images Taken from Rear End of Eucalyptus Timber Trucks",
        "abstract": "The visual estimation of log volume and size distribution of eucalyptus logs on a truck is a challenging task. In Thailand, inspectors at paper mills typically perform this task. The information is used to determine whether the logs pass the criteria for the mill and to find the appropriate price. This method is far from accurate and not efficient. This paper presents a new approach to automatically detects eucalyptus logend cut area from rear-end images of eucalyptus timber trucks. The method used machine learning and image processing techniques. It consists of three parts: timber truck detection, log segmentation, and log counting. The proposed system was tested with 300 images of timber truck dataset and achieved an average accuracy of 94.45% in log segmentation and 2.71% of false negative.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8457388/",
        "year": "2018"
    },
    {
        "title": "Log-Gabor Weber Descriptor for Face Recognition",
        "abstract": "It is well recognized that image representation is the most fundamental task of the face recognition, effective and efficient image feature extraction not only has small intraclass variations and large interclass similarity but also robust to the impact of pose, illumination, expression and occlusion. This paper proposes a new local image descriptor for face recognition, named Log‚ÄìGabor Weber descriptor (LGWD). The idea of LGWD is based on the image Log-Gabor wavelet representation and the Weber local binary pattern(WLBP) features. The main motivation of the LGWD is to enhance the multiple scales and orientations Log-Gabor magnitude and phase feature by applying the WLBP coding method. Histograms extracted from the encoded magnitude and phase images are concatenated into one to form the image description finally. The experimental results on the ORL, Yale and UMIST face database verify the representation ability of our proposed descriptor.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-16628-5_39",
        "year": "2015"
    },
    {
        "title": "Log-gain Principles for Metabolic P Systems",
        "abstract": "Metabolic P systems, shortly MP systems, are a special class of P systems, introduced for expressing biological metabolism. Their dynamics are computed by metabolic algorithms which transform populations of objects according to a mass partition principle, based on suitable generalizations of chemical laws. In this paper, the basic principles of MP systems are formulated for introducing the Log-gain principles, and it is shown how to use them for constructing MP models from experimental data of given metabolic processes.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-88869-7_28",
        "year": "2009"
    },
    {
        "title": "Log-likelihood metric for LDPC coded BDPSK-OFDM transmission",
        "abstract": "We investigate the performance of LDPC codes with BDPSK-OFDM modulation over the AWGN channel with unknown carrier phase. The proposed A-TSOI-LLR metric is performing better and more tolerant to SNR estimation error than the Gaussian metric.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6276422/",
        "year": "2012"
    },
    {
        "title": "Log-likelihood ratio (LLR) conversion schemes in orthogonal code hopping multiplexing",
        "abstract": "Log-likelihood ratio (LLR) conversion schemes are proposed to reduce the effect of perforations that occur in orthogonal code hopping multiplexing (OCHM), which was previously proposed to accommodate more downlink channels than the number of orthogonal codewords. The proposed LLR conversion schemes greatly reduce the required signal-to-noise ratio (SNR) in channel decoding even when the perforation probability is high. The performance of the proposed schemes is evaluated by simulation in terms of the required E/sub b//N/sub 0/ for a 1% block error rate.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1187371/",
        "year": "2003"
    },
    {
        "title": "Log-likelihood ratio based detection ordering for the V-BLAST",
        "abstract": "We propose a new detection ordering for the V-BLAST. The main idea is to detect and cancel sub-streams in order of the magnitude of log-likelihood ratio (LLR), i.e. the symbol with the largest magnitude of LLR is detected first. The motivation is that the reliability of data decision increases with increasing magnitude of LLR. As a result, the error propagation associated with a wrong decision and the resulting error probability for the remaining sub-streams can be minimized. It is shown that the proposed LLR-based ordering significantly outperforms the conventional SNR-based ordering. Simplified LLR-based ordering and envelope-based ordering that require a much less computation, but provide a performance virtually identical to the LLR-based ordering, are also proposed.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1258248/",
        "year": "2003"
    },
    {
        "title": "Log-likelihood ratio based successive interference cancellation in CDMA systems",
        "abstract": "We propose a new successive interference cancellation (SIC) scheme that cancels the interference in order of the magnitude of log-likelihood ratio (LLR) in CDMA systems. The motivation is that the reliability of a decision increases with increasing magnitude of LLR. As a result, the error propagation associated with a wrong direction and the resulting error probability for the remaining users can be minimized. We show that the magnitude of LLR depends on the signal strength as well as the instantaneous multi-user interference (MUI), and the proposed LLR-based SIC scheme, taking both the signal strength and the instantaneous interference into account, significantly outperforms the conventional signal strength-based SIC scheme. We also examine the effect of channel estimation error for various SIC schemes.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1208818/",
        "year": "2003"
    },
    {
        "title": "Log-linear companding‚ÄîA digital companding technique",
        "abstract": "A new method of near-logarithmic companding is proposed. The method consists of first uniformly quantizing each sample and then processing the resulting binary number digitally. This is in contrast to the usual scheme of first compressing the input and then uniformly quantizing it. The method presented here is extremely simple to implement and requires no arithmetic operations. The inverse transformation is equally simple to implement.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1449333/",
        "year": "1969"
    },
    {
        "title": "Log-logistic flood frequency analysis",
        "abstract": "The log-logistic (LLG) distribution is evaluated for flood frequency analysis. Some of its properties and methods of parameter estimation are given, including a new method based on generalised least squares (GLS). The performance of the log-logistic distribution is compared with those of the generalised extreme value (GEV), three parameter log-normal (LN3) and three parameter Pearson (P3) distributions. The results are reported in terms of empirical distribution function (EDF) tests of goodness of fit, on both individual and regional flood series through the application of these distributions to a set of reasonably long annual maximum series for part of Scotland. Some reproductive properties of the LLG and GEV are also compared. In terms of four key properties the LLG performs better than the GEV, LN3 and P3 distributions and is thus commended for further analysis.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0022169488900157",
        "year": "1988"
    },
    {
        "title": "LogBase: a scalable log-structured database system in the cloud",
        "abstract": "Numerous applications such as financial transactions (e.g., stock trading) are write-heavy in nature. The shift from reads to writes in web applications has also been accelerating in recent years. Write-ahead-logging is a common approach for providing recovery capability while improving performance in most storage systems. However, the separation of log and application data incurs write overheads observed in write-heavy environments and hence adversely affects the write throughput and recovery time in the system.In this paper, we introduce LogBase -- a scalable log-structured database system that adopts log-only storage for removing the write bottleneck and supporting fast system recovery. It is designed to be dynamically deployed on commodity clusters to take advantage of elastic scaling property of cloud environments. LogBase provides in-memory multiversion indexes for supporting efficient access to data maintained in the log. LogBase also supports transactions that bundle read and write operations spanning across multiple records. We implemented the proposed system and compared it with HBase and a disk-based log-structured record-oriented system modeled after RAMCloud. The experimental results show that LogBase is able to provide sustained write throughput, efficient data access out of the cache, and effective system recovery.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2336673",
        "year": "2012"
    },
    {
        "title": "Logging and Log Analysis",
        "abstract": "Operating systems and applications typically come with mechanisms for reporting errors as well as security-relevant actions such as users logging on and off. These events are reported as entries in log files. The objective of logging is to make these events transparent and comprehensible. The log files can be used to analyze and optimize services as well as to detect and diagnose security breaches.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-24474-2_5",
        "year": "2011"
    },
    {
        "title": "Logging based IP Traceback in switched ethernets",
        "abstract": "IP Traceback systems facilitate tracing of IP packets back to their origin, despite possibly forged or overwritten source address data. A common shortcoming of existing proposals is that they identify source network, but not the source host. Our work extends the traceback process to allow tracing of (switched) Ethernet frames. We build on SPIE (which operates at IP routers) to design and implement 'switch-SPIE'. Traffic logging is deployed in a 'switch-DGA' tap-box at each switch. The (switched) Ethernet traffic visibility issue is resolved with port mirroring, and the MAC address table establishes causality between source MAC address and source switch port. Our solution works for any network topology, as opposed to earlier layer 2 extensions to IP Traceback. We provide an implementation and experimental evaluation to establish the efficacy of our approach, with respect to processing overhead and memory use.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1355286",
        "year": "2008"
    },
    {
        "title": "Logging brand personality online: website content analysis of Middle Eastern and North African destinations",
        "abstract": "The travel and tourism industry has always been regarded as having the greatest potential in promoting peace and social harmony throughout an advancing global society. Despite this innate characteristic of bringing world cultures together, travel and tourism is vulnerable to existing economic, social and geopolitical conditions (Clements \u0026 Georgiou, 1998). As apprehension and insecurity creeps in, the image of a destination changes in the mind of a would-be traveller. Any perceived or actual risk associated with travelling to a region or country where one‚Äôs basic need for personal security is compromised affects the tourism system. Several scholars have assessed the impact of terrorism and security issues on tourism from a Western, European, and Asian Pacific perspective (Chen, Chen, 2003; Henderson, 2003). Few researchers have assessed how destinations in the MENA manage their brand image from the perspective of the traveller while these destinations are at the heart of civil, religious and political unrest. The purpose of this research was to evaluate the perceptions of travellers to Middle East and North African (MENA) destinations through their Internet travel blog postings. Using Aaker‚Äôs (1997) brand personality framework as a guide, the study compared keywords that potentially describes MENA for the purposes of developing marketing campaigns with positive messages about the region as a tourism destination. The study utilized CATPAC, a neural network analysis computer program optimized for analyzing text, to examine 346 online travel journal entries (travel blogs) from travellers who were visiting or had already visited any of the Middle East and North African (MENA) countries. Findings from this research suggest that the dimensions relating to brand personality for MENA are excitement and ruggedness. A discussion follows on the importance of Internet blogs as a source of customer information useful in conducting market research.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/3-211-32710-X_46",
        "year": "2006"
    },
    {
        "title": "Logging library migrations: a case study for the apache software foundation projects",
        "abstract": "Developers leverage logs for debugging, performance monitoring and load testing. The increased dependence on logs has lead to the development of numerous logging libraries which help developers in logging their code. As new libraries emerge and current ones evolve, projects often migrate from an older library to another one.In this paper we study logging library migrations within Apache Software Foundation (ASF) projects. From our manual analysis of JIRA issues, we find that 33 out of 223 (i.e., 14%) ASF projects have undergone at least one logging library migration. We find that the five main drivers for logging library migration are: 1) to increase flexibility (i.e., the ability to use different logging libraries within a project) 2) to improve performance, 3) to reduce effort spent on code maintenance, 4) to reduce dependence on other libraries and 5) to obtain specific features from the new logging library. We find that over 70% of the migrated projects encounter on average two post-migration bugs due to the new logging library. Furthermore, our findings suggest that performance (traditionally one of the primary drivers for migrations) is rarely improved after a migration.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2901769",
        "year": "2016"
    },
    {
        "title": "Logging residues from regeneration fellings for biofuel production‚Äìa GIS-based availability analysis in Finland",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0961953404001576",
        "year": "2005"
    },
    {
        "title": "LogMine: Fast Pattern Recognition for Log Analytics",
        "abstract": "Modern engineering incorporates smart technologies in all aspects of our lives. Smart technologies are generating terabytes of log messages every day to report their status. It is crucial to analyze these log messages and present usable information (e.g. patterns) to administrators, so that they can manage and monitor these technologies. Patterns minimally represent large groups of log messages and enable the administrators to do further analysis, such as anomaly detection and event prediction. Although patterns exist commonly in automated log messages, recognizing them in massive set of log messages from heterogeneous sources without any prior information is a significant undertaking. We propose a method, named LogMine, that extracts high quality patterns for a given set of log messages. Our method is fast, memory efficient, accurate, and scalable. LogMine is implemented in map-reduce framework for distributed platforms to process millions of log messages in seconds. LogMine is a robust method that works for heterogeneous log messages generated in a wide variety of systems. Our method exploits algorithmic techniques to minimize the computational overhead based on the fact that log messages are always automatically generated. We evaluate the performance of LogMine on massive sets of log messages generated in industrial applications. LogMine has successfully generated patterns which are as good as the patterns generated by exact and unscalable method, while achieving a 500√ó speedup. Finally, we describe three applications of the patterns generated by LogMine in monitoring large scale industrial systems.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2983358",
        "year": "2016"
    },
    {
        "title": "LogRank: An Approach to Sample Business Process Event Log for Efficient Discovery",
        "abstract": "Considerable amounts of business process event logs can be collected by modern information systems. Process discovery aims to uncover a process model from an event log. Many process discovery approaches have been proposed, however, most of them have difficulties in handling large-scale event logs. Motivated by PageRank, in this paper we propose LogRank, a graph-based ranking model, for event log sampling. Using LogRank, a large-scale event log can be sampled to a smaller size that can be efficiently handled by existing discovery approaches. Moreover, we introduce an approach to measure the quality of a sample log with respect to the original one from a discovery perspective. The proposed sampling approach has been implemented in the open-source process mining toolkit ProM. The experimental analyses with both synthetic and real-life event logs demonstrate that the proposed sampling approach provides an effective solution to improve process discovery efficiency as well as ensuring high quality of the discovered model.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-99365-2_36",
        "year": "2018"
    },
    {
        "title": "LogSed: Anomaly Diagnosis through Mining Time-Weighted Control Flow Graph in Logs",
        "abstract": "Detecting execution anomalies is very important to monitoring and maintenance of cloud systems. People often use execution logs for troubleshooting and problem diagnosis, which is time consuming and error-prone. There is great demand for automatic anomaly detection based on logs. In this paper, we mine a time-weighted control flow graph (TCFG) that captures healthy execution flows of each component in cloud, and automatically raise anomaly alerts on observing deviations from TCFG. We outlined three challenges that are solved in this paper, including how to deal with the interleaving of multiple threads in logs, how to identify operational logs that do not contain any transactional information, and how to split the border of each transaction flow in the TCFG. We evaluate the effectiveness of our approach by leveraging logs from an IBM public cloud production platform and two simulated systems in the lab environment. The evaluation results show that our TCFG mining and anomaly diagnosis both perform over 80% precision and recall on average.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8030620/",
        "year": "2017"
    },
    {
        "title": "Logtracker: learning log revision behaviors proactively from software evolution history",
        "abstract": "Log statements are widely used for postmortem debugging. Despite the importance of log messages, it is difficult for developers to establish good logging practices. There are two main reasons for this. First, there are no rigorous specifications or systematic processes to guide the practices of software logging. Second, logging code co-evolves with bug fixes or feature updates. While previous works on log enhancement have successfully focused on the first problem, they are hard to solve the latter. For taking the first step towards solving the second problem, this paper is inspired by code clones and assumes that logging code with similar context is pervasive in software and deserves similar modifications. To verify our assumptions, we conduct an empirical study on eight open-source projects. Based on the observation, we design and implement LogTracker, an automatic tool that can predict log revisions by mining the correlation between logging context and modifications. With an enhanced modeling of logging context, LogTracker is able to guide more intricate log revisions that cannot be covered by existing tools. We evaluate the effectiveness of LogTracker by applying it to the latest version of subject projects. The results of our experiments show that LogTracker can detect 199 instances of log revisions. So far, we have reported 25 of them, and 6 have been accepted.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3196328",
        "year": "2018"
    },
    {
        "title": "Low complexity OFDM receiver using Log-FFT for coded OFDM system",
        "abstract": "In this paper, we describe a low complexity orthogonal frequency-division multiplexing (OFDM) receiver using Log-FFT for coded OFDM system. The complexity of the Log-FFT depends on the size of the look-up table, which is determined by the bit width of logarithmic number systems (LNS) representation. In coded OFDM system, simulation results show that there is no degradation in bit error rate performance when only two fractional bits are used for the LNS. As the bit width is so small, the look-up table can be easily implemented using a few combinational logic gates. Comparing the complexity and power consumption of the Log-FFT butterfly module with those of fixed point FFT butterfly module, about 60% reduction can be achieved.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1010256/",
        "year": "2002"
    },
    {
        "title": "Low Overhead Log Replication for Main Memory Database System",
        "abstract": "Log replication is the key component of high available database system. To guarantee data consistency and reliability, modern database systems often use Paxos protocol to replicate log in multiple database instance sites. Since the replicated logs need to contain some metadata such as committed log sequence number (LSN), this increases the overhead of storage and network. It has significantly negative impact on the throughput in the update intensive work load. In this paper, we present an implementation of log replication and database recovery, which adopts the idea of piggybacking, i.e. committed LSN is embedded in the commit logs. This practice not only retains virtues of Paxos replication, but also reduces disk and network IO effectively, which enhances performance and decreases recovery time. We implemented and evaluated our approach in a main memory database system (Oceanbase), and found that our method can offer 1.3x higher throughput than traditional log replication with synchronization mechanism.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39958-4_13",
        "year": "2016"
    },
    {
        "title": "Low-activity spectrometric gamma-ray logging technique for delineation of coal/rock interfaces in dry blast holes",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0969804307000383",
        "year": "2007"
    },
    {
        "title": "Making sense of log management for security purposes‚Äìan approach to best practice log collection, analysis and management",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S1361372307700477",
        "year": "2007"
    },
    {
        "title": "MALL: a life log based music recommendation system and portable music player",
        "abstract": "We may like to listen to particular types of tunes under the particular situation or environment, such as events, weather, time, and place. However, it is not always easy to manually choose such particular types of tunes just by looking at metadata such as titles or artist names. It is effective and enjoyable if such tunes are automatically recommended after learning the tendency of the users. This paper presents MALL (Music Adviser with Life Log), a life log based music recommendation system and portable music player. The system records the history of listened tunes with the situation and environment on the Android-based portable music player. It then discovers association rules between the situation or environments and musical feature values of the tunes. Finally, the system recommends particular types of tunes based on the discovered association rules. We names this system MALL because it advises the tunes based on the life logs of the users.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2554861",
        "year": "2014"
    },
    {
        "title": "Managing the Windows Event Log",
        "abstract": "Windows is a complex application, to say the least. Things happen that the user is aware of‚Äîfor example, starting Microsoft Word‚Äîand things happen that the user may not be aware of, such as a privilege audit of a security access to a file on the C drive. In the latter case, the person sitting at the computer sees the folder open or an error about access being denied. The event log contains a record of these events to keep administrators informed about what actions are being taken on a computer.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4842-1851-8_2",
        "year": "2016"
    },
    {
        "title": "Mathematical and graphical interpretation of the log-normal law for particle size distribution analysis",
        "abstract": "The log-normal law serves as an excellent mathematical model for particle size distribution analysis to the extent that the various mathematical terms are properly interpreted. In particular, the probability density function (or frequency function) is shown to be more than an abstract mathematical variable. Particular attention is given to (a) the proper methods of analyzing log-normal distributions, quite different from normal distributions; (b) the physical significance of the mathematical variables and how they relate to experimentally observed data; (c) data-gathering; and (d) the correct modes of portraying and analyzing experimental data graphically.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0095852264900698",
        "year": "1964"
    },
    {
        "title": "Memetic Algorithms for Mining Change Logs in Process Choreographies",
        "abstract": "The propagation and management of changes in process choreographies has been recently addressed as crucial challenge by several approaches. A change rarely confines itself to a single change, but triggers other changes in different partner processes. Specifically, it has been stated that with an increasing number of partner processes, the risk for transitive propagations and costly negotiations increases as well. In this context, utilizing past change events to learn and analyze the propagation behavior over process choreographies will help avoiding significant costs related to unsuccessful propagations and negotiation failures, of further change requests. This paper aims at the posteriori analysis of change requests in process choreographies by the provision of mining algorithms based on change logs. In particular, a novel implementation of the memetic mining algorithm for change logs, with the appropriate heuristics is presented. The results of the memetic mining algorithm are compared with the results of the actual propagation of the analyzed change events.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-45391-9_4",
        "year": "2014"
    },
    {
        "title": "Merging Computer Log Files for Process Mining: An Artificial Immune System Technique",
        "abstract": "Process mining techniques try to discover and analyse business processes from recorded process data. These data have to be structured in so called computer log files. If processes are supported by different computer systems, merging the recorded data into one log file can be challenging. In this paper we present a computational algorithm, based on the Artificial Immune System algorithm, that we developed to automatically merge separate log files into one log file. We also describe our implementation of this technique, a proof of concept application and a real life test case with promising results.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-28108-2_9",
        "year": "2012"
    },
    {
        "title": "Merging Event Logs with Many to Many Relationships",
        "abstract": "Process mining techniques enable the discovery and analysis of business processes, identifying opportunities for improvement. However, processes are often comprised of separately managed procedures that have separate log files, impossible to mine in an integrative manner. A preprocessing step that merges log files is quite straightforward when the logs have common case IDs. However, when cases in the different logs have many-to-many relationships among them this is more challenging. In this paper we present an approach for merging event logs which is capable of dealing with all kinds of relationships between logs, one-to-one or many-to-many. The approach matches cases in the logs, using temporal relations and text mining techniques. We have implemented the algorithm and tested it on a comprehensive set of synthetic logs.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-15895-2_28",
        "year": "2015"
    },
    {
        "title": "Method for the Extraction of Loess Shoulder-Line from Grid Dems Based on Log Edge Detector",
        "abstract": "This paper proposes a new method for extracting loess shoulder-lines from grid DEMs. The morphological characteristics of loess shoulder-lines are investigated firstly. Loess shoulder-lines is the border line of loess inner gully area and inter gully area and its upper side is the relatively flat area(with slope less than 20 degrees) called Inner gully, the lower side is the inter gully with slope between 30 degrees and 35 degrees. There is remarkable morphological variation between the upper side and the lower side. The variation is a clue to extract candidate points of loess shoulder-lines. By applying the edge detection operator LOG to noise removed DEM, the method for extracting loess shoulder-lines candidate points is proposed based on the prominent height variation of the points along the shoulder-lines. This method can detect shoulder-lines described above. The algorithm then connects the candidate points to small line segments by morphological method which labels points in an eight neighborhood window as a line, and finally, precise and systematic loess shoulder lines are extracted after refining the line segments by a simple filter of line length. Experiments in the loess hilly areas show that the loess shoulder-lines extracted by LOG operator has very good matching ratio to standard shoulder-lines extracted manually and good performances on line integration. This ratio is achieved by the so called contour matching method for line matching. Our experiments show that the LOG edge detector has the potential of detecting the explicit shoulder-lines by the zero cross of height second derivatives and the morphological filter can restrain the affection of random noise. The matching scores of extracted shoulder-lines with the standard shoulder-lines from real terrain show the LOG edge detector is a good shoulder line extraction method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5629708/",
        "year": "2010"
    },
    {
        "title": "Mining access patterns efficiently from web logs",
        "abstract": "With the explosive growth of data available on the World Wide Web, discovery and analysis of useful information from the World Wide Web becomes a practical necessity. Web access pattern, which is the sequence of accesses pursued by users frequently, is a kind of interesting and useful knowledge in practice.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/3-540-45571-X_47",
        "year": "2000"
    },
    {
        "title": "Mining and extraction of personal software process measures through IDE interaction logs",
        "abstract": "The Personal Software Process (PSP) is an effective software process improvement method that heavily relies on manual collection of software development data. This paper describes a semi-automated method that reduces the burden of PSP data collection by extracting the required time and size of PSP measurements from IDE interaction logs. The tool mines enriched event data streams so can be easily generalized to other developing environment also. In addition, the proposed method is adaptable to phase definition changes and creates activity visualizations and summarizations that are helpful for software project management. Tools and processed data used for this paper are available on GitHub at: https://github.com/unknowngithubuser1/data.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3196462",
        "year": "2018"
    },
    {
        "title": "Mining Attribute-Based Access Control Policies from Logs",
        "abstract": "Attribute-based access control (ABAC) provides a high level of flexibility that promotes security and information sharing. ABAC policy mining algorithms have potential to significantly reduce the cost of migration to ABAC, by partially automating the development of an ABAC policy from information about the existing access-control policy and attribute data. This paper presents an algorithm for mining ABAC policies from operation logs and attribute data. To the best of our knowledge, it is the first algorithm for this problem.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-43936-4_18",
        "year": "2014"
    },
    {
        "title": "Mining Business Process Stages from Event Logs",
        "abstract": "Process mining is a family of techniques to analyze business processes based on event logs recorded by their supporting information systems. Two recurrent bottlenecks of existing process mining techniques when confronted with real-life event logs are scalability and interpretability of the outputs. A common approach to tackle these limitations is to decompose the process under analysis into a set of stages, such that each stage can be mined separately. However, existing techniques for automated discovery of stages from event logs produce decompositions that are very different from those that domain experts would produce manually. This paper proposes a technique that, given an event log, discovers a¬†stage decomposition that maximizes a measure of modularity borrowed from the field of social network analysis. An empirical evaluation on real-life event logs shows that the produced decompositions more closely approximate manual decompositions than existing techniques.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-59536-8_36",
        "year": "2017"
    },
    {
        "title": "Mining causes of network events in log data with causal inference",
        "abstract": "Network log message (e.g., syslog) is valuable information to detect unexpected or anomalous behavior in a large scale network. However, pinpointing failures and their causes is not an easy problem because of a huge amount of system log data in daily operation. In this study, we propose a method extracting failures and their causes from network syslog data. The main idea of the method relies on causal inference that reconstructs causality of network events from a set of the time series of events. Causal inference allows us to reduce the number of correlated events by chance, thus it outputs more plausible causal events than a traditional cross-correlation based approach. We apply our method to 15 months network syslog data obtained in a nation-wide academic network in Japan. Our method significantly reduces the number of pseudo correlated events compared with the traditional method. Also, through two case studies and comparison with trouble ticket data, we demonstrate the effectiveness of our method for network operation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7987263/",
        "year": "2017"
    },
    {
        "title": "Mining Conditional Partial Order Graphs from Event Logs",
        "abstract": "Process mining techniques rely on event logs: the extraction of a process model (discovery) takes an event log as the input, the adequacy of a process model (conformance) is checked against an event log, and the enhancement of a process model is performed by using available data in the log. Several notations and formalisms for event log representation have been proposed in the recent years to enable efficient algorithms for the aforementioned process mining problems. In this paper we show how Conditional Partial Order Graphs¬†(CPOGs), a recently introduced formalism for compact representation of families of partial orders, can be used in the process mining field, in particular for addressing the problem of compact and easy-to-comprehend representation of event logs with data. We present algorithms for extracting both the control flow as well as the relevant data parameters from a given event log and show how CPOGs can be used for efficient and effective visualisation of the obtained results. We demonstrate that the resulting representation can be used to reveal the hidden interplay between the control and data flows of a process, thereby opening way for new process mining techniques capable of exploiting this interplay. Finally, we present open-source software support and discuss current limitations of the proposed approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-53401-4_6",
        "year": "2016"
    },
    {
        "title": "Mining Event Logs to Assist the Development of Executable Process Variants",
        "abstract": "Developing process variants has been proven as a principle task to flexibly adapt a business process model to different markets. Contemporary research on variant development has focused on conceptual process models. However, process models do not always exist, even when process logs are available in information systems. Moreover, process logs are often more detailed than process models and reflect more closely to the behavior of the process. In this paper, we propose an activity recommendation approach that takes into account process logs for assisting the development of executable process variants. To this end, we define a notion of neighborhood context for each activity based on logs, which captures order constraints between activities with their occurrence frequency. The similarity of the neighborhood context between activities provides us then with a basis to recommend activities during the process of creating a new process model. The approach has been implemented as a plug-in for ProM. Furthermore, we conducted experiments on a large collection of process logs. The results indicate that our approach is feasible and applicable in real use cases.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07881-6_37",
        "year": "2014"
    },
    {
        "title": "Mining Frequent Attack Sequence in Web Logs",
        "abstract": "As a crucial part of web servers, web logs record information about client requests. Logs contain not only the traversal sequences of malicious users but the operations of normal users. Taking advantage of web logs is important for learning the operation of websites. Furthermore, web logs are helpful when conducting postmortem security analysis. However, common methods of analyzing web logs typically focus on discovering preferred browsing paths or improving the structure of website, and thus can not be used directly in security analysis. In this paper, we propose an approach to mining frequent attack sequence based on PrefixSpan. We perform experiments on real data, and the evaluations show that our method is effective in identifying both the behavior of scanners and attack sequences in web logs.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39077-2_16",
        "year": "2016"
    },
    {
        "title": "Mining Frequent Logical Sequences with SPIRIT-LoG",
        "abstract": "Sequence mining is an active research field of data mining because algorithms designed in that domain lead to various valuable applications. To increase efficiency of basic sequence mining algorithms, generally based on a levelwise approach, more recent algorithms try to introduce some constraints to prune the search space during the discovery process. Nevertheless, existing algorithms are actually limited to extract frequent sequences made up of items of a database. In this paper, we generalize the notion of sequence to define what we call logical sequence where each element of a sequence may contain some logical variables. Then we show how we can extend constrained sequence mining to constrained frequent logical sequence mining1.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-36468-4_11",
        "year": "2003"
    },
    {
        "title": "Mining Frequent Patterns in Print Logs with Semantically Alternative Labels",
        "abstract": "It is common today for users to print the informative information from webpages due to the popularity of printers and internet. Thus, many web printing tools such as Smart Print and PrintUI are developed for online printing. In order to improve the users‚Äô printing experience, the interaction data between users and these tools are collected to form a so-called print log data, where each record is the set of urls selected for printing by a user within a certain period of time. Apparently, mining frequent patterns from these print log data can capture user intentions for other applications, such as printing recommendation and behavior targeting. However, mining frequent patterns by directly using url as item representation in print log data faces two challenges: data sparsity and pattern interpretability. To tackle these challenges, we attempt to leverage delicious api (a social bookmarking web service) as an external thesaurus to expand the semantics of each url by selecting tags associated with the domain of each url. In this setting, the frequent pattern mining is employed on the tag representation of each url rather than the url or domain representation. With the enhancement of semantically alternative tag representation, the semantics of url is substantially improved, thus yielding the useful frequent patterns. To this end, in this paper we propose a novel pattern mining problem, namely mining frequent patterns with semantically alternative labels, and propose an efficient algorithm named PaSAL (Frequent Patterns with Semantically Alternative Labels Mining Algorithm) for this problem. Specifically, we propose a new constraint named conflict matrix to purify the redundant patterns to achieve a high efficiency. Finally, we evaluate the proposed algorithm on a real print log data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-53917-6_10",
        "year": "2013"
    },
    {
        "title": "Mining High Utility Web Access Sequences in Dynamic Web Log Data",
        "abstract": "Mining web access sequences can discover very useful knowledge from web logs with broad applications. By considering non-binary occurrences of web pages as internal utilities in web access sequences, e.g., time spent by each user in a web page, more realistic information can be extracted. However, the existing utility-based approach has many limitations such as considering only forward references of web access sequences, not applicable for incremental mining, suffers in the level-wise candidate generation-and-test methodology, needs several database scans and does not show how to mine web traversal sequences with external utility, i.e., different impacts/significances for different web pages. In this paper, we propose a new approach to solve these problems. Moreover, we propose two novel tree structures, called UWAS-tree (utility-based web access sequence tree), and IUWAS-tree (incremental UWAS tree), for mining web access sequences in static and dynamic databases respectively. Our approach can handle both forward and backward references, static and dynamic data, avoids the level-wise candidate generation-and-test methodology, does not scan databases several times and considers both internal and external utilities of a web page. Extensive performance analyses show that our approach is very efficient for both static and incremental mining of high utility web access sequences.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5521504/",
        "year": "2010"
    },
    {
        "title": "Mining Massive Web Log Data of an Official Tourism Web Site as a Step towards Big Data Analysis in Tourism",
        "abstract": "The focus of this paper is on the conceptual and technical solution design to the analysis of massive web log data of an official tourism web site when integrated with web mining and big data technology. With the rapid development of Internet and World Wide Web, web log becomes one the fastest growing user generated contents, and web log mining plays an important role in many fields, such as personalized information service, design and service improvement of web sites. The underlying technology for analyzing massive web log data includes web log mining and big data analysis. In this paper, we give a comprehensive overview at the underlying technology, and then we propose an open architecture of big data solution design in tourism with mining the massive web log data. We include the discussion on the difficulties in implementing the proposed architecture as a conclusion.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2818906",
        "year": "2015"
    },
    {
        "title": "Mining of an Alarm Log to Improve the Discovery of Frequent Patterns",
        "abstract": "In this paper we propose a method to pre-process a telecommunication alarm log with the aim of discovering more accurately frequent patterns. In a first step, the alarm types which present the same temporal behavior are clustered with a self organizing map. Then, the log areas which are rich in alarms of the clusters are searched. The sublogs are built based on the selected areas. We will show the efficiency of our preprocessing method through experiments on an actual alarm log from an ATM network.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-30185-1_16",
        "year": "2005"
    },
    {
        "title": "Mining of Flexible Manufacturing System Using Work Event Logs and Petri Nets",
        "abstract": "One of buzzwords for modern manufacturing industry are flexible manufacturing systems (FMS), in which several machines are interlinked by an automated information and material flow system. Description and control upon these systems are of prominent significance. This paper is concerned with mining and construction of the established FMS from work event logs. A novel Petri nets based algorithm is developed to implement such an idea. When an FMS is mined and constructed, its corresponding Petri net is used to evaluate, analyze, and control the system. Theoretical and experimental results are illustrated to show the effectiveness and efficiency of this approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11811305_42",
        "year": "2006"
    },
    {
        "title": "Mining Predictive Process Models out of Low-level Multidimensional Logs",
        "abstract": "Process Mining techniques have been gaining attention, especially as concerns the discovery of predictive process models. Traditionally focused on workflows, they usually assume that process tasks are clearly specified, and referred to in the logs. This limits however their application to many real-life BPM environments (e.g. issue tracking systems) where the traced events do not match any predefined task, but yet keep lots of context data. In order to make the usage of predictive process mining to such logs more effective and easier, we devise a new approach, combining the discovery of different execution scenarios with the automatic abstraction of log events. The approach has been integrated in a prototype system, supporting the discovery, evaluation and reuse of predictive process models. Tests on real-life data show that the approach achieves compelling prediction accuracy w.r.t. state-of-the-art methods, and finds interesting activities‚Äô and process variants‚Äô descriptions.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07881-6_36",
        "year": "2014"
    },
    {
        "title": "Mining Preferences from OLAP Query Logs for Proactive Personalization",
        "abstract": "The goal of personalization is to deliver information that is relevant to an individual or a group of individuals in the most appropriate format and layout. In the OLAP context personalization is quite beneficial, because queries can be very complex and they may return huge amounts of data. Aimed at making the user‚Äôs experience with OLAP as plain as possible, in this paper we propose a proactive approach that couples an MDX-based language for expressing OLAP preferences to a mining technique for automatically deriving preferences. First, the log of past MDX queries issued by that user is mined to extract a set of association rules that relate sets of frequent query fragments; then, given a specific query, a subset of pertinent and effective rules is selected; finally, the selected rules are translated into a preference that is used to annotate the user‚Äôs query. A set of experimental results proves the effectiveness and efficiency of our approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-23737-9_7",
        "year": "2011"
    },
    {
        "title": "Mining Preferences on Identifying Werewolf Players from Werewolf Game Logs",
        "abstract": "The deception party game ‚ÄúAre You a Werewolf?‚Äù requires players to guess other‚Äôs roles through discussions that are based on one‚Äôs own role and other players‚Äô crucial utterances. This paper proposes a method to mine the empirical preference data used to identify werewolves from game logs. This involves obtaining an empirical preference related to the practice of divination. In this method, if one of the three players revealing oneself as a seer divines a player as a human, while the other two players divine the player as a werewolf, then it can be judged that the divined player‚Äôs role is a werewolf.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-66715-7_38",
        "year": "2017"
    },
    {
        "title": "Mining process models from workflow logs",
        "abstract": "Modern enterprises increasingly use the workflow paradigm to prescribe how business processes should be performed. Processes are typically modeled as annotated activity graphs. We present an approach for a system that constructs process models from logs of past, unstructured executions of the given process. The graph so produced conforms to the dependencies and past executions present in the log. By providing models that capture the previous executions of the process, this technique allows easier introduction of a workflow system and evaluation and evolution of existing process models. We also present results from applying the algorithm to synthetic data sets as well as process logs obtained from an IBM Flowmark installation.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/BFb0101003",
        "year": "1998"
    },
    {
        "title": "Mining Process Performance from Event Logs",
        "abstract": "In systems where process executions are not strictly enforced by a predefined process model, obtaining reliable performance information is not trivial. In this paper, we analyzed an event log of a real-life process, taken from a Dutch financial institute, using process mining techniques. In particular, we exploited the alignment technique [2] to gain insights into the control flow and performance of the process execution. We showed that alignments between event logs and discovered process models from process discovery algorithms reveal insights into frequently occurring deviations and how such insights can be exploited to repair the original process models to better reflect reality. Furthermore, we showed that the alignments can be further exploited to obtain performance information. All analysis in this paper is performed using plug-ins within the open-source process mining toolkit ProM.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-36285-9_23",
        "year": "2013"
    },
    {
        "title": "Mining Query Log to Assist Ontology Learning from Relational Database",
        "abstract": "Ontology learning plays a significant role in migrating legacy knowledge base into the Semantic Web. Relational database is the vital source that stores the structured knowledge today. Some prior work has contributed to the learning process from relational database to ontology. However, a majority of the existing methods focus on the schema dimension, leaving the data dimension not well exploited. In this paper we present a novel approach that exploits the data dimension by mining user query log to glorify the ontology learning process. In addition, we propose a set of rules for schema extraction which serves as the basis of our theme. The presented approach can be applied to a broad range of today‚Äôs relational data warehouse.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11610113_39",
        "year": "2006"
    },
    {
        "title": "Mining Query Logs",
        "abstract": "Web Search Engines (WSEs) have stored in their query logs information about users since they started to operate. This information often serves many purposes. The primary focus of this tutorial is to introduce to the discipline of query log mining. We will show its foundations, by giving a unified view on the literature on query log analysis, and also present in detail the basic algorithms and techniques that could be used to extract useful knowledge from this (potentially) infinite source of information. Finally, we will discuss how the extracted knowledge can be exploited to improve different quality features of a WSE system, mainly its effectiveness and efficiency.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-00958-7_94",
        "year": "2009"
    },
    {
        "title": "Mining Query Logs of USPTO Patent Examiners",
        "abstract": "In this paper we analyze a highly professional search setting of patent examiners of the United Patent and Trademark Office (USPTO). We gain insight into the search behavior of USPTO patent examiners to explore ways for enhancing query generation in patent searching. We show that query generation is highly patent domain specific and patent examiners follow a strict scheme for generating text queries. Means to enhance query generation in patent search are to suggest synonyms and equivalents, co-occurring terms and keyword phrases to the searchable features of the invention. Further, we show that term networks including synonyms and equivalents can be learned from the query logs for automatic query expansion in patent searching.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-40802-1_17",
        "year": "2013"
    },
    {
        "title": "Mining search query logs for spoken language understanding",
        "abstract": "In a spoken dialog system that can handle natural conversation between a human and a machine, spoken language understanding (SLU) is a crucial component aiming at capturing the key semantic components of utterances. Building a robust SLU system is a challenging task due to variability in the usage of language, need for labeled data, and requirements to expand to new domains (movies, travel, finance, etc.). In this paper, we survey recent research on bootstrapping or improving SLU systems by using information mined or extracted from web search query logs, which include (natural language) queries entered by users as well as the links (web sites) they click on. We focus on learning methods that help unveiling hidden information in search query logs via implicit crowd-sourcing.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2390466",
        "year": "2012"
    },
    {
        "title": "Mining term association patterns from search logs for effective query reformulation",
        "abstract": "Search engine logs are an emerging new type of data that offers interesting opportunities for data mining. Existing work on mining such data has mostly attempted to discover knowledge at the level of queries (e.g., query clusters). In this paper, we propose to mine search engine logs for patterns at the level of terms through analyzing the relations of terms inside a query. We define two novel term association patterns (i.e., context-sensitive term substitutions and term additions) and propose new methods for mining such patterns from search engine logs. These two patterns can be used to address the mis-specification and under-specification problems of ineffective queries. Experiment results on real search engine logs show that the mined context-sensitive term substitutions can be used to effectively reword queries and improve their accuracy, while the mined context-sensitive term addition patterns can be used to support query refinement in a more effective way.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1458147",
        "year": "2008"
    },
    {
        "title": "Mining Ticketing Logs for Usage Characterization with Nonnegative Matrix Factorization",
        "abstract": "Understanding urban mobility is a fundamental question for institutional organizations (transport authorities, city halls) and it involves many different fields like social sciences, urbanism or geography. With the increasing number of probes tracking human locations, like RFID pass for urban transportation, road sensors, CCTV systems or cell phones, mobility data are exponentially growing. Mining the activity logs in order to model and characterize efficiently our mobility patterns is a challenging task involving large scale noisy datasets.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-29009-6_8",
        "year": "2016"
    },
    {
        "title": "Mining uncertain web log sequences with access history probabilities",
        "abstract": "This paper proposes (1) modeling uncertainty in web log sequences using the most recent periodic web log which attaches computed existential probabilities between 0 and 1, to events in the sequences, (2) using the newly proposed uncertain PLWAP web sequential miner for these uncertain access sequences. While PLWAP only considers a session of web logs, U-PLWAP takes more sessions of web logs from which existential probabilities are generated and there is the need to traverse each suffix tree from the root in order to scan for existential probabilities of items already found along the path. Experiments show that U-PLWAP is faster than U-Apriori, and UF-growth.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1982417",
        "year": "2011"
    },
    {
        "title": "Mining user access logs to optimize navigational structure of adaptive web sites",
        "abstract": "Web sites may contain numerous documents. Using some web techniques, it's possible to analyze users' data about using resources, contents of those documents and structure of web sites. Adaptive web sites automatically change their structure and representation based on visitor's behavior. Shortcutting is an approach that enables connecting two documents which has never been connected before. Most of existing approaches enables connecting the first and the last document in user's navigation path, not considering the possibility that some of the documents within the navigation path might contain useful information for reaching intended document. These documents, which are positioned within user's navigation path, are called wayposts, and they may contain useful information that can help users to get to the specific ‚Äútarget‚Äù document. The goal of this paperwork is to discuss about all the possibilities of identifying those waypost documents in users' navigation paths and to propose an optimization of navigation structure of a web site based on users' navigation paths, initial and target documents.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5672233/",
        "year": "2010"
    },
    {
        "title": "Mining user access patterns based on Web logs",
        "abstract": "In this paper, different from usual order, not directly use the maximal forward reference path to mine sequence patterns but use DBSCAN algorithm to cluster the Web pages that have been accessed by users. Then, decide the Web page class that each page belongs to based on heuristic rules. Next, cluster the users who have the same interest in one or some kinds of Web pages. One user can belong to several classes, because the user may be interested in different types of Web pages. Finally, based on theory of sequence patterns mining, mine out user access patterns in each class by GSP algorithm. The benefit of using cluster methods is to find out layers' or classes' relationships from data even without any layer information of data. In this way, the user access patterns can be found more precisely",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1557444/",
        "year": "2005"
    },
    {
        "title": "Mining User Position Log for Construction of Personalized Activity Map",
        "abstract": "Consider a scenario in which a smart phone automatically saves the user‚Äôs positional records for personalized location-based applications. The smart phone will infer patterns of user activities from the historical records and predict user‚Äôs future movements. In this paper, we present algorithms for mining the evolving positional logs in order to identify places of significance to user and representative paths connecting these places, based on which a personalized activity map is constructed. In addition, the map is designed to contain information of speed and transition probabilities, which are used in predicting the user‚Äôs future movements. Our experiments show that the user activity map well matches the actual traces and works effectively in predicting user‚Äôs movements.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-03348-3_43",
        "year": "2009"
    },
    {
        "title": "Mining User‚Äôs Location Intention from Mobile Search Log",
        "abstract": "Much attention has been paid to web search personalization and query optimization over the past decade. With the prevalence of smart phones, the mobile search results for the same query may vary in regard to the user‚Äôs location. In order to provide more precise results for users, it‚Äôs essential to take geographic location into account along with the user‚Äôs input query. In this paper, we try to identify queries that have location intentions. For example, query ‚Äúweather forecast‚Äù has a location intention of local city while ‚ÄúThe Statue of Liberty‚Äù has a location intention of ‚ÄúNew York city‚Äù. To identify the location intention behind a query, we propose a novel method to extract a set of features and use neural network to classify queries. In the classification of queries without explicit location names, our experiment shows that our approach achieves 82.5% at F1 measure and outperforms baselines by 4.2%.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-25159-2_37",
        "year": "2015"
    },
    {
        "title": "Mining Web log data based on key path",
        "abstract": "A Web log mining method is presented. First, minimal key path set (MKPS) is defined and an algorithm to find the MKPS online is given. At the same time, for any key path in the MPKS, this algorithm can find out all transactions relevant to it. After scanning the transaction database only once, a relevant matrix is set up, where the key paths in MKPS are taken as columns and the transactions are taken as rows. Compared to previous methods, our method considers the three major features of users' accessing the Web: ordinal, contiguous, and duplicate. Moreover, for clustering transactions, we have lesser dimensions than the previous method. Using the clustering algorithm based on the relevant matrix, better clustering results will be obtained more precisely and quickly. Experiments show the effectiveness of the method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1176728/",
        "year": "2002"
    },
    {
        "title": "Mining Web Log Sequential Patterns with Layer Coded Breadth-First Linked WAP-Tree",
        "abstract": "Sequential mining is the process of applying data mining techniques to a sequential database for the purposes of discovering the correlation relationships that exist among an ordered list of events. An important application of sequential mining techniques is web usage mining, for mining web log accesses, which the sequences of web page accesses made by different web users over a period of time, through a server, are recorded. Web access pattern tree (WAP-tree) mining is a sequential pattern mining technique for web log access sequences. This paper proposes a more efficient approach for using the BFWAP-tree to mine frequent sequences, which reflects ancestor-descendant relationship of nodes in BFWAP tree directly and efficiently. The proposed algorithm builds the frequent header node links of the original WAP-tree in a Breadth-First fashion and uses the layer code of each node to identify the ancestor-descendant relationships between nodes of the tree. It then, finds each frequent sequential pattern, through progressive Breadth-First sequence search, starting with its first Breadth-First subsequence event. Experiments show huge performance gain over the WAP-tree technique.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5574025/",
        "year": "2010"
    },
    {
        "title": "Mining Web logs for a personalized recommender system",
        "abstract": "As the Web rapidly grows, however, the number of matching pages increases at a tremendous rate when users use the search engine for finding some information. It is not easy for a user to retrieve the exact information he/she requires. In particular, browsing a Web set is an expensive operation, both in time and cognitive effort. Recommender systems have then become valuable resources for users seeking intelligent ways to search through the enormous volume of information available to them. In this paper we propose a new framework based on Web logs mining for building a personalized recommender system. At the core of personalization is the task of building a profile of the user. We have developed an approach that user's information learned from user's Web logs data to construct accurate comprehensive individual profiles. One part of this profile contains facts about a user, and the other part contains rules describing that user's behavior. We use Web usage mining to derive the behavioral rules from the data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1503162/",
        "year": "2005"
    },
    {
        "title": "Mining Web Logs for Actionable Knowledge",
        "abstract": "Everyday, popular Websites attract millions of visitors. These visitors leave behind vast amounts of Websites traversal information in the form of Web server and query logs. By analyzing these logs, it is possible to discover various kinds of knowledge, which can be applied to improve the performance of Web services. A particularly useful kind of knowledge is knowledge that can be immediately applied to the operation of the Web-sites; we call this type of knowledge actionable knowledge. In this chapter, we present three examples of actionable Web log mining. The first method is to mine a Web log for Markov models that can be used for improving caching and prefetching of Web objects. A second method is to use the mined knowledge for building better, adaptive user interfaces. The new user interface can adjust as the user behavior changes with time. Finally, we present an example of applying Web query log knowledge to improving Web search for a search engine application.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-07952-2_8",
        "year": "2004"
    },
    {
        "title": "Mining web logs to debug distant connectivity problems",
        "abstract": "Content providers base their business on their ability to receive and answer requests from clients distributed across the Internet. Since disruptions in the flow of these requests directly translate into lost revenue, there is tremendous incentive to diagnose why some requests fail and prod the responsible parties into corrective action. However, a content provider has only limited visibility into the state of the Internet outside its domain. Instead, it must mine failure diagnoses from available information sources to infer what is going wrong and who is responsible.Our ultimate goal is to help Internet content providers resolve reliability problems in the wide-area network that are affecting end-user perceived reliability. We describe two algorithms that represent our first steps towards enabling content providers to extract actionable debugging information from content provider logs, and we present the results of applying the algorithms to a week's worth of logs from a large content provider, during which time it handled over 1 billion requests originating from over 10 thousand ASes.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1162680",
        "year": "2006"
    },
    {
        "title": "Mining web logs to improve hit ratios of prefetching and caching",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S095070510600195X",
        "year": "2008"
    },
    {
        "title": "Mining web logs: applications and challenges",
        "abstract": "Web logs record the primary interaction of users with web pages in general and search engines in particular. There are two sources for such logs: user trails obtained from toolbars and query/click information obtained from search engines. In this talk we will address the task of mining this rich data to improve user experience on the web. We will illustrate a few applications, together with the modeling and algorithmic challenges that stem from these applications. We will also discuss the privacy issues that arise in this context.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1557022",
        "year": "2009"
    },
    {
        "title": "Mining Web Server Logs for Creating Workload Models",
        "abstract": "We present a tool-supported approach where we used data mining techniques for automatically inferring workload models from historical web access log data. The workload models are represented as Probabilistic Timed Automata (PTA) and describe how users interact with the system. Via their stochastic nature, PTAs have more advantages over traditional approaches which simply playback scripted or pre-recorded traces: they are easier to create and maintain and achieve higher coverage of the tested application. The purpose of these models is to mimic real-user behavior as closely as possible when generating load. To show the validity and applicability of our proposed approach, we present a few experiments. The results show, that the workload models automatically derived from web server logs are able to generate similar load with the one applied by real-users on the system and that they can be used as the starting point for performance testing process.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-25579-8_8",
        "year": "2015"
    },
    {
        "title": "Mining Website Log to Improve Its Findability",
        "abstract": "Under the network environments with large amounts of digitalized data, websites are the information strongholds that institutions, organizations or enterprises must set up for their specific purposes. No matter how they have been built, websites should offer the capability that users can find their required information quickly and intuitively. Surfing around the library websites, the website logs always keep tracks of users‚Äô factual behaviors of finding their required information. Thus we can apply data mining techniques possibly to explore users‚Äô information seeking behavior. Based on these evidences, we attempt to reconstruct the websites to promote their internal findability. In this paper, we proposed a heuristic algorithm to clean the website log data, to extract user sub-sessions according to their respective the critical time of session navigation, and to calculate each sub-session‚Äôs the threshold time of target page with different weights to determine its navigating parent page. We utilized the alternate parent pages of weights to reconstruct various websites. We conduct task-oriented experiments of 4 tasks and 25 participants to measure the effects of their findability respectively. By the analysis of variance on time to complete the tasks, the result has shown that the reconstructed website has better findability performance.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-14306-9_24",
        "year": "2010"
    },
    {
        "title": "Missing Data Problem in the Event Logs of Transport Processes",
        "abstract": "Data and process mining techniques are very helpful in analyzing transport problems. The model of the process can be built using the available data. It leads to make possible the operational support which improves the process. Very important task is to record data in the proper way. Unfortunately some errors may occur. In this kind of situation some data lacks can be observed. On the other hand the data may be complete but having very high error coefficient. The model of the process should have as minimum error as possible and has to be reliable. Although some missing data can occur, there are some ways to do some data recovery. In this paper the problem of missing numerical data in the event log is described. Different solutions and conclusions are presented.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-66251-0_9",
        "year": "2017"
    },
    {
        "title": "MLLE - A Microprocessor Based Large Logging Experimentation System",
        "abstract": "MLLE is a system for on-line data acquisition as well as off-line data analysis. Several microprocessors are connected to a large data memory via an asynchronous bus. Each processor performs only one task and handles the data in parallel with the other microprocessors. The communication between the processors is done via a control bus. Dedicated processors act as interfaces to CAMAC and to a host computer, respectively. The host computer performs tasks like tape handling, graphic display, and user communication, whereas all time critical jobs are done by the MLLE processors.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4333016/",
        "year": "1983"
    },
    {
        "title": "Model and Event Log Reductions to Boost the Computation of Alignments",
        "abstract": "The alignment of observed and modeled behavior is a pivotal issue in process mining because it opens the door for assessing the quality of a process model, as well as the usage of the model as a precise predictor for the execution of a process. This paper presents a novel technique for reduction of a process model based on the notion of indication, by which, the occurrence of an event in the model reveals the occurrence of some other events, hence relegating the later set as less important information when model and log alignment is computed. Once indications relations are computed in the model, both model and log can be reduced accordingly, and then fed to the state of the art approaches for computing alignments. Finally, the (macro)-alignment derived is expanded in these parts containing high-level events that represent a set of indicated events, by using an efficient algorithm taken from bioinformatics that guarantees optimality in the local parts of the alignment. The implementation of the presented techniques shows a significant reduction both in computation time and in memory usage, the latter being a significant barrier to apply the alignment technology on large instances.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-74161-1_1",
        "year": "2018"
    },
    {
        "title": "Model-Driven Development of Multidimensional Models from Web Log Files",
        "abstract": "Analyzing Web log data is important in order to study the usage of a website. Even though some approaches propose data warehousing techniques for structuring the Web log data into a multidimensional model, they present two main drawbacks: (i) they are based on informal guidelines and must be manually applied; and (ii) they consider data tailored to a specific Web log format, thus being restricted to specific analysis tools. To overcome these limitations, we present a model-driven approach for obtaining a conceptual multidimensional model from Web log data in a comprehensive, integrated and automatic manner. This approach consists of the following steps: (i) obtaining a conceptual model of the Web log data based on a unified metamodel, (ii) deriving a multidimensional model from this Web log model by formally defining a set of QVT (Query/View/Transformation) transformation rules.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-16385-2_22",
        "year": "2010"
    },
    {
        "title": "Modeling Personal Preferences on Commodities by Behavior Log Analysis with Ubiquitous Sensing",
        "abstract": "Consumers may take some specific behavior preference or favorite items to get more information, such as the material and the price, in shopping. We have been developing a smart room to estimate their preference and favorite items through observation using ubiquitous sensors, such as RFID and Web cameras. We assumed the decision decision-making process in shopping as AIDMA rule, and detected specific behavior, which are ‚ÄúSee‚Äù, ‚ÄúTouch‚Äù and ‚ÄúTake‚Äù, to estimate user‚Äôs interest. We found that we can classify consumers by their behavior patterns of the times and duration of the behaviors. In our experiment we have tested twenty-eight subjects on twenty-four T-shirts. In the experiment, we got better precision ratio for each subjects on estimating preference and favorite items by discriminate analysis on his or her behavior log, and behavior patterns classification above.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-02580-8_32",
        "year": "2009"
    },
    {
        "title": "Monitoring Compliance Policies over Incomplete and Disagreeing Logs",
        "abstract": "When monitoring system behavior to check compliance against a given policy, one is sometimes confronted with incomplete knowledge about system events. In IT systems, such incompleteness may arise from logging infrastructure failures and corrupted log files, or when the logs produced by different system components disagree on whether actions took place. In this paper, we present a policy language with a three-valued semantics that allows one to explicitly reason about incomplete knowledge and handle disagreements. Furthermore, we present a monitoring algorithm for an expressive fragment of our policy language. We illustrate through examples how our approach extends compliance monitoring to systems with logging failures and disagreements.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-35632-2_17",
        "year": "2013"
    },
    {
        "title": "Monitoring email transaction logs by text-mining email contents",
        "abstract": "Monitoring every single email takes a lot of effort especially when the size of email transaction log is very large. This study proposed to find a wise option to monitor only the contents of important emails. Depth First Search algorithm, multi-digraph, email scoring model, WordNet, and Vector Space Model are used to create a model for filtering important emails and mining email contents. The findings showed that using email filtering module together with term enhancing module can help in reducing the processing time and keeping high precision and recall values of the system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6108439/",
        "year": "2011"
    },
    {
        "title": "Monitoring logging in the tropical forest of Republic of Congo with Landsat imagery",
        "abstract": "Selective logging is the most extensive land use in the Congo Basin, with more than 40% of the forest allocated to timber concessions. Little information is available to monitor the spatial expansion of timber extraction and its impacts on tropical forests. As part of the INFORMS project, \"An Integrated Forest Monitoring System for Central Africa,\" we have developed a simple system to monitor the extension of logging roads using multi-temporal Landsat imagery. We also tested the utility of Landsat imagery in estimating logging intensity. This system is being used within the national forest services of the Republic of Congo and Cameroon to monitor logging expansion.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1294510/",
        "year": "2003"
    },
    {
        "title": "Monitoring Parkinson's Disease from phonation improvement by Log Likelihood Ratios",
        "abstract": "Parkinson's Disease (PD), contrary to other neurodegenerative diseases, supports certain treatments which can improve patients' conditions or at least mitigate disease effects. Treatments, either pharmacological, surgical or rehabilitative need longitudinal monitoring of patients to assess the progression or regression of thier condition, to optimize resources and benefits. As it is well known, PD leaves important marks in phonation, thus correlates obtained from spoken recordings taken at periodic intervals may be used in longitudinal monitoring of PD. The most preferred correlates are mel-cepstral coefficients, distortion features (jitter, shimmer, HNR, PPE, etc.), tremor indicators, or biomechanical coefficients. Feature templates estimated from each periodic evaluation have to be compared to establish potential progression or regression. The present work is devoted to propose a comparison framework based on Log Likelihood Ratios. This methodology shows to be very sensitive and allows a three-band based comparison: pre-treatment status vs post-treatment status in reference to a control subject or to a control population. Results from a database of eight male patients recorded weekly during a month are shown with comments regarding their severity condition. The conclusions derived show that several distortion, biomechanical and tremor features are quite relevant in monitoring PD phonation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7160152/",
        "year": "2015"
    },
    {
        "title": "Morbidity and mortality after colorectal procedures: comparison of data from the American College of Surgeons case log system and the ACS NSQIP",
        "abstract": "Improving the quality of surgical care depends upon collection of robust data. The American College of Surgeons Case Log System enables surgeons to self-report patient risk factors and outcomes. In contrast, the American College of Surgeons National Surgical Quality Improvement Program (NSQIP) uses trained data abstractors to record similar data and uses a strict data collection methodology. The objective of this study was to assess bias in data entry for colorectal cases by comparing data in these 2 registries.Study DesignOne year of NSQIP (July 1, 2008 to June 30, 2009) and 7 years of Case Log (2003 to 2010) data were examined. Colorectal cases were identified by current procedural terminology code. The frequencies of comparably defined variables were compared, and mortality models were developed using logistic regression. Observed and expected mortality rates were compared.ResultsRates of most risk factor and outcome variables were significantly higher in NSQIP than those in Case Log. NSQIP had a higher unadjusted mortality rate (4.46% versus 3.69%, p \u003c 0.001); however, the adjusted odds of mortality was significantly higher in Case Log (odds ratio 1.32, p \u003c 0.05). The Case Log model overpredicted mortality in NSQIP by 22%, whereas the NSQIP model underpredicted mortality in Case Log by 12%.ConclusionsSignificant differences exist between risk factor and outcome data in NSQIP and Case Log for colorectal procedures. These differences demonstrate the need for standardized data collection methods, as is required by NSQIP, including use of standard definitions, adherence to a follow-up period for outcomes, and use of audits. These measures would improve the validity of using a self-reported database to evaluate and benchmark performance.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1072751511001827",
        "year": "2011"
    },
    {
        "title": "Multi-level Intrusion Detection System and log management in Cloud Computing",
        "abstract": "Cloud Computing is a new type of service which provides large scale computing resource to each customer. Cloud Computing systems can be easily threatened by various cyber attacks, because most of Cloud Computing systems provide services to so many people who are not proven to be trustworthy. Therefore, a Cloud Computing system needs to contain some Intrusion Detection Systems(IDSs) for protecting each Virtual Machine(VM) against threats. In this case, there exists a tradeoff between the security level of the IDS and the system performance. If the IDS provide stronger security service using more rules or patterns, then it needs much more computing resources in proportion to the strength of security. So the amount of resources allocating for customers decreases. Another problem in Cloud Computing is that, huge amount of logs makes system administrators hard to analyse them. In this paper, we propose a method that enables Cloud Computing system to achieve both effectiveness of using the system resource and strength of the security service without trade-off between them.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/5745874/",
        "year": "2011"
    },
    {
        "title": "Multi-level logs based web performance evaluation and analysis",
        "abstract": "The execution of web applications always involves many levels (Operating system, Java virtual machine, Web server, Database server, application itself). Each level can produce its corresponding logs, which record the information of the level's running state and history. Based on the multi-level logs generated by the running web systems, this paper proposes a comprehensive method to evaluate and analyze the online web applications' performance. The advantages of this method are that we can not only use it in the testing stage, but also we can discover the online web applications' performance problems not found in the performance testing stage, and can analyze the reasons of the problems effectively. Case study shows that it is an effective method for performance evaluation and analysis.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/5620512/",
        "year": "2010"
    },
    {
        "title": "Multi-perspective Comparison of Business Process Variants Based on Event Logs",
        "abstract": "A process variant represents a collection of cases with certain shared characteristics, e.g. cases that exhibit certain levels of performance. The comparison of business process variants based on event logs is a recurrent operation in the field of process mining. Existing approaches focus on comparing variants based on directly-follows relations such as ‚Äúa task directly follows another one‚Äù or a ‚Äúresource directly hands-off to another resource‚Äù. This paper presents a more general approach to log-based process variant comparison based on so-called perspective graphs. A perspective graph is a graph-based abstraction of an event log where a node represents any entity referred to in the log (e.g. task, resource, location) and an arc represents a relation between these entities within or across cases (e.g. directly-follows, co-occurs, hands-off to, works-together with). Statistically significant differences between two perspective graphs are captured in a so-called differential perspective graph, which allows us to compare two logs from any perspective. The paper illustrates the approach and compares it to an existing baseline using real-life event logs.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-00847-5_32",
        "year": "2018"
    },
    {
        "title": "Multi-pose face detection for accurate face logging",
        "abstract": "In this paper we present a technique for real-time face logging in video streams. Our system is capable of detecting faces across a range of poses and of tracking multiple targets in real time, grabbing face images and evaluating their quality in order to store only the best for each detected target. An advantage of our approach is that we qualify every logged face in terms of a quality measure based both on face pose and on resolution. Extensive qualitative and quantitative evaluation of the performance of our system is provided on many hours of realistic surveillance footage captured in different environments. Results show that our system can simultaneously minimizing false positives and identity mismatches, while balancing this against the need to obtain face images of all people in a scene.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6460662/",
        "year": "2012"
    },
    {
        "title": "Multifocus image fusion using the log-Gabor transform and a multisize windows technique",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1566253508000468",
        "year": "2009"
    },
    {
        "title": "Multilingual log analysis: LogCLEF",
        "abstract": "The current lack of recent and long-term query logs makes the verifiability and repeatability of log analysis experiments very limited. A first attempt in this direction has been made within the Cross-Language Evaluation Forum in 2009 in a track named LogCLEF which aims to stimulate research on user behaviour in multilingual environments and promote standard evaluation collections of log data. We report on similarities and differences of the most recent activities for LogCLEF.",
        "include": true,
        "url": "https://link.springer.com/10.1007%2F978-3-642-20161-5_68",
        "year": "2011"
    },
    {
        "title": "MultiMarker propagation ‚Äî Web log mining algorithms based on weighted matrix cluster",
        "abstract": "Web server log files registered browsing patterns of users, in order to extract users with similar accessing patterns, providing personal service for them. In this paper, a new method of analysis on web log is proposed. A User-URL matrix is created and a weighted matrix is introduced, additionally, a multimarker propagation algorithm based on weighted matrix cluster is proposed. The experiments show that this algorithm has high-performance and reliability to cluster users and pages in web log mining.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5497445/",
        "year": "2010"
    },
    {
        "title": "Multiscale Corner Detection of Gray Level Images Based on Log-Gabor Wavelet Transform",
        "abstract": "This paper presents a novel corner detection method for gray level images based on log-Gabor wavelet transform (WT). The input image is decomposed at multiscales and along multi-orientations. The magnitudes of the decomposition are formulated into the second moment matrix. The smaller eigenvalue of the second moment matrix is used as the \"cornerness\" measurement. Compared with the most famous Harris detector, SUSAN detector and the recently published detector - Gabor wavelet transform based detector, the proposed method shows good localization and single response to the higher order corner structures. The simulation results also shows the higher detection rate of the proposed method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4217314/",
        "year": "2007"
    },
    {
        "title": "Na√Øve Bayesian filters for log file analysis: Despam your logs",
        "abstract": "System log files are critical for troubleshooting complex modern computer systems. Systems can easily produce more log file entries than a human can realistically use. However, there are a number of good filtering and clustering technologies that are used in various areas of data mining. This research focuses on using very easily accessible Bayesian spam filters for categorizing log entries. Results of this research have confirmed that these filters can be effectively used to discover log entries related to known issues, and to effectively disprove outage relationships. Both of these techniques can be easily instrumented in a log analysis framework and provide administrators with much needed filtering for similar logs and thus, similar outages.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6211972/",
        "year": "2012"
    },
    {
        "title": "NetflowVis: A Temporal Visualization System for Netflow Logs Analysis",
        "abstract": "Netflow logs record the interactions between host pairs on both sides of the monitored border, and have got more attention from researchers for security concerns. Such data allows analysts to find interesting patterns and security anomalies. Visual analytics provides interaction and visualization techniques that can support these tasks. In this paper, we present a system called NetflowVis to analyze communication patterns and network abnormalities from netflow logs. This system consists of four views, including the communication trajectories view, the traffic line view, the snapshot view and the protocol view. The communication trajectories view is a composite view that dynamically describes the communication trajectories. This view combines a link-node tree and an improved ThemeRiver. The protocol view is designed to display statistical data of the upstream and downstream traffic on different protocols, which is an improved radial view based on an area filling strategy. The system provides a multilevel analysis architecture for netflow cognition. In this paper, we also present a case study to demonstrate the effectiveness and usefulness of our system.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-46771-9_27",
        "year": "2016"
    },
    {
        "title": "Network log analysis based on the topic word mover's distance",
        "abstract": "Telecommunication networks continuously generate various system logs which include plentiful information of system status. So these logs can be used to detect whether a network is under a fault scenario or not. In this paper, we propose an improved word mover's distance (WMD) called Topic Word Mover's Distance (T-WMD) to measure the distance between two log samples and then classify different fault logs to determine the fault root cause. Compared with original WMD, T-WMD takes topic information into consideration and provides more latent semantic information of log corpus. Experiments of k-nearest neighbor (k-nn) fault logs classification show that our T-WMD metric outperforms the original WMD.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8407832/",
        "year": "2018"
    },
    {
        "title": "Neural Network Based Web Log Analysis for Web Intrusion Detection",
        "abstract": "With the increased attacks of web servers and web applications, it is urgent to develop a system to detect web intrusions. Web log files are stream data recording users‚Äô clicks behavior during surfing the Internet. By carefully analyzing these log files, we can reveal some potential anomalies or attacks so as to reduce the loss of property. A method, that applies neural network method to web intrusion detection based on web server access logs, is proposed in this paper. Before feeding the raw log files into neural network algorithms, we need to preprocess these text files and make sure processed logs are of good quality with less noisy and errors. At the result part, our evaluations also demonstrate that the proposed method is superior to decision tree classifier, which shows neural network method can be transplant to web intrusion detection effectively.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-72395-2_19",
        "year": "2017"
    },
    {
        "title": "Neural Networks Training Based on Differential Evolution in Radial Basis Function Networks for Classification of Web Logs",
        "abstract": "With the fastest growth of World Wide Web it is quite difficult to track and understand users‚Äô need for the owners of a website. Hence, an intelligent analyzer is proposed to find out the browsing patterns of a user. Moreover the pattern, which is revealed from this deluge of web access logs must be interesting, useful, and understandable. In this paper, a two phases learning algorithm with a modified kernel for radial basis function neural networks is proposed to classify the web pages on time of access and region of access. In phase one a meta-heuristic approach known as differential evolution is used to reveal the parameters of the modified kernel. The second phase focus on optimization of weights for learning the networks. The simulation result shows that the proposed learning mechanism is evidently producing better classification accuracy vis-√†-vis radial basis function neural networks.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-36071-8_14",
        "year": "2013"
    },
    {
        "title": "New Techniques for Data Preprocessing Based on Usage Logs for Efficient Web User Profiling at Client Side",
        "abstract": "User profiling at client side is an important key task for intelligent information delivery in Web environment. However, several preprocessing also should be carried out prior to constructing user profiles. Moreover, the client side preprocessing should be carried out mostly based on usage logs without knowledge about contents and structures of Websites. In this perspective, we conducted an experiment in users‚Äô natural Web environments during a period of several days, and analyzed collected usage logs and user feedbacks. Based on the results, we designed three important preprocessing techniques and evaluated the performances. We found that the required data preprocessing tasks can be conducted efficiently at client side based solely on usage logs.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1632236",
        "year": "2009"
    },
    {
        "title": "New VLSI design of a max-log-MAP decoder",
        "abstract": "This paper proposes a new design of a MAP decoder suitable for ASIC. The decoder architecture is parallel and pipeline, which makes the design applicable to bit rate communication systems. Max-log-MAP algorithm, which offers a good compromise between performance and complexity, is selected for implementation. One of the components of this algorithm, namely, the transition probability (/spl gamma/) calculation unit is studied and a new low complexity design of this unit is proposed. Overall decoder design is flexible to the transmission block lengths, which makes it suitable for variable length transmission systems.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1359009/",
        "year": "2004"
    },
    {
        "title": "New wideband microstrip antenna using log-periodic technique",
        "abstract": "A novel wideband microstrip antenna using a series-fed linear array of patch resonators in a log-periodic arrangement is described. A 9-element example gives good input v.s.w.r. and radiation control over a 30% bandwidth with better than 70% efficiency. The new array configuration gives wider bandwidth than single-layer or stacked microstrip patches combined with better efficiency than patches on lossy substrates or microstrip spirals.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/abstract/document/4243890/",
        "year": "1980"
    },
    {
        "title": "Non-intrusive transaction monitoring using system logs",
        "abstract": "We consider the problem of online monitoring of transaction instances in enterprise environments, based on footprints left by the instances in system logs and using a state-based reference model of the transaction. Unlike existing approaches, we do not rely on any platform-specific knowledge, neither do we assume footprints to carry correlating identifiers, as injected through instrumentation. We outline a solution for tracking transaction instances at individual and aggregate levels, present preliminary results on theoretical analysis of monitoring precision and conclude with directions of ongoing and future research.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/4575237/",
        "year": "2008"
    },
    {
        "title": "Noncoherent sequence detection of orthogonally modulated signals in flat fading with log-linear complexity",
        "abstract": "Frequency-shift keying (FSK) is an orthogonal modulation technique that is primarily used in relatively low-rate communication systems that operate in the power-limited regime. Optimal noncoherent detection of FSK takes the form of sequence detection and has exponential complexity in the sequence length when implemented through an exhaustive search among all possible sequences. In this work, for the first time in the literature, we present an algorithm that performs generalized-likelihood-ratio-test (GLRT) optimal noncoherent sequence detection of orthogonally modulated signals in flat fading with log-linear complexity in the sequence length. Moreover, for Rayleigh fading channels, the proposed algorithm is equivalent to the maximum-likelihood (ML) noncoherent sequence detector. Finally, we show that our algorithm also solves efficiently the optimal noncoherent sequence detection problem in contemporary radio-frequency-identification (RFID) systems.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7178516/",
        "year": "2015"
    },
    {
        "title": "Nonlinear inversion in electrode logging in a highly deviated formation with invasion using an oblique coordinate system",
        "abstract": "Electrode logging as known in the oil industry is a method for determining the electrical conductivity distribution around a borehole or between two boreholes from the static-field (dc) measurements in the borehole. The authors discuss the reconstruction of the three-dimensional (3D) conductivity around a borehole in a highly deviated formation with invasion. At this moment, they have not included the borehole effect. To solve this problem, the full vector analysis is required. In most available algorithms, for the forward and inverse modeling of the resistivity data, the dipping bed environment is approximated using the staircase-discretization grid. In contrast, they have modeled the dipping-bed environment by introducing an oblique (nonorthogonal) coordinate system. By using the oblique coordinate system, they have gained some advantages over the usual approach. First, the use of the staircasing approximation for the dipping-bed environment can be avoided. This means that they reduce the discretization error, and they can suffice with less discretization points to obtain the results with the same degree of accuracy as the problem formulated in the Cartesian coordinate system. Secondly, the horizontally-symmetry constraints of the conductivity distribution can be included easily in the inversion procedure. Several numerical results are presented to demonstrate the performance of the inversion method using the synthetic \"measured\" data, which are generated by solving a forward-scattering problem numerically with the help of the conjugate gradient fast Fourier transform (CGFFT) method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/823898/",
        "year": "2000"
    },
    {
        "title": "Not-First and Not-Last Detection for Cumulative Scheduling in ${\\cal O}(n^3\\log n)$",
        "abstract": "Not-first/not-last detection is the pendant of edge-finding in constraint-based disjunctive and cumulative scheduling. Both methods provide strong pruning algorithms in constraint programming. This paper shows that the not-first/not-last detection algorithm presented by Nuijten that runs in time Óàª(ùëõ3ùëò) is incorrect and incomplete, where n is the number of tasks and k is the number of different capacity requirements of these tasks. A new correct and complete detection algorithm for cumulative scheduling is then presented which runs in Óàª(ùëõ3logùëõ).",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11963578_6",
        "year": "2006"
    },
    {
        "title": "Notice of Retraction\u003cBR\u003eSimulation and analysis of effect on urban operation of urban storm water logging",
        "abstract": "This article has been retracted by the publisher.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5486651/",
        "year": "2010"
    },
    {
        "title": "Notice of Violation of IEEE Publication Principles\u003cbr\u003eSecure audit logs with forward integrity message authentication codes",
        "abstract": "Notice of Violation of IEEE Publication Principles\"Secure Audit Logs with Forward Integrity Message Authentication Codes\"by Tao Jiang, Ji-qiang Liu, Zhen Hanin the Proceedings of the International Conference on Signal Processing, September 2004, pp. 2655-2658After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.This paper contains portions of original text from the paper cited below. The original text was copied with insufficient attribution (including appropriate references to the original author(s) and/or paper title) and without permission.\"Forward Integrity for Secure Audit Logs\"by Mihir Bellare and Bennet S. Yeein Technical Report CS98-580, Department of Computer Science and Engineering, University of California at San Diego, November 1997\"Cryptographic Support for Secure Logs on Untrusted Machines\"by Bruce Schneier, John Kelseyin The Seventh USENIX Security Symposium Proceedings, USENIX Press, January 1998, pp. 53-62We propose a forward integrity message authentication codes (MACs) method making all log entries generated prior to the logging machine's compromise impossible for the attacker to modify or destroy undetectably. As a good solution to keep the log data from being tempered with, the method is used in a system called secure log server in which we collect the log information for further system monitoring and auditing in our own way. A prototype was developed and the impact of its deployment in a real environment was measured.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1442328/",
        "year": "2004"
    },
    {
        "title": "Novel pre-processing technique for web log mining by removing global noise and web robots",
        "abstract": "Today internet has made the life of human dependent on it. Almost everything and anything can be searched on net. Web pages usually contain huge amount of information that may not interest the user, as it may not be the part of the main content of the web page. Web Usage Mining (WUM) is one of the main applications of data mining, artificial intelligence and so on to the web data and forecast the user's visiting behaviors and obtains their interests by investigating the samples. Since WUM directly involves in applications, such as, e-commerce, e-learning, Web analytics, information retrieval etc. Weblog data is one of the major sources which contain all the information regarding the users visited links, browsing patterns, time spent on a particular page or link and this information can be used in several applications like adaptive web sites, modified services, customer summary, pre-fetching, generate attractive web sites etc. There are varieties of problems related with the existing web usage mining approaches. Existing web usage mining algorithms suffer from difficulty of practical applicability. This paper continues the line of research on Web access log analysis is to analyze the patterns of web site usage and the features of users behavior. It is the fact that the normal Log data is very noisy and unclear and it is vital to preprocess the log data for efficient web usage mining process. Preprocessing is the process comprises of three phases which includes data cleaning, user identification, and pattern discovery and pattern analysis. Log data is characteristically noisy and unclear, so preprocessing is an essential process for effective mining process. In this paper, a novel pre-processing technique is proposed by removing local and global noise and web robots. Preprocessing is an important step since the Web architecture is very complex in nature and 80% of the mining process is done at this phase. Anonymous Microsoft Web Dataset and MSNBC.com Anonymous Web Dat...(View more)",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6412976/",
        "year": "2012"
    },
    {
        "title": "OBDA for Log Extraction in Process Mining",
        "abstract": "Process mining is an emerging area that synergically combines model-based and data-oriented analysis techniques to obtain useful insights on how business processes are executed within an organization. Through process mining, decision makers can discover process models from data, compare expected and actual behaviors, and enrich models with key information about their actual execution. To be applicable, process mining techniques require the input data to be explicitly structured in the form of an event log, which lists when and by whom different case objects (i.e., process instances) have been subject to the execution of tasks. Unfortunately, in many real world set-ups, such event logs are not explicitly given, but are instead implicitly represented in legacy information systems. To apply process mining in this widespread setting, there is a pressing need for techniques able to support various process stakeholders in data preparation and log extraction from legacy information systems. The purpose of this paper is to single out this challenging, open issue, and didactically introduce how techniques from intelligent data management, and in particular ontology-based data access, provide a viable solution with a solid theoretical basis.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-61033-7_9",
        "year": "2017"
    },
    {
        "title": "Object Detection and Localization Using Deep Convolutional Networks with Softmax Activation and Multi-class Log Loss",
        "abstract": "We introduce a deep neural network that can be used to localize and detect a region of interest (ROI) in an image. We show how this network helped us extract ROIs when working on two separate problems: a whale recognition problem and a heart volume estimation problem. In the former problem, we used this network to localize the head of the whale while in the later we used it to localize the heart left ventricle from MRI images. Most localization networks regress a bounding box around the region of interest. Unlike these architecture, we treat the problem as a classification problem where each pixel in the image is a separate class. The network is trained on images along with masks which indicate where the object is in the image. We treat the problem as a multi-class classification. Therefore, the last layer has a softmax activation. Furthermore, during training, the mutli-class log loss is minimized just like any classification task.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-41501-7_41",
        "year": "2016"
    },
    {
        "title": "OFDM radar detection in Log normal clutter",
        "abstract": "Today, target detection in Orthogonal Frequency Division Multiplexing (OFDM) Radars is very interesting. Recently target detection in OFDM radars has been reported But in all of them assumed the RCS of target is constant and clutter has Gaussian distribution. However, in this paper, we investigate the effect of target fluctuation (Swerling case II) and Non Gaussian (Log normal) clutter in the OFDM radar performance We assume the fluctuation occurs only between subcarriers. For comparisons, we consider two scenarios We consider the GLR detector for target detection and investigate the target fluctuation on GLR detector performance. We provide a few numerical examples to illustrate the detection performance for moving target under conditions of two scenarios and fluctuating model in presence of Log normal clutter.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6419336/",
        "year": "2012"
    },
    {
        "title": "On Analyzing Web Log Data: A Parallel SequenceMining Algorithm",
        "abstract": "stract:This chapter contains sections titled:IntroductionRelated WorkClick Stream Analysis: An Integrated SolutionPerformance and a Sample Business AnalysisFuture Work and ConclusionThis chapter contains sections titled:AcknowledgmentsReferences]]\u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5769732",
        "year": "2005"
    },
    {
        "title": "On building entity recommender systems using user click log and freebase knowledge",
        "abstract": "Due to their commercial value, search engines and recommender systems have become two popular research topics in both industry and academia over the past decade. Although these two fields have been actively and extensively studied separately, researchers are beginning to realize the importance of the scenarios at their intersection: providing an integrated search and information discovery user experience. In this paper, we study a novel application, i.e., personalized entity recommendation for search engine users, by utilizing user click log and the knowledge extracted from Freebase.To better bridge the gap between search engines and recommender systems, we first discuss important heuristics and features of the datasets. We then propose a generic, robust, and time-aware personalized recommendation framework to utilize these heuristics and features at different granularity levels. Using movie recommendation as a case study, with user click log dataset collected from a widely used commercial search engine, we demonstrate the effectiveness of our proposed framework over other popular and state-of-the-art recommendation techniques.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2556233",
        "year": "2014"
    },
    {
        "title": "On data logging in real-time process control systems",
        "abstract": "Real-time process control systems require hard real-time data logging tasks to access all of the data sampled from plants and monitoring tasks to analyze the status of the plants. This paper presents an integrated scheduling scheme for hard real-time data logging tasks and soft real-time monitoring tasks, both types of which require disk I/Os. Conventional disk scheduling policies like Shortest-Seek-Time-First (SSTF), SCAN, and C-SCAN, reorder disk I/O requests to minimize the disk head's seek time, thus making it very difficult to guarantee the timely completion of the tasks requiring disk I/Os. Our scheduling scheme provides timeliness guarantees by serializing all disk I/O requests of these tasks based on their priorities. Because it doesn't require any special-purpose real-time disk scheduling algorithm, the proposed scheme can be implemented on top of general-purpose operating systems like UNIX.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/554993/",
        "year": "1996"
    },
    {
        "title": "On Improving Website Connectivity by Using Web-Log Data Streams",
        "abstract": "When people visit Websites, they desire to efficiently and exactly access the contents they are interested in without delay. However, due to the constant changes of site contents and user patterns, the access efficiency of Websites cannot be optimized, especially in peak hours. In this paper, we first address the problems of access efficiency in Websites during peak hours and then propose new measures to evaluate access efficiency. An efficient algorithm is introduced to detect user access patterns using Website topology and Web-log stream data. Adopting this method, we can online modify a Website topology so that the new topology can improve the Website connectivity to adapt current visitors‚Äô access patterns. A real sports Website is used to evaluate the effectiveness of our proposed method of accelerating user access to related contents. The results of the evaluation presented in this paper suggest that this method is feasible to online improve the connectivity of a Website intelligently.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-24571-1_32",
        "year": "2004"
    },
    {
        "title": "On Systematic Approach to Discovering Periodic Patterns in Event Logs",
        "abstract": "Discovering periodic patterns from historical information is a computationally hard problem due to the large amounts of historical data to be analyzed and due to a high complexity of the patterns. This work shows how the derivations rules for periodic patterns can be applied to discover complex patterns in case of logs of events. The paper defines a concept of periodic pattern and its validation in a workload trace created from the logs of events. A system of derivations rules that transforms periodic patterns into the logically equivalent ones is proposed. The paper presents a systematic approach based on the system of derivation rules to discovery of periodic patterns in logs of events.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-45243-2_23",
        "year": "2016"
    },
    {
        "title": "On the log-power NHPP software reliability model",
        "abstract": "A simple software reliability model, the log-power nonhomogeneous Poisson process (NHPP) model, is studied. The log-power NHPP model has several interesting properties, such as simple graphical interpretations and simple forms of the maximum likelihood estimates for the parameters. The authors assess this model by considering its simplicity, data fitting and predicting ability. They have applied the log-power model to many sets of existing software reliability data. The results show that this model is able to fit different data sets and has a relatively high predicting ability. This, together with its simplicity and the graphical interpretation which provides a useful reliability engineering tool, shows that the log-power model is an applicable software reliability model in practice.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/285862/",
        "year": "1992"
    },
    {
        "title": "On the n log n isomorphism technique (A Preliminary Report)",
        "abstract": "Tarjan has given an algorithm for deciding isomorphism of two groups of order n (given as multiplication tables) which runs in O(n(log2n+O(1)) steps where n is the order of the groups. Tarjan uses the fact that a group of n is generated by log n elements. In this paper, we show that Tarjan's technique generalizes to isomorphism of quasigroups, latin squares, Steiner systems, and many graphs generated from these combinatorial objects.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=804331",
        "year": "1978"
    },
    {
        "title": "On the nlog n isomorphism technique (A Preliminary Report)",
        "abstract": "Tarjan has given an algorithm for deciding isomorphism of two groups of order n (given as multiplication tables) which runs in O(n(log2n+O(1)) steps where n is the order of the groups. Tarjan uses the fact that a group of n is generated by log n elements. In this paper, we show that Tarjan's technique generalizes to isomorphism of quasigroups, latin squares, Steiner systems, and many graphs generated from these combinatorial objects.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=804331",
        "year": "1978"
    },
    {
        "title": "On the study of GRBF and polynomial kernel based support vector machine in web logs",
        "abstract": "World Wide Web continues to grow day-by-day and it is difficult to track and understand users' need for the owners of a website. Therefore, an intelligent analyzer is required to find out the browsing patterns of a user. Moreover, the pattern which is revealed from this surge of web access logs must be useful, motivating, and logical. In this paper, two different kernel functions of support vector machine (SVM) are used to classify the web pages based on access time and region. Additionally, kernel parameters are also varied to study the trends of the accuracy of classification. Experimental results reveal that Gaussian radial basis function (GRBF) kernel based S VM is performing better than the polynomial kernel based SVM.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6691384/",
        "year": "2013"
    },
    {
        "title": "On the ubiquity of logging in distributed file systems",
        "abstract": "It is argued that logging should be at the forefront of techniques considered by a system designer when implementing a distributed file system. The use of logging in different guises in the Coda file system is described. Coda is a distributed file system whose goal is to provide highly available, scalable, secure and efficient shared file access in an environment of Unix workstations. High availability is achieved through two complementary mechanisms, i.e., server replication and disconnected operation. Logging is used in at least three distinct ways in the current implementation of Coda. First, value logging forms the basis of the recovery technique for recoverable virtual memory (RVM), a transactional virtual memory package. Second, operating logging is used in the replay log that records update activity made by a client while disconnected from all servers. Third, operation logging is used in resolution logs on severs to allow transparent resolution of directory updates made to partitioned server replicas.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/275676/",
        "year": "1992"
    },
    {
        "title": "One Graph Is Worth a Thousand Logs: Uncovering Hidden Structures in Massive System Event Logs",
        "abstract": "In this paper we describe our work on pattern discovery in system event logs. For discovering the patterns we developed two novel algorithms. The first is a sequential and efficient text clustering algorithm which automatically discovers the templates generating the messages. The second, the PARIS algorithm (Principle Atom Recognition In Sets), is a novel algorithm which discovers patterns of messages that represent processes occurring in the system. We demonstrate the usefulness of our analysis, on real world logs from various systems, for debugging of complex systems, efficient search and visualization of logs and characterization of system behavior.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-04180-8_32",
        "year": "2009"
    },
    {
        "title": "Online and incremental mining of separately-grouped Web access logs",
        "abstract": "The rising popularity of electronic commerce makes data mining an indispensable technology for business competitiveness. The World Wide Web provides abundant raw data in the form of Web access logs, Web transaction logs and Web user profiles. Without data mining tools, it is impossible to make any sense of such massive data. We focus on Web usage mining because it deals most appropriately with understanding user behavioral patterns which is the key to successful customer relationship management. Previous work dealt separately with specific issues of Web usage mining and made assumptions without taking a holistic view and thus, had limited practical applicability. We formulate a novel and more holistic version of Web usage mining termed transactionized logfile mining (TRALOM) to effectively and correctly identify transactions as well as to mine useful knowledge from Web access logs. We also introduce a new data structure, called the WebTrie, to efficiently hold useful preprocessed data so that TRALOM can be done in an online and incremental fashion. Experiments conducted on real Web server logs verify the usefulness and practicality of our proposed techniques.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1181643/",
        "year": "2002"
    },
    {
        "title": "Online anomaly detection using dimensionality reduction techniques for HTTP log analysis",
        "abstract": "Modern web services face an increasing number of new threats. Logs are collected from almost all web servers, and for this reason analyzing them is beneficial when trying to prevent intrusions. Intrusive behavior often differs from the normal web traffic. This paper proposes a framework to find abnormal behavior from these logs. We compare random projection, principal component analysis and diffusion map for anomaly detection. In addition, the framework has online capabilities. The first two methods have intuitive extensions while diffusion map uses the Nystr√∂m extension. This fast out-of-sample extension enables real-time analysis of web server traffic. The framework is demonstrated using real-world network log data. Actual abnormalities are found from the dataset and the capabilities of the system are evaluated and discussed. These results are useful when designing next generation intrusion detection systems. The presented approach finds intrusions from high-dimensional datasets in real time.",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S1389128615002650",
        "year": "2015"
    },
    {
        "title": "Online Event Correlations Analysis in System Logs of Large-Scale Cluster Systems",
        "abstract": "It has been long recognized that failure events are correlated, not independent. Previous research efforts have shown the correlation analysis of system logs is helpful to resource allocation, job scheduling and proactive management. However, previous log analysis methods analyze the history logs offline. They fail to capture the dynamic change of system errors and failures. In this paper, we purpose an online log analysis approach to mine event correlations in system logs of large-scale cluster systems. Our contributions are three-fold: first, we analyze the event correlations of system logs of a 260-nodes production Hadoop cluster system, and the result shows that the correlation rules of logs change dramatically in different periods; Second, we present a online log analysis algorithm Apriori-SO; third, based on the online event correlations mining, we present an online event prediction method that can predict diversities of failure events with the great detail. The experiment result of a 260-nodes production Hadoop cluster system shows that our online log analysis algorithm can analyze the log streams to obtain event correlation rules in soft real time, and our online event prediction method can achieve higher precision rate and recall rate than the offline log analysis approach.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-15672-4_23",
        "year": "2010"
    },
    {
        "title": "Online game bot detection based on party-play log analysis",
        "abstract": "As online games become popular and the boundary between virtual and real economies blurs, cheating in games has proliferated in volume and method. In this paper, we propose a framework for user behavior analysis for bot detection in online games. Specifically, we focus on party play which reflects the social activities among gamers: in a Massively Multi-user Online Role Playing Game (MMORPG), party play is a major activity that game bots exploit to keep their characters safe and facilitate the acquisition of cyber assets in a fashion very different from that of normal humans. Through a comprehensive statistical analysis of user behaviors in game activity logs, we establish threshold levels for the activities that allow us to identify game bots. Based on this, we also build a knowledge base of detection rules, which are generic. We apply our rule reasoner to AION, a popular online game serviced by NCsoft, Inc., a leading online game company based in Korea.",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0898122112000442",
        "year": "2013"
    },
    {
        "title": "Onomatopoeia Learning Support for Japanese Language Learners Using Ubiquitous Learning Log System with eBook",
        "abstract": "This paper describes our study where we aim to support onomatopoeia learning for Japanese language learners using the ubiquitous learning log system called SCROLL (System for Capturing and Reminding Of Learning Log). We have improved our system by adding a new function EPUB (Electronic PUBlication) in order to improve quiz efficiency and to enrich its contents. Our pilot evaluation indicates that the quiz function worked sufficiently. However, some user-unfriendliness was pointed out.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7756993/",
        "year": "2016"
    },
    {
        "title": "Ontogeny of plunge diving behaviour in brown boobies: application of a data logging technique to hand-raised seabirds",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0967064507000136",
        "year": "2007"
    },
    {
        "title": "Ontology and Web Usage Mining towards an Intelligent Web Focusing Web Logs",
        "abstract": "Today, Internet is a huge database which comprises of a large number of Web sites, search engines and other information. Due to the unstructured and semi structured data in the web pages, it is a challenging task for researchers to make a relevant and efficient search in warehouse of such type of database. Ontology may be a good mechanism for achieving this goal and Web Mining technique may be used to discover and extract meaningful or relevant information from the Web documents. In this paper, analysis of web usage mining has been made with the help of an example of sample data for which WebLog analyzer tool, ‚ÄúWeb Log Expert‚Äù has been used and it has been appended with the development of an Ontology for an intelligent or efficient web and it's relation with web usage mining. Finally, it also summarizes some other research challenges towards an intelligent machine and web environment.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5702011/",
        "year": "2010"
    },
    {
        "title": "Ontology change detection using a version log",
        "abstract": "In this article, we propose a new ontology evolution approach that combines a top-down and a bottom-up approach. This means that the manual request for changes (top-down) by the ontology engineer is complemented with an automatic change detection mechanism (bottom-up). The approach is based on keeping track of the different versions of ontology concepts throughout their lifetime (called virtual versions). In this way, changes can be defined in terms of these virtual versions.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/11574620_42",
        "year": "2005"
    },
    {
        "title": "Ontology-Based Data Access for Extracting Event Logs from Legacy Data: The onprom Tool and Methodology",
        "abstract": "Process mining aims at discovering, monitoring, and improving business processes by extracting knowledge from event logs. In this respect, process mining can be applied only if there are proper event logs that are compatible with accepted standards, such as extensible event stream (XES). Unfortunately, in many real world set-ups, such event logs are not explicitly given, but instead are implicitly represented in legacy information systems. In this work, we exploit a framework and associated methodology for the extraction of XES event logs from relational data sources that we have recently introduced. Our approach is based on describing logs by means of suitable annotations of a conceptual model of the available data, and builds on the ontology-based data access (OBDA) paradigm for the actual log extraction. Making use of a real-world case study in the services domain, we compare our novel approach with a more traditional extract-transform-load based one, and are able to illustrate its added value. We also present a set of tools that we have developed and that support the OBDA-based log extraction framework. The tools are integrated as plugins of the ProM process mining suite.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-59336-4_16",
        "year": "2017"
    },
    {
        "title": "Ontology-Based Partitioning of Data Steam for Web Mining: A Case Study of Web Logs",
        "abstract": "This paper presents a nevel method partitioning steaming data based on ontology. Web directory service is applied to enrich semantics to web logs, as categorizing them to all possible hierarchical paths. In order to detect the candidate set of session identifiers, semantic factors like semantic mean, deviation, and distance matrix are established. Eventually, each semantic session is obtained based on nested repetition of top-down partitioning and evaluation process. For experiment, we applied this ontology-oriented heuristics to sessionize the access log files for one week from IRCache. Compared with time-oriented heuristics, more than 48% of sessions were additionally detected by semantic outlier analysis.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-24685-5_31",
        "year": "2004"
    },
    {
        "title": "Ontology-Driven Extraction of Event Logs from Relational Databases",
        "abstract": "Process mining is an emerging discipline whose aim is to discover, monitor and improve real processes by extracting knowledge from event logs representing actual process executions in a given organizational setting. In this light, it can be applied only if faithful event logs, adhering to accepted standards (such as XES), are available. In many real-world settings, though, such event logs are not explicitly given, but are instead implicitly represented inside legacy information systems of organizations, which are typically managed through relational technology. In this work, we devise a novel framework that supports domain experts in the extraction of XES event log information from legacy relational databases, and consequently enables the application of standard process mining tools on such data. Differently from previous work, the extraction is driven by a conceptual representation of the domain of interest in terms of an ontology. On the one hand, this ontology is linked to the underlying legacy data leveraging the well-established ontology-based data access (OBDA) paradigm. On the other hand, our framework allows one to enrich the ontology through user-oriented log extraction annotations, which can be flexibly used to provide different log-oriented views over the data. Different data access modes are then devised so as to view the legacy data through the lens of XES.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-42887-1_12",
        "year": "2016"
    },
    {
        "title": "Open entity extraction from web search query logs",
        "abstract": "In this paper we propose a completely unsupervised method for open-domain entity extraction and clustering over query logs. The underlying hypothesis is that classes defined by mining search user activity may significantly differ from those typically considered over web documents, in that they better model the user space, i.e. users' perception and interests. We show that our method outperforms state of the art (semi-)supervised systems based either on web documents or on query logs (16% gain on the clustering task). We also report evidence that our method successfully supports a real world application, namely keyword generation for sponsored search.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1873839",
        "year": "2010"
    },
    {
        "title": "Open or Closed Mouth State Detection: Static Supervised Classification Based on Log-Polar Signature",
        "abstract": "The detection of the state open or closed of mouth is an important information in many applications such as hypo-vigilance analysis, face features segmentation or emotions recognition. In this work we propose a supervised classification method for mouth state detection based on retina filtering and cortex analysis inspired by the human visual system. The first stage of the method is the learning of reference signatures (Log Polar Spectrums) from some open and closed mouth images manually classified. The signatures are constructed by computing the amplitude log-polar spectrum of the retina filtered images. Principal Components Analysis (PCA) is then performed using the Log Polar Spectrum as feature vectors to reduce the number of dimension by keeping 95 % of the total variance. Finally a binary SVM classifier is trained using the projections the principal components given by the PCA in order to classify the mouth.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-88458-3_99",
        "year": "2008"
    },
    {
        "title": "Operation Logging System using the database for the synchrotron radiation beam lines at the Photon Factory",
        "abstract": "The operation Logging System has been designed and built using the Oracle database for the twenty-two synchrotron radiation beam lines at the 2.5 GeV positron storage ring at the Photon Factory, where X-ray/VUV synchrotron radiation experiments are simultaneously carried out. The operation Logging System has a real-time capability to automatically store the database with all possible operational events of all vacuum valves/shutters and interlock signals, and all static operational data, including the pressures of the beam lines and the storage ring, and related operational data which represent the physical behaviors of the beam lines. By retrieving any combination of operational data, the system allows one to reproduce the physical behaviors that have occurred in the beam lines.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/751239/",
        "year": "1997"
    },
    {
        "title": "Operational-Log Analysis for Big Data Systems: Challenges and Solutions",
        "abstract": "Big data systems (BDSs) are complex, consisting of multiple interacting hardware and software components, such as distributed computing nodes, databases, and middleware. Any of these components can fail. Finding the failures' root causes is extremely laborious. Analysis of BDS-generated logs can speed up this process. The logs can also help improve testing processes, detect security breaches, customize operational profiles, and aid with any other tasks requiring runtime-data analysis. However, practical challenges hamper log analysis tools' adoption. The logs emitted by a BDS can be thought of as big data themselves. When working with large logs, practitioners face seven main issues: scarce storage, unscalable log analysis, inaccurate capture and replay of logs, inadequate log-processing tools, incorrect log classification, a variety of log formats, and inadequate privacy of sensitive data. Some practical solutions exist, but serious challenges remain. This article is part of a special issue on Software Engineering for Big Data Systems.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7412635/",
        "year": "2016"
    },
    {
        "title": "Optimal Process Mining for Large and Complex Event Logs",
        "abstract": "This paper addresses the problem of process discovery from large and complex event logs. We depart from the existing literature and formulate the problem of optimal process discovery. A formal mathematical programming model is given based on a novel hierarchical structuration of the event logs. Desired properties of event trace score functions are described, and the properties of optimal process models are proved. A combination of Monte Carlo optimization and tabu search is proposed to overcome the complexity related to the huge size of the event logs and the combinatorial solution space. Numerical results show that our approach is suitable for large event logs and that it performs better than the state-of-the-art approaches. We also demonstrate the applicability of our method on a real case study in health care. This paper illustrates the benefits of combining techniques from the operational research and the process mining fields.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8252788/",
        "year": "2018"
    },
    {
        "title": "Optimization of Laplace of Gaussian (LoG) filter for enhanced edge detection: A new approach",
        "abstract": "In this paper a very remarkable approach on edge detection has been explored. The most important aspect of image segmentation is edge detection. Edge detection is a meaningful interpretation of discontinuities of similar intensity values in image analysis [1]. Laplace of Gaussian (LoG) filter is a conventional edge detecting tool. Threshold neighboring pixel value (T) and standard deviation parameters are optimized with the help of Cuckoo Search optimization in order to augment the edge detection potential of LoG. PFOM (Pratt's Figure of Merit) is used as quality factor for edge detection analysis.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6959172/",
        "year": "2014"
    },
    {
        "title": "A project restarting support system using the historical log of a user's window usage",
        "abstract": "This paper proposes a project restarting support system using the historical log of window usage on a computer desktop. The system allows users to simultaneously reopen files associated with a main file by automatically detecting important windows and associated windows from a log. By using our system, users can manage windows in a project without having to manually group windows. We conduct an experiment to evaluate the system. The results indicate that the system can accurately identify as important windows the main files a user used for projects by clustering keyboard operation times on windows; it can also detect files associated with the main file without detection omissions by clustering window z values and visible representation ratios.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1952231",
        "year": "2010"
    },
    {
        "title": "A Quantitative Comparison of Speed and Reliability for Log-Polar Mapping Techniques",
        "abstract": "A space-variant representation of images is of great importance for active vision systems capable of interacting with the environment. A precise processing of the visual signal is achieved in the fovea, and, at the same time, a coarse computation in the periphery provides enough information to detect new saliences on which to bring the focus of attention. In this work, different techniques to implement the blind-spot model for the log-polar mapping are quantitatively analyzed to assess the visual quality of the transformed images and to evaluate the associated computational load. The technique with the best trade-off between these two aspects is expected to show the most efficient behaviour in robotic vision systems, where the execution time and the reliability of the visual information are crucial.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-23968-7_5",
        "year": "2011"
    },
    {
        "title": "A query log analysis of dataset search",
        "abstract": "Data is one of the most important digital assets in the world and its availability on the web is increasing. To use it effectively, we need tools that can retrieve the most relevant datasets to match our information needs. Web search engines are not well suited for this task, as they are designed primarily for documents, not data. In this paper, we present the first query log analysis for dataset search, based on logs of four national open data portals. Our aim is to gain a better understanding of the typical users of these portals and the types of queries they issue, and frame the findings in the broader context of dataset search. The logs suggest that queries issued on data portals differ from those issued to web search engines in their length and structure. From the analysis we could also infer that the portals are used exploratively, rather than to answer focused questions. These insights can inform the design of more effective dataset retrieval technology, and improve the user experience of data portals.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-319-60131-1_29",
        "year": "2017"
    },
    {
        "title": "A reduced complexity implementation of the Log-Map algorithm for turbo-codes decoding",
        "abstract": "In this paper, we propose a reduced complexity implementation scheme of the Log-Map algorithm for turbo-codes decoding. By re-arranging the structure of the computation and using adaptive approximation, the computation in the Log-Map algorithm such as the state metric and the log-likelihood ratio calculation could be simplified or reduced adaptively. Simulation results show that more than 45% of the computation can be reduced with almost no performance degradation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/861007/",
        "year": "2000"
    },
    {
        "title": "A sampling-based approach to accelerating queries in log management systems",
        "abstract": "Log management systems are common in industry and an essential part of a system administrator‚Äôs toolkit. Examples include Splunk, elk, Log Insight, Sexilog, and more. Logs in these systems are characterized by a small number of predefined fields such as timestamp and host, with the bulk of an entry being unstructured text. System administrators query these logs using a combination of range constraints over predefined fields and patterns or regular expressions over the text portion of the message. These queries are both complex and diverse.We propose a method for maintaining a subset of these logs in a much smaller database known as a sublog. Because queries are issued against a much smaller data set they run to completion quickly and avoid common scaling bottlenecks. However, the improvement in performance comes at a price. Because we only consider a subset of the original data, we are only able to provide approximate responses. Nonetheless, the reduction in accuracy is minimal and we are able to produce high-quality, high-performance results.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2989221",
        "year": "2016"
    },
    {
        "title": "A Search Engine Based on Query Logs, and Search Log Analysis by Automatic Language Identification",
        "abstract": "This work describes a variation on the traditional Information Retrieval paradigm, where instead of text documents being indexed according to their content, they are indexed according to the search terms previous users have used in finding them. We determine the effectiveness of this approach by indexing a sample of query logs from the European Library, and describe its usefulness for multilingual searching. In our analysis of the search logs, we determine the language of the past queries automatically, and annotate the search logs accordingly. From this information, we derive matrices to show that a) users tend to persist with the same query language throughout a query session, and b) submit queries in the same language as the interface they have selected.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-15754-7_64",
        "year": "2010"
    },
    {
        "title": "A Search Engine Log Analysis of Music-Related Web Searching",
        "abstract": "We explore music search behavior by identifying music-related queries in a large (over 20 million queries) search engine log, gathered over three months in 2006. Music searching is a significant information behavior: approximately 15% of users conduct at least one music search in the time period studied, and approximately 1.35% of search activities are connected to music. We describe the structural characteristics of music searches‚Äîquery length and frequency for result selection‚Äîand also summarize the most frequently occurring search terms and destinations. The findings are compared to earlier studies of general search engine behavior and to qualitative studies of natural language music information needs statements. The results suggest the need for specialized music search facilities and provide implications for the design of a music information retrieval system.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-12090-9_7",
        "year": "2010"
    },
    {
        "title": "A ship-borne data logging and processing system for acoustic fish surveys",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0165783681900163",
        "year": "1981"
    },
    {
        "title": "A simple logging system for safe internet use",
        "abstract": "Although the Internet offers numerous advantages, it raises many information security risks, especially against young people and children, who are today amongst the largest user groups of mobile and online technologies all around the world. Therefore, to empower and protect Internet users, it is necessary to develop proper strategies and tools to encapsulate their needs, and identify and prevent all types of the information security risks that may arise during the use of the Internet. In this study, a tracking system to ensure the safe use of the Internet is proposed. Considering the distribution of its potential users, the system has an easy-to-use graphical user interface. By recognizing dangerous web sites and IP addresses, the proposed system blocks access to those sites and this way provides reliable Internet access to its users. During the Internet access, it analyzes accessed IP addresses and port numbers in terms of access type and time and informs the user of the corresponding port numbers which must be closed for safe Internet access. Moreover, by continuously checking the host redirection file of the computer it runs on, it identifies redirections from web addresses to specific IP addresses and this way provides protection against phishing attacks which are becoming one of the most common Internet threats. Although the proposed system is a simple application since it is an open source, freeware application designed for children, it can be improved to consider more sophisticated attack types.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8090252/",
        "year": "2017"
    },
    {
        "title": "A simulation model for the Philippine selective logging system: Case study of a timber company",
        "abstract": "This paper describes a simulation model developed for the Philippine Selective Logging System. The model is purposely designed to be flexible and capable of addressing various forest management concerns such as: (1) the length of cutting cycle; (2) growth rate: (3) stand treatments; (4) site degradation; (5) diameter cutting limit; (6) volume of allowable cut; (7) logging and utilization efficiency; and (8) the extent and intensity of timber stand improvement activities.The model is used to simulate the timber management activities and practices of an actual timber company. Results from the simulation runs indicate that selective logging as practiced by the timber company is not sustainable, particularly in terms of the calculated allowable volume of harvest. Variants of the allowable cut formula prescribed by the selective logging systems are examined and are found to be non-sustainable at the current level of allowable cut. Various treatment combinations are also examined. Most of them generated sustainable yield only up to the second cutting cycle. The few strategies that reached the third cutting cycle include timber stand improvement as one of the treatments.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S030147978371056X",
        "year": "1993"
    },
    {
        "title": "A single tri-axial accelerometer-based real-time personal life log system capable of human activity recognition and exercise information generation",
        "abstract": "Recording a personal life log (PLL) of daily activities in a ubiquitous environment is an emerging application of information technology. In this work, we present a single tri-axial accelerometer-based PLL system capable of human activity recognition and exercise information generation. Our PLL system exhibits two main functions: activity recognition and exercise information generation. For activity recognition, the system first recognizes a state of daily activities based on the statistical and spectral features of the accelerometer signals. An activity within the recognized state is then recognized using a set of augmented features, including autoregressive coefficients, signal magnitude area, and tilt angle, via linear discriminant analysis and hierarchical artificial neural networks. Upon the recognition of each activity, the system further estimates exercise information that includes energy expenditure based on metabolic equivalents, stride length, step count, walking distance, and walking speed. Our PLL system operates in real-time, and the life log information it generates is archived in a daily log database. We have validated our PLL system for six daily activities (i.e., lying, standing, walking, going-upstairs, going-downstairs, and driving) via subject-independent and subject-dependent recognition on a total of twenty subjects, achieving an average recognition accuracy of 94.43 and 96.61%, respectively. Our results demonstrate the feasibility of a portable real-time PLL system that could be used for u-lifecare and u-healthcare services in the near future.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2071122",
        "year": "2011"
    },
    {
        "title": "A snapshot system based on cloud storage Log-Structured Block System",
        "abstract": "In order to enhance fault-tolerance of cloud storage system in cloud computing environment, current snapshot technology and log-structured file-system mechanism are studied. A snapshot system based on HLBS(Hadoop Distributed File System based Log-Structured Block storage System) is designed and implemented. By this design, new snapshots are added to HLBS, from which user data could be saved and recovered for cloud storage system HLBS. HLBS is implemented upon Hadoop Distributed File System (HDFS) with the idea of Log-Structured File System (LFS) without changing the source code of HDFS. Snapshot system of HLBS could not only prevent data from missing but also help HLBS rollback and forward to any time of storage data. To improve the expandabilities, availabilities and fault-tolerance of HLBS, snapshot system is a good feature to be realized. At last, HLBS snapshot system is analyzed and compared with other snapshot technologies.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6819658/",
        "year": "2014"
    },
    {
        "title": "A Spanning Tree-Based Human Activity Prediction System Using Life Logs from Depth Silhouette-Based Human Activity Recognition",
        "abstract": "In this work, we propose a Human Activity Prediction (HAP) system using activity sequence spanning trees constructed from a life-log created by a video sensor-based daily Human Activity Recognition (HAR) system using time-sequential Independent Component (IC)-based depth silhouette features with Hidden Markov Models (HMMs). In the daily HAR system, the IC features are extracted from the collection of the depth silhouettes containing various daily human activities such as walking, sitting, lying, cooking, eating etc. Using these features, HMMs are used to model the time sequential features and recognize various human activities. The depth silhouette-based human activity recognition system is used to recognize daily human activities automatically in real time, which creates a life-log of daily activity events. In this work, we propose a method for human activity prediction using fixed-length activity sequence spanning trees based on the life-log. Utilizing the consecutive activities recorded in an activity sequence database (i.e. life-log) for a specific period of time of each day over a period such as a month, the fixed-length spanning trees can be constructed for the sequences starting with each activity where the leaf nodes contain the frequency of the fixed-length consecutive activity sequences. Once the trees are constructed, to predict an activity after a sequence of activities, we traverse the spanning trees until a path up to the previous node of the leaf nodes is matched with the testing pattern. Finally, we can predict the next activity based on the highest frequency of the leaf nodes along the matched path. The prediction experiments over the computer simulated data which is based on the daily logs show satisfactory results. Our video sensor-based human activity recognition and prediction systems can be utilized for practical applications such as smart and proactive healthcare.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-23672-3_37",
        "year": "2011"
    },
    {
        "title": "A Study for Classification of Web Browser Log and Timeline Visualization",
        "abstract": "Types of logs, such as cache, history, cookie and downloads list, are created by a web browser. Digital forensic investigators analyze these logs and obtain useful information related to cases. In fact, most of the existing tools simply parse log files. As a result, investigators have to classify and analyze log data at firsthand in the process of digital forensic investigation. In particular, in the case of massive data, they should spend enormous time analyzing the data. Therefore, in this paper, with parsed information on cache, history, cookie and download list, we propose data classification and timeline visualization method to improve analysis in efficient way for reducing investigation time and work. Also, ‚ÄùWEFA‚Äù, a developed tool based on the research work, is to be introduced.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-35416-8_14",
        "year": "2012"
    },
    {
        "title": "A survey of query log privacy-enhancing techniques from a policy perspective",
        "abstract": "As popular search engines face the sometimes conflicting interests of protecting privacy while retaining query logs for a variety of uses, numerous technical measures have been suggested to both enhance privacy and preserve at least a portion of the utility of query logs. This article seeks to assess seven of these techniques against three sets of criteria: (1) how well the technique protects privacy, (2) how well the technique preserves the utility of the query logs, and (3) how well the technique might be implemented as a user control. A user control is defined as a mechanism that allows individual Internet users to choose to have the technique applied to their own query logs.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1409222",
        "year": "2008"
    },
    {
        "title": "A survey of web log data and their application in use-based design",
        "abstract": "Web-based logs contain potentially useful data with which designers can assess the usability and effectiveness of their choices. Most guides to World Wide Web (Web) design derived from artistic or usability principles feature no empirical validation, while empirical studies of Web use typically rely on observer ratings. Several sources of unobtrusive usage data are available to Web designers, including Web server logs, client-side logs, and other data. The naturally-occurring traces recorded in these logs offer a rich data source, amenable to normative use assessments and to experimental research comparing alternative Web designs. Identification of types of Web server logs, client logs, types and uses of log data, and issues associated with the validity these data, are enumerated. Finally, frameworks that outline how sources of use-based data can be triangulated to assess Web design are illustrated, and an approach to experimentation that overcomes many log data validity issues is presented.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/926541/",
        "year": "2001"
    },
    {
        "title": "A survey on predicting user behavior based on web server log files in a web usage mining",
        "abstract": "A presently internet is most imperative part of human life. The internet is an growing day by day, so online users are also escalating. The interesting information for knowledge of extracting from such enormous data demands for new logic and the new method. Every user spends their most of the time on the internet and their behavior is different from one and another. The application of data mining techniques service to need of web-based applications. Web usage mining is leading research area in Web Mining concerned about the web user's behavior. This paper gives an attention on Web usage mining to predict the behavior of web users based on web server log files. Users using web pages, a frequent access path's and frequent access pages, links are stored in web server log files. A Web log along with the individuality of the user captures their browsing behavior on a website and discussing regarding the behavior from analysis of different algorithms and different methods.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7725340/",
        "year": "2016"
    },
    {
        "title": "A survey on session detection methods in query logs and a proposal for future evaluation",
        "abstract": "Search engine logs provide a highly detailed insight of users‚Äô interactions. Hence, they are both extremely useful and sensitive. The datasets publicly available to scholars are, unfortunately, too few, too dated and too small. There are few because search engine companies are reluctant to release such data; they are dated because they were collected in late 1990s or early 2000s; and they are small because they comprise data for at most one day and just a few hundreds of thousands of users. Even worse, the large query log disclosed by AOL in 2006 caused more harm than good because of a big privacy flaw. In this paper the author provides an overall view of the possible applications of query logs, the privacy concerns researchers must face when working on such datasets, and several ways in which query logs can be easily sanitized. One of such measures consists of segmenting the logs into short topical sessions. Therefore, the author offers a comprehensive survey of session detection methods, as well as a thorough description of a new evaluation framework with performance results for each of the different methods. Additionally, a new, simple, but outperforming session detection method is proposed. It is a heuristic-based technique which works on the basis of a geometric interpretation of both the time gap between queries and the similarity between them in order to flag a topic shift.",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S002002550900053X",
        "year": "2009"
    },
    {
        "title": "A SVM-based method for abnormity detection of log curves",
        "abstract": "Rapid and accurate abnormity detection of log curves is critical in the quality control for logging industry. Traditional methods based on manual detection have been proven to be ineffective and unreliable. A machine learning method based on Support Vector Machine (SVM) is proposed in this paper to address this problem. The SVM classifiers are established according to the suspicious sections selected from log curves and the detected results given by experts. A genetic algorithm (GA) is introduced for optimization of parameters. With GA and SVM fusions, the optimal models for classifiers are determined to detect abnormity sections. Experimental results of China XiangJiang Oilfield show that an accuracy of 95% is achieved for suspicious straight sections and 96% is achieved for suspicious bouncing sections, which has proven the feasibility of this method.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5540755/",
        "year": "2010"
    },
    {
        "title": "A system for detection of internal log defects by computer analysis of axial CT images",
        "abstract": "The paper presents a system for detection of some important internal log defects via analysis of axial CT images. Two major procedures are used. The first is the segmentation of a single computer tomography (CT) image slice which extracts defect-like regions from the image slice, the second is correlation analysis of the defect-like regions across CT image slices. The segmentation algorithm for a single CT image is basically a complex form of multiple thresholding that exploits both the prior knowledge of wood structure and gray value characteristics of the image. The defect-like region extraction algorithm first locates the pith, groups the pixels in the segmented image on the basis of their connectivity and classifies each region as either a defect-like region or a defect-free region using shape, orientation and morphological features. Each defect-like region is classified as a defect or non-defect via correlation analysis across corresponding defect-like regions in neighboring CT image slices.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/572064/",
        "year": "1996"
    },
    {
        "title": "Optimizing Pattern Queries for Web Access Logs",
        "abstract": "Web access logs, usually stored in relational databases, are commonly used for various data mining and data analysis tasks. The tasks typically consist in searching the web access logs for event sequences that support a given sequential pattern. For large data volumes, this type of searching is extremely time consuming and is not well optimized by traditional indexing techniques. In this paper we present a new index structure to optimize pattern search queries on web access logs. We focus on its physical structure, maintenance and performance issues.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/3-540-44803-9_12",
        "year": "2001"
    },
    {
        "title": "Optimizing windows 10 logging to detect network security threats",
        "abstract": "The collection and analysis of event logs allows detection and debugging of operating system and application configuration errors. An appropriate selection of event logs allows you to detect cyber-attacks and prevent potential damage. In the article, we focused on the selection and optimization of event logs for the Microsoft Windows operating system. We have experimentally verified the structure and amount of produced logs and we proposed their optimization.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/8109438/",
        "year": "2017"
    },
    {
        "title": "Organizational Modeling from Event logs",
        "abstract": "Many process mining approaches have been published, which would be helpful to process modeling, but little attempt has been made for role mining used for organizational modeling. In this paper, a method of role mining using event logs as input is introduced. We adopt cosine metrics as the similarity measurement to deduce flat organizational model by clustering people base on their similar skills. In contrast to traditional techniques (e.g., questionnaire), our approach is objective and efficient. This paper also points out some challenges for future research.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4293845/",
        "year": "2007"
    },
    {
        "title": "Organizational Structure Mining Based on Workflow Logs",
        "abstract": "In order to mining the organizational setting and interactions among coworkers from workflow logs, we define metrics to establish the relationships among originators by analyzing the information in the workflow logs. Three methods for mining organizational structure from workflow logs are presented in this paper, default mining, mining based on the similarity of activities and mining based on the similarity of cases. By applying these methods, we can construct the corresponding organizational network which reflects the organizational entities involved in the workflow process and corrects presented the organizational structure. In the end of the paper, we give an example to explain it.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5208847/",
        "year": "2009"
    },
    {
        "title": "Original and Normalized Web Log Metrics as Functions of Controllable Variables of Log Study",
        "abstract": "Different studies of Web search engine query logs calculate the same metrics using logs sampled during different periods and processed under different values of two controllable variables: a client discriminator used to exclude clients who are agents and a temporal cutoff used to segment client transactions into sessions. These peculiar to the Web log analysis variables markedly affect the resulting metrics, and metrics calculated under different values of the variables may not be compared directly. To extract a contribution of the controllable variables we introduce metrics normalized by their values corresponding to the unit values of the variables. Whilst differences between values of the original metric may be enormous among different logs, normalized metrics are relatively similar functions of the controllable variables. As a result, one can use available logs as a learning collection to determine these functions and to use them to approximate metrics values for unavailable logs by reported values of metrics and controllable variables. The Yandex (2005, 2007) and Excite (2001) logs were used as a learning collection to estimate log-free normalized metrics as functions of the controllable variables. The revealed functions were applied to previously reported AltaVista (1998, 2002) results to extrapolate values reported for one combination of controllable variables onto all possible combinations of variables.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4383153/",
        "year": "2007"
    },
    {
        "title": "Overview of iCLEF 2008: Search Log Analysis for Multilingual Image Retrieval",
        "abstract": "This paper summarises activities from the iCLEF 2008 task. In an attempt to encourage greater participation in user-orientated experiments, a new task was organised based on users participating in an interactive cross-language image search experiment. Organizers provided a default multilingual search system which accessed images from Flickr, with the whole iCLEF experiment run as an online game. Interaction by users with the system was recorded in log files which were shared with participants for further analyses, and provide a future resource for studying various effects on user-orientated cross-language search. In total six groups participated in iCLEF, providing a combined effort in generating results for a shared experiment on user-orientated cross-language retrieval.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-04447-2_29",
        "year": "2009"
    },
    {
        "title": "Packet logging mechanism for adaptive online fault detection on Network-on-Chip",
        "abstract": "The shrinking size of transistors and smaller interconnect elements contribute to higher probability of on-chip faults. In order to sustain the functionality of a system in the presence of faults, fault tolerance becomes one of the key feature in Network-on-Chip (NoC) design methodology. Existing end-to-end (E2E) error detection and correction (EDC) performs well at low error rate whereas switch-to-switch (S2S) EDC performs better at high error rate. Nonetheless, choosing between both techniques is required with changing fault occurrence probability. This paper proposes an adaptive online fault detection based on packet logging mechanism. In this proposed mechanism, each router logs transmitted packets and NACK packets as well as monitors its fault level continuously. Then, the router will determine either to use E2E or S2S EDC based on error probability. Based on experimental results, our proposed adaptive method switches between E2E or S2S relative to error probability performs better than only E2E or S2S.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6865496/",
        "year": "2014"
    },
    {
        "title": "Parameter logging \u0026 remote monitoring for microgrid demonstration project",
        "abstract": "As an element of research initiative to aid the prospective smart grid issues and to integrate green energy facilities lucratively a demonstration project ‚ÄúDesign \u0026 Development of Smart Micro-Grid‚Äù is being executed in Energy Center, Maulana Azad National Institute of Technology (MANIT), Bhopal. In view of the fact that most renewable sources are intermittent in nature, their integration into the power grid infrastructure is a challenging task which is envisaged to be effectively carried through micro grids. Real time Data Acquisition from renewable energy generators and their monitoring play an extremely important role in overall supervision and the desired coordinated operation control of such diverse energy resources. The main idea of this paper is to present a continuous remote monitoring system for renewable energy sources which is one amongst the significant footsteps taken for accomplishment of the microgrid Research \u0026 Development project. It is a PC-based monitoring system able to measure remotely the real-time data of various renewable energy sources and generate reports as tables and graphs. The user may well monitor and display the data remotely by using the local area network through specific software.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7019099/",
        "year": "2014"
    },
    {
        "title": "Paranoid penguin: stealthful sniffing, intrusion detection and logging",
        "abstract": "No abstract available.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=571794",
        "year": "2002"
    },
    {
        "title": "Paranoid penguin: swatch: automated log monitoring for the vigilant but lazy",
        "abstract": "No abstract available.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=509808",
        "year": "2001"
    },
    {
        "title": "Pariket: Mining Business Process Logs for Root Cause Analysis of Anomalous Incidents",
        "abstract": "Process mining consists of extracting knowledge and actionable information from event-logs recorded by Process Aware Information Systems (PAIS). PAIS are vulnerable to system failures, malfunctions, fraudulent and undesirable executions resulting in anomalous trails and traces. The flexibility in PAIS resulting in large number of trace variants and the large volume of event-logs makes it challenging to identify anomalous executions and determining their root causes. We propose a framework and a multi-step process to identify root causes of anomalous traces in business process logs. We first transform the event-log into a sequential dataset and apply Window-based and Markovian techniques to identify anomalies. We then integrate the basic event-log data consisting of the Case ID, time-stamp and activity with the contextual data and prepare a dataset consisting of two classes (anomalous and normal). We apply Machine Learning techniques such as decision tree classifiers to extract rules (explaining the root causes) describing anomalous transactions. We use advanced visualization techniques such as parallel plots to present the data in a format making it easy for a process analyst to identify the characteristics of anomalous executions. We conduct a triangulation study to gather multiple evidences to validate the effectiveness and accuracy of our approach.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-16313-0_19",
        "year": "2015"
    },
    {
        "title": "Particle size analysis utilizing grouped data and the log-normal distribution",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0021850271900541",
        "year": "1971"
    },
    {
        "title": "Path-Colored Flow Diagrams: Increasing Business Process Insights by Visualizing Event Logs",
        "abstract": "Event logs of a self-care troubleshooting portal indicate that most customers do not follow the directions up¬†to a conclusive end. Consequently, customers risk losing confidence in the support channel, which undermines the competitive strength of the business. We present a method for visual analysis of the event logs that employs graph reduction, and the use of path classification to create a novel type of flow diagram. These diagrams help to discover and communicate new insights, such as important trends about the way the customer traverses through the underlying business process.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-65000-5_6",
        "year": "2017"
    },
    {
        "title": "Pattern and Policy Driven Log Analysis for Software Monitoring",
        "abstract": "The component-based nature of large industrial software systems that consist of a number of diverse collaborating applications, pose significant challenges with respect to system maintenance, monitoring, auditing, and diagnosing. In this context, a monitoring and diagnostic system interprets log data to recognize patterns of significant events that conform to specific threat models. Threat models have been used by the software industry for analyzing and documenting a systempsilas risks in order to understand a systempsilas threat profile. In this paper, we propose a framework whereby patterns of significant events are represented as expressions of a specialized monitoring language that are used to annotate specific threat models. An approximate matching technique that is based on the Viterbi algorithm is then used to identify whether system generated events, fit the given patterns. The technique has been applied and evaluated considering threat models and monitoring policies in logs that have been obtained from multi-user MS-Windows based systems.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/4591541/",
        "year": "2008"
    },
    {
        "title": "Pattern classification of EEG signals using a log-linearized Gaussian mixture neural network",
        "abstract": "In this paper, we propose a pattern classification method of EEG (electroencephalogram) signals measured by a simple and handy electroencephalograph to evaluate possibility of the EEG signals as a human interface tool. Subjects are asked to switch their eye states or exposed a flash light turning on and off alternatively according to pseudo-random series for 450 seconds. The EEG signals are measured during experiments and used for classification. Each EEG signal may have different distribution depending on two states of the stimulation such as eye opening/closing and presence/absence of the flash light. Therefore a log-linearized Gaussian mixture neural network incorporated a statistical model is used. It is shown from the experiments that the EEG signals can be classified sufficiently and classification rates change depending on the number of training data and the dimension of feature vectors.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/487751/",
        "year": "1995"
    },
    {
        "title": "Patterns for a Log-Based Strengthening of Declarative Compliance Models",
        "abstract": "LTL-based declarative process models are very effective when modelling loosely structured processes or working in environments with a lot of variability. A process model is represented by a set of constraints that must be satisfied during the process execution. An important application of such models is compliance checking: a process model defines then the boundaries in which a system/organisation may work, and the actual behaviour of the system, recorded in an event log, can be checked on its compliance to the given model.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-30729-4_23",
        "year": "2012"
    },
    {
        "title": "Performance analysis of all-optical amplify and forward relaying over log-normal FSO channels",
        "abstract": "In this paper, we propose a comprehensive analysis of all-optical relaying free space optical (FSO) systems in the presence of all main noise sources, including background, thermal, and amplified spontaneous emission noise, by considering the effect of the optical degree-of-freedom. Using full channel-state information (CSI) and semi-blind CSI relaying, we derive closed-form expressions for the ergodic capacity, outage capacity, and outage probability of the considered dual-hop FSO system. Numerical and analytical simulation results are provided to verify the accuracy of the proposed mathematical analysis. To simplify analytical expressions of full CSI relaying, we also propose and analyze the validity of different commonly used approximations in the context of dual-hop FSO communications.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8293999/",
        "year": "2018"
    },
    {
        "title": "Performance analysis of Dual-hop Multi-relaying System over Rayleigh plus Log-normal Fading Channel",
        "abstract": "Up-to-date the log-normal fading effect is still not fully studied for the relaying system because its underlying statistics generally does not lead to a closed-form expression. In this paper, we proposed an analytical frame work for the performance analysis of dual-hop multi-relaying over indoor Rayleigh plus log-normal (composite) fading channel, where the selected relaying link has end-to-end power constraint. Thus, we first derive a closed-form expression of outage probability under conditioned constraint that relaying end-to-end power has much larger signal-to-noise (SNR) level ( 1 chi ) relative to the targeted threshold (chiT ) ;chi \u003e\u003e chi T 1 , then investigate the bit-error-rate (BER) performance, with a variety of shadowing standard deviations. The closed-form expressions allow rapid calculation of system performances in terms of the shadowing standard deviation, relative end-to-end relaying SNR and a number of relay nodes. This was not possible previously. Numerical analyses, in comparison with the asymptotic results from accurate solutions with non-closed form, demonstrated that our proposed scheme provides a simple and efficient approach to evaluate the impact of the shadowing effect and thus analytical optimization and relaying output power margin becomes feasible in real system design applications.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6582754/",
        "year": "2013"
    },
    {
        "title": "Performance Analysis of Hadoop-Based SQL and NoSQL for Processing Log Data",
        "abstract": "Recently, many companies and research organizations are seeking scalable solutions by using Hadoop ecosystems. The log data management with large-scale and real-time properties is one of the appropriate application on top of Hadoop. In this paper, we focus on SQL and NoSQL choices for building Hadoop-based log data management system. For this purpose, we first select major products supporting SQL and NoSQL, and we then present an appropriate scheme for each product by considering its own characteristics. All the schema are for real-time monitoring and analyzing the log data. For each product, we implement insertion and selection operations of log data in Hadoop, and we analyze the performance of these operation. Analysis results show that MariaDB and MongoDB are fast in the insertion, and PostgreSQL and HBase are fast in the selection. We believe that our evaluation results will be very helpful for users to choose Hadoop SQL and NoSQL products for handling large-scale and real-time log data.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-22324-7_30",
        "year": "2015"
    },
    {
        "title": "Performance enhancement of LOG MAP Turbo Decoder for mobile applications",
        "abstract": "In mobile communication, transmission should be with least delay meeting the performance requirements. Turbo code has proven its effectiveness in the field of wireless communication system. The purpose of this paper is to implement the LOG MAP Turbo Decoder in VHDL. As it helps to achieve the highest coding gain therefore It is said, the best coding scheme for the error correction in high-speed wireless communication systems. However, the implementation of various Turbo Decoders suffers from a large propagation delay and high power consumption. For this reason, they are not suitable for Mobile communication applications. Hence, by implementing a Log Map Turbo Decoder in mobile handsets would make the processing fast as well as increase the quality of signal and accuracy of our whole system. It can be created by a serial connection of a turbo encoder and a decoder. The purpose of this paper is to implement the LOG MAP Turbo Decoder in VHDL with minimized delay.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7281406/",
        "year": "2015"
    },
    {
        "title": "Performance enhancement of modified log MAP decoding algorithm for turbo codes",
        "abstract": "Turbo decoder uses any one of the decoding algorithm, Maximum A posteriori Probability (MAP), or Soft Output Viterbi Algorithm (SOVA) because it produces error correction near to Shannon's limit. The Log MAP is a Soft Input Soft Output (SISO) algorithm, which determines the log likelihood of each transmitted data bit. A simple but effective technique to improve the performance of Log MAP algorithm is to scale the extrinsic information exchanged between two decoders. Modified Log MAP (MMAP) algorithm is achieved by fixing an arbitrary value for inner decoder (S 2 ) and an optimized value for the outer decoder (S 1 ). In Enhanced Log MAP (EMAP), both S 1 and S 2 are optimized. This paper presents the performance enhancement for the modified Log MAP decoding algorithm by optimizing the scaling factors S 1 , S 2 and Eb/No to achieve low bit error rate (BER). A comprehensive analysis of the selection of scaling factors according to channel conditions and decoding iterations are presented. The performance of various scaling factors is compared and optimized scaling factor is obtained. Choosing an empirical scaling factor for all Eb/No is compared with the best scaling factor selection for changing channel conditions and iterations. The use of an emphatically determined optimal scaling factor improved the performance of decoding algorithms in terms of BER. A typical BER improvement is in the order of 10 -2 for Additive White Gaussian Noise Channel (AWGN). Appropriate mathematical relationship between scaling factor and Eb/No is also obtained.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5738759/",
        "year": "2010"
    },
    {
        "title": "Performance evaluation of a Turbo Codec with Log-MAP algorithm on FPGA and CPU",
        "abstract": "It has been demonstrated that transmission systems using Turbo codes can achieve performance close to the Shannon limit. Although these codes were introduced two decades ago, their hardware implementations are still challenging because of the complexity of their equations. This paper presents a Field Programmable Gate Array-based implementation of a Turbo Codec system. This system includes an encoder and a decoder based on the Log-MAP algorithm. The implementation is simulated using ModelSim software and the results are compared to the results of the software-based implementation. These results show that the FPGA-based Turbo Codec is able to estimate correctly the information sequence; therefore its BER performance is comparable to the software-based implementation. However, the results of the comparison also show that the FPGA-based system is faster and consumes less energy that the software-based system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5978619/",
        "year": "2011"
    },
    {
        "title": "Performance Evaluation of Asymmetric Turbo Codes Using Log-MAP Decoding Technique",
        "abstract": "In this paper, the performance evaluation of an asymmetric turbo code for different codec parameters have been done. The asymmetric turbo codes performances are compared with symmetric counter part of the turbo code. With proper selection of weight configuration, the turbo code can show better results in both water fall region as well as error floor region simultaneously. So, in some cases asymmetric turbo code's performance is more nearer to Shannon limit than symmetric one. This paper includes the effect of different codec parameters (like no. of iteration, frame size, code-rate, and generator polynomial Structure \u0026 constraint length etc) on asymmetric turbo code and also explains its performance in comparison with symmetric turbo code. Although, the asymmetric turbo code of all combinations don't show better performance always. Here, we have evaluated the bit error rate (BER) performance using sub-optimal Log-MAP (Logarithmic Maximum-A-Posteriori) decoding algorithm under Additive White Noise (AWGN) channel. The simulation model was developed for AWGN case using MATLAB.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5738457/",
        "year": "2011"
    },
    {
        "title": "Performance Evaluation of Data Mining Frameworks in Hadoop Cluster Using Virtual Campus Log Files",
        "abstract": "With the fast development in Cloud computing technologies, most computing platforms and stand alone applications are being deployed in Cloud platforms and offered as a service (SaaS). Likewise, Data Mining Frameworks (DMFs) such as Weka and R, are being ported to Cloud platforms, while other frameworks properly designed for Cloud platforms are emerging such as Mahout. For existing DMFs, which were designed before Cloud computing appeared, the main issue is if porting them to Cloud platforms would bring any benefits. One the one hand, by porting them to Cloud, it is possible to offer them as Cloud service, which would alleviate the final user from the burden of installing and configuring DMFs at local computer or local networking infrastructure. On the other hand, porting DMFs to Cloud should allow to tackle mining of very large data sets, i.e. Big Data. In this work we evaluate some DMFs, including Weka and Mahout, under a Hadoop cluster and show that while there are improvements in time efficiency to a certain scale, some mining functions, which are part of DMFs, were not able to finalize for data sets of more than 20Gb, namely, mining log files of a virtual campus. The study revealed that indeed porting DMFs to Cloud might not necessarily help tackling Big Data, as such DMFs were conceived without Big Data requirements.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7312074/",
        "year": "2015"
    },
    {
        "title": "Performance Investigation of Pilot-Aided Log-Likelihood Ratios for LDPC Coded CO-OFDM",
        "abstract": "Pilot-subcarrier and pilot-tone aided log-likelihood ratios (PA-LLR, PT-LLR), proposed for LDPC coded coherent optical OFDM (CO-OFDM) system, is reviewed in this paper. The knowledge of common phase error based on pilot-subcarriers or pilot-tone is incorporated into the new PA-LLR or PT-LLR metric, which eliminates the need for prior common phase error estimation and compensation. The formulation of both metrics is presented in a unified way. The performance of both metrics, in their approximate versions, is compared against the approximate conventional LLR (AC-LLR) for different modulation formats using different LDPC codes in a back-to-back case. APA-LLR or APT-LLR outperforms AC-LLR for higher-order QAM, with smaller number of pilot-subcarriers (PSCs) or at smaller pilot-tone-to-signal power ratios (PSRs). A time-domain blind intercarrier interference (BL-ICI) mitigation algorithm is employed to improve the performance, which eliminates the error floor at large laser linewidth. Furthermore, we examine the tolerance of different LLR metrics to linear fiber impairment (chromatic dispersion) or nonlinear phase noise (self-phase modulation), in which case, our metrics still outperform the conventional one. Iterative demodulation using new tentative-decision-based phasor offers almost 1-dB OSNR improvement for smaller number of PSCs. Finally, we also analytically prove that APA-LLR or APT-LLR converges to AC-LLR as the number of PSCs or PSR value increases. The optimal OSNR for calculating noise power is found to be around 10 dB for actual OSNR values beyond 10 dB in either the back-to-back case or after 13 600-ps/nm/km dispersion with channel compensation. APA-LLR and APT-LLR offer better performance than AC-LLR without any increase in complexity.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7014221/",
        "year": "2015"
    },
    {
        "title": "Performance of a modified log-likelihood detector for direct sequence spread spectrum",
        "abstract": "Computer simulation is used to show that the proposed detector has a significant improvement over the simple log-likelihood detector.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/168648/",
        "year": "1992"
    },
    {
        "title": "Performance of adaptive MQAM in cellular system Nakagami fading and log-normal shadowing",
        "abstract": "We study the performance of adaptive M-ary quadrature amplitude modulation (MQAM) in wireless cellular system with Nakagami fading and log-normal shadowing. An interference-limited system is assumed and the statistic of co-channel interference is analyzed. Our analysis shows that the instantaneous signal-to-interference ratio (SIR) in cellular system with Nakagami fading and log-normal shadowing can be approximated as having the log-normal distribution. We derive the probability density function (PDF) of the instantaneous SIR and use it to evaluate the spectrum efficiency of MQAM in cellular systems. The impacts of Nakagami fading, shadowing and the target bit error rate (BER) on average throughput and average outage probability are presented and discussed.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1260317/",
        "year": "2003"
    },
    {
        "title": "Performance of CDMA cellular system with effects of soft handover under log-normal shadow channels",
        "abstract": "Next generation wireless communication is based on a global system of fixed and wireless mobile services that are transportable across different network backbones, network service providers and network geographical boundaries. Although power control is essential for direct sequence code division multiple access (DS-CDMA) cellular systems to overcome the near-far problem, one of the most important problems for the system is handover management and its effects on the system. This paper presents an approach to investigate the effects of soft handover and perfect power control on the forward link in a DS-CDMA cellular system. Especially, the relationships between the size of the handover zone and the capacity gain are evaluated under a log-normal shadow channel. The optimization of the maximum forward capacity is very necessary for matching the maximum size of the soft handover zone to the various system characteristics.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1180632/",
        "year": "2002"
    },
    {
        "title": "Performance of Log-MAP algorithm for graph-based detections on the 2-D interference channel",
        "abstract": "The read channel of magnetic recording system is affected by two-dimensional interference channel that is a combination of inter-symbol interference and inter-track interference. Inter-track interference is one of the biggest challenges in the high density magnetic recording systems and it can degrade the performance of the channels. In this work, we compare three variations of log-MAP algorithms for graph-based detectors in two-dimensional interference channels. The simulation results show the performance of the graph-based detector with the full log-MAP and extended log-MAP algorithms achieve the gains of about 0.1 and 0.05 dB at BER = 10 -5 , over the max approximation on bit patterned media recording system with the areal density 2.5 Tbits/in 2 .",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6804075/",
        "year": "2014"
    },
    {
        "title": "Performance of relay-assisted free-space optical CDMA systems over log-normal atmospheric turbulence channels",
        "abstract": "In this paper, we propose to use relay transmission as a powerful technique for performance improvement of free-space optical code-division multiple-access (FSO/CDMA) systems. Relay nodes are based on chip detect-and-forward to avoid decoding process that makes the relay nodes more complex. We formulate a closed-form expression for bit-error rate (BER) of proposed FSO/CDMA systems over log-normal atmospheric turbulence channels taking into account channel loss due to attenuation and beam divergence. Multiple-access interference and background noise are also considered in our analysis. The numerical results show that the relay transmission is an efficient solution to improve the system performance. Thanks to this solution, FSO systems can achieve low BER, long distance, and high data rate. In addition, the advantages of using two-dimensional (2-D) prime code over 1-D one are also investigated in this paper.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6923849/",
        "year": "2014"
    },
    {
        "title": "Performance optimisation for bit-interleaved coded modulation with iterative demapping with max-log- maximum a posterior detection",
        "abstract": "Max-log-maximum a posterior (MAP) detection is preferred in practical systems rather than log-MAP because of its lower complexity, but also suffers from a considerable performance loss. In this study, the authors focus on the performance optimisation for (doped) bit-interleaved coded modulation with iterative demapping schemes where max-log-MAP detection is employed. First, they study the effects of max-log-MAP detection to the extrinsic information transfer curves of the (doped) demapper and decoder, and reselect the constellation labelling. Second, they find that the small difference between max-log-MAP and log-MAP detection in each iteration would be accumulated during the whole iterative procedure referred to as error accumulation, which causes a large performance loss, and consequently they propose some methods to control the error accumulation. Owing to the labelling reselection and error-accumulation control, the proposed scheme exhibits several tenths to 1 dB gains, compared with the conventional schemes, while maintaining low complexity.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7272345/",
        "year": "2015"
    },
    {
        "title": "Performance study of OFDM receiver using FFT based on log number system",
        "abstract": "In this paper, we study the performance of the OFDM receiver using FFT based on the log number system (Log-FFT) in a wireless environment. By using the log number system (LNS), complex multiplications of the FFT are reduced to simple additions. The effect of finite bit precision on the receiver performance is analyzed theoretically under the AWGN channel. Simulation results for both AWGN and fading channels are presented. It is shown that the bit width requirement of the receiver is quite small. In a coded OFDM system, there is no degradation in bit error rate performance when only two fractional bits are used for the LNS under a slow fading channel. The size of the look-up tables are small and the complexity of the log-FFT implementation is simple. Thus log-FFT is an attractive implementation method for the OFDM receiver design in wireless environment.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1002816/",
        "year": "2002"
    },
    {
        "title": "Performing Web Log Analysis and Predicting Intelligent Navigation Behavior Based on Student Accessing Distance Education System",
        "abstract": "We have introduced a concept of capturing different web log file, while the user is accessing the Distance Education System website and provide the user with the intelligent navigation behavior on his browser. Web log file is saved in text (.txt) format with ‚Äúcomma‚Äù separated attributes. Since the Log files consist of irrelevant and inconsistent access information, therefore there was a need to perform Web log preprocessing which includes different techniques such as field extraction, data cleaning, and data summarization. Preprocessed information is been given to intelligent navigation module and it allows the student to have most frequently viewed subject at the top of the list, which allows them to have easy access to the tutorial or chapter within the subject. The analysis allows enhancing the personalization services in distance education system and making the system much effective.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-36321-4_7",
        "year": "2013"
    },
    {
        "title": "Permeability and alteration within the Soultz granite inferred from geophysical and flow log analysis",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0375650506000629",
        "year": "2006"
    },
    {
        "title": "Personalized life log media system in ubiquitous environment",
        "abstract": "In this paper, we propose new system for storing and retrieval of personal life log media on ubiquitous environment. We can gather personal life log media from intelligent gadgets which are connected with wireless network. Our intelligent gadgets consist of wearable gadgets and environment gadgets. Wearable gadgets include audiovisual device, GPS, 3D-accelerometer and physiological reaction sensors. Environment gadgets include the smart sensors attached to the daily supplies, such as cup, chair, door and so on. User can get multimedia stream with wearable intelligent gadget and also get the environmental information around him from environment gadgets as personal life log media. These life log media(LLM) can be logged on the LLM server in realtime. In LLM server, learning-based activity analysis engine will process logged data and create meta data for retrieval automatically. By using proposed system, user can log with personalized life log media and can retrieve the media at any time. To give more intuitive retrieval, we provide intuitive spatiotemporal graphical user interface in client part. Finally we can provide user-centered service with individual activity registration and classification for each user with our proposed system.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-540-71789-8_3",
        "year": "2007"
    },
    {
        "title": "Personalized mining of preferred paths based on web log",
        "abstract": "With the development of the Internet, web service generates a large amount of log information, how to mine user preferred browsing paths from web log information is an important research area. Current researches mainly focus on the mining of user preferred browsing paths, however, they do not delve into the personalization of preferred paths and paths lack semantic information. To provide personalized preferred paths to fulfill users need, this paper proposes a innovative method of user preferred browsing path analysis based on vector space model. Firstly, path eigenvectors are adopted to denote the obtained preferred paths, and field eigenvectors are given by field experts. Secondly, the cosine similarity of path eigenvectors and field eigenvectors are computed. Thirdly, the preferred paths are partitioned into clusters according to the cosine similarity. Finally, the clusters are annotated using related fields. After clustering and annotation, the website can automatically recommend the related preferred paths for users according to the choice of users. Experiments show that it is accurate and scalable. It can be applied to optimize website or design personalized service.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6743199/",
        "year": "2013"
    },
    {
        "title": "Personalized Recommendation System Based on Web Log Mining and Weighted Bipartite Graph",
        "abstract": "Recently, recommendation systems based on bipartite graph algorithm have been widely applied to many areas including E-business, but the weight of edge is ignored. Therefore, the commodity with high rating has not got the priority to be recommended. In order to solve the problem, we propose a personalized recommendation system based on user's interest. The results of web log mining are introduced to weighted bipartite graph, greatly improving the practicability of the recommendation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6643076/",
        "year": "2013"
    },
    {
        "title": "Petri Net-Based Episode Detection and Story Generation from Ubiquitous Life Log",
        "abstract": "As mobile devices improve their performance in ubiquitous spaces, they have a variety of applications with more information. Active exploration follows for collecting and utilizing the information accumulated in ubiquitous environment. If user information in mobile devices can be summarized in a more intuitive and interesting form, like cartoon, it can help to share his experience with other people, and recall his meaningful memory. In this paper, we propose a method that organizes a story and generates cartoons using the information collected in ubiquitous environment. We generate cartoons with Petri net-based method, which describes causal relationship between experienced events and the precondition of events. Mobile information used in the experiment is collected from two female undergraduate students for two weeks. We confirm the potential of the proposed method for ambient intelligence through the analysis of the results.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-69293-5_42",
        "year": "2008"
    },
    {
        "title": "Pilot-aided log-likelihood ratio for LDPC coded M-QAM CO-OFDM system",
        "abstract": "Pilot-aided log-likelihood ratio as well as its approximation are derived for LDPC coded M-QAM CO-OFDM system with consideration of laser phase noise. Our metric performs better than the conventional metric in 16QAM and 64QAM simulation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6887151/",
        "year": "2014"
    },
    {
        "title": "Pilot-Aided Log-Likelihood Ratio for LDPC Coded MPSK-OFDM Transmission",
        "abstract": "The pilot-aided log-likelihood ratio (LLR) is derived from low-density parity-check coded coherent optical M-ary phase-shift keying orthogonal frequency division multiplexing transmission system with laser phase noise. Approximations of the metrics, which yield lower computational cost and comparable performance, are also introduced. The bit error ratio performance is tested through simulation for the proposed LLR metrics.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6472045/",
        "year": "2013"
    },
    {
        "title": "Pilot-Tone Assisted Log-Likelihood Ratio for LDPC Coded CO-OFDM System",
        "abstract": "Pilot-tone assisted log-likelihood ratio (PT-LLR) is derived for low-density parity-check coded, coherent optical orthogonal frequency division multiplexing systems in the presence of linear phase noise. The knowledge of common phase error (CPE) obtained from the pilot-tone is incorporated into the new LLR metric, which eliminates the need for prior CPE estimation and compensation. We compare our metric with the conventional LLR (C-LLR) through extensive simulation using their approximate versions (APT-LLR, AC-LLR). APT-LLR has the same order of complexity as AC-LLR while it outperforms AC-LLR for higher order modulation formats (16-QAM, 64-QAM) at smaller pilot-tone-to-signal power ratios. In addition, we show that with the help of time-domain blind intercarrier interference mitigation, both metrics perform better in the presence of larger laser linewidth.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6826532/",
        "year": "2014"
    },
    {
        "title": "Pith detection on CT-Cross-Section images of logs: An experimental comparison",
        "abstract": "We propose and compare different image processing approaches for an automatic pith detection using CT-cross-section images of logs. Our techniques apply different image preprocessing approaches as well as different actual pith detection methods. Besides acquiring knowledge about the anatomical centre of a log, determination of pith position is often a preliminary step for further automated detection of wood characteristics, like annual ring features or compression wood areas.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4537273/",
        "year": "2008"
    },
    {
        "title": "PLEDS: A Personalized Entity Detection System Based on Web Log Mining Techniques",
        "abstract": "With the expansion of the internet, many specialized, high-profile sites have become available that bring very technical subject matter to readers with non-technical backgrounds. While the theme of these sites may be of interest to these readers, the posts themselves may contain terms that non-experts may be unfamiliar with and may wish to know more about. We developed PLEDS, a personalized entity detection system which identifies interesting entities and provides related information for individual users by mining web logs and query logs. The experimental results of a systemic user study shows that with PLEDS's aid, users can experience the benefits of an enriched internet surfing experience.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4597039/",
        "year": "2008"
    },
    {
        "title": "Pointing Error Effects on Performance of Amplify-and-Forward Relaying MIMO/FSO Systems Using SC-QAM Signals Over Log-Normal Atmospheric Turbulence Channels",
        "abstract": "In this work, we analyze pointing error effects on performance of Amplify-and-Forward (AF) relaying multiple-input multiple-output (MIMO) free space optical (FSO) communication system employing subcarrier quadrature amplitude modulation (SC-QAM) signal over log-normal distributed atmospheric turbulence channels. We study the pointing error effect by taking into account the influence of beam-width, aperture size and jitter variance on the average symbol error rate (ASER), which is derived in closed-form expressions of MIMO/FSO and SISO/FSO systems. In addition, the number of relaying stations is taken into account in the statistical model of the combined channel including atmospheric loss, atmospheric turbulence and pointing error. The numerical results show that by combining AF relaying stations and MIMO/FSO configurations, the link length can be extended due to the transmitted power is reduced accordingly to the amplifier gain. Moreover, performance of AF relaying MIMO/FSO systems is better than that of AF relaying SISO/FSO systems at the same link length.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-49390-8_59",
        "year": "2016"
    },
    {
        "title": "Polymicrobial sepsis: an analysis of 184 cases using log linear models",
        "abstract": "Polymicrobial sepsis is a common and frequently fatal clinical condition that has received relatively little attention in published reports. Retrospectively. we reviewed the case records of 184 patients with polymicrobial sepsis seen at three Dallas hospitals between 1972 and 1977. Analysis of clinical data using log linear models enabled us to identify significant positive correlations (p\u003cO.05) between mortality resulting from polymicrobial sepsis and underlying disease category. failure to manifest fever. a pulmonary portal of entry. hypotension. and hospitalassociated sepsis. No significant correlation with outcome could be demonstrated for age. hospital service. species of infecting microorganisms. number of microorganisms isolated from blood. WBC count. or antimicrobial therapy. In spite of indirect evidence for synergistic relationships between microorganisms responsible for polymicrobial sepsis in man. we could not resolve whether antimicrobial regimens that are effective against all of the microorganisms participating in polymicrobial infections are required to insure a favorable outcome.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S000296291537587X",
        "year": "1980"
    },
    {
        "title": "Portable data logging system for long-term continuous monitoring of biomedical signals",
        "abstract": "The described portable data-logging system is intended for continuous and long-term monitoring of the heart rate variability (HRV). For this purpose, the system enables the acquisition of the following signals: RR interval calculated from the electrocardiogram (ECG), body and environment temperature as well as the state of the event marker (EM) button. The attained small dimensions (90 /spl times/ 50 /spl times/ 15 mm) and small weight (100 g), together with the leather bag and the fastening belts enable the portable device to be worn comfortably during the measurement time. The user-friendly PC software enables the management of all required pieces of information about subjects and measured signal data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1346893/",
        "year": "2004"
    },
    {
        "title": "Portable system to monitor vital signs with a Java based computer display and data logging",
        "abstract": "The first step to improving public health is by the assessment of vital signs, which are valuable indicators of overall health. In the present day there are a number of complex, albeit expensive instruments that can measure various parameters accurately. However, these instruments often tend to be bulky, expensive and difficult to operate. Hence it is difficult to use these instruments in isolated locations such as rural or disaster stricken areas, for bedside monitoring or even in military camps. The objective of this paper is to introduce the concept of a user friendly modular system which can be used to monitor various physiological parameters efficiently. In the proposed system different sensing circuits are attached to a common MicroController Unit (MCU) which can be updated with the associated firmware. Any general computing system such as a laptop is connected to the MCU via a USB-UART bridge. After the connection is established the controller starts sending data to the computer. A JavaScript is run on the laptop using open-source software called Processing 2.0, which initiates a customized display of the various parameters using the incoming data. For data logging we store the serial data in text files. As proof of principle we have measured two parameters, namely the body temperature using the LM35 sensor and the heart rate using photoplethysmography. Such a switchable system can be used as a tool to measure various parameters like oxygen saturation, blood pressure, ECG etc. on the computer screen.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7746154/",
        "year": "2016"
    },
    {
        "title": "Position coded pre-order linked WAP-tree for web log sequential pattern mining",
        "abstract": "Web access pattern tree algorithm mines web log access sequences by first storing the original web access sequence database on a prefix tree (WAP-tree). WAP-tree algorithm then mines frequent sequences from the WAP-tree by recursively re-constructing intermediate WAP-trees, starting with their suffix subsequences.This paper proposes an efficient approach for using the preorder linked WAP-trees with binary position codes assigned to each node, to mine frequent sequences, which eliminates the need to engage in numerous re-construction of intermediate WAP-trees during mining. Experiments show huge performance advantages for sequential mining using prefix linked WAP-tree technique.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1760939",
        "year": "2003"
    },
    {
        "title": "Poster abstract: BioLogger: A wireless physiological monitoring and logging system",
        "abstract": "This paper presents the design and development of BioLogger, a wireless physiological signal monitoring and logging system. BioLogger can simultaneously acquire multiple types of physiological signals and display their waveforms on a computer. Furthermore energy saving is integrated to the both hardware and software design intended to prolong the time period of the on-body sensor node. Last, a simple scheduler is implemented to make sure the emergency message would not be missed.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1602209",
        "year": "2009"
    },
    {
        "title": "Poster abstract: enabling a cloud-based logging service for ball screw with an autonomous networked sensor system",
        "abstract": "Precision ball screw assembly (hereafter called \"ball screw\"), as shown in Fig. 1, is a mechanical wear out part that widely used in CNC (computer numerical control) machine tools to control the movement of processing targets and spindles. Up until now, there has been no simple way to directly measure ball screw for knowing the state of wear quantitatively. An indirect approach is logging all the signals (vibration, temperature, and preload change) during the operation of ball screw, and to use them to construct the wear model for estimating its remaining lifetime. To achieve this goal, we proposed a cloud-based logging system in this study that emphasizes (1) logging all the signals during operation in a ball screw's whole lifetime, and transferring to the data server without data loss; and (2) saving all the data into the cloud data storage of the ball screw's manufacturer. The data collected from many ball screws can be used to analyze and construct the wear model of ball screw, allowing the manufacturer to understand the state of wear and send a warning to the tool machine's owner before excessive wear.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2461438",
        "year": "2013"
    },
    {
        "title": "POSTER: Toward Automating the Generation of Malware Analysis Reports Using the Sandbox Logs",
        "abstract": "In recent years, the number of new examples of malware has continued to increase. To create effective countermeasures, security specialists often must manually inspect vast sandbox logs produced by the dynamic analysis method. Conversely, antivirus vendors usually publish malware analysis reports on their website. Because malware analysis reports and sandbox logs do not have direct connections, when analyzing sandbox logs, security specialists can not benefit from the information described in such expert reports. To address this issue, we developed a system called ReGenerator that automates the generation of reports related to sandbox logs by making use of existing reports published by antivirus vendors. Our system combines several techniques, including the Jaccard similarity, Natural Language Processing (NLP), and Generation (NLG), to produce concise human-readable reports describing malicious behavior for security specialists.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2989064",
        "year": "2016"
    },
    {
        "title": "Power system protection-power monitoring-data logging-remote interrogation system",
        "abstract": "A power monitoring system is described which uses microprocessor technology to provide protection, real-time status displays, event logging, and power management and control for industrial AC power distribution and generation systems. The system uses an industrial-grade host personal computer to monitor various device communication systems. Each device communication system consists of a display and monitoring unit connected by a single twisted-pair EIA RS-485 communications interface to microprocessor-controlled power meters, trip units, overcurrent protective relays, and/or motor protective relays on power distribution systems. Both metering and protection functions use RMS sensing to properly account for harmonics that distort waveforms.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/66471/",
        "year": "1990"
    },
    {
        "title": "Power system protection-power monitoring, data logging, remote interrogation system",
        "abstract": "A power monitoring system is described that uses microprocessor technology to provide protection, real-time status displays, and event logging for industrial AC power distribution and generation systems. The system consists of a display and monitoring unit connected by a single twisted-pair EIA RS-485 communications interface to as many as 32 microprocessor-controlled power meters and/or trip units on low-voltage power circuit breakers. Both metering and protection functions use RMS sensing to properly account for harmonics that distort waveforms.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/37244/",
        "year": "1989"
    },
    {
        "title": "Predicting Deadline Transgressions Using Event Logs",
        "abstract": "Effective risk management is crucial for any organisation. One of its key steps is risk identification, but few tools exist to support this process. Here we present a method for the automatic discovery of a particular type of process-related risk, the danger of deadline transgressions or overruns, based on the analysis of event logs. We define a set of time-related process risk indicators, i.e., patterns observable in event logs that highlight the likelihood of an overrun, and then show how instances of these patterns can be identified automatically using statistical principles. To demonstrate its feasibility, the approach has been implemented as a plug-in module to the process mining framework ProM and tested using an event log from a Dutch financial institution.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-36285-9_22",
        "year": "2013"
    },
    {
        "title": "Predicting student's learning outcome from Learning management system logs",
        "abstract": "Teaching is complex activity which requires professors to employ the most effective and efficient teaching strategies to enable students to make progress. Main problem in teaching professors should consider different teaching approaches and learning techniques to suit every student. Today, in computer age, electronic learning (e-learning) is widely used in practice. Development of World Wide Web, especially Web2.0 has led to revolution in education. Student interaction with Learning management systems - LMS result in creating large data sets which are interesting for research. LMS systems also provide tools for following every individual student and statistical view for deeper analyzing result of student - system interaction. However, these tools do not include artificial intelligence algorithms as a support mechanism for decision. In this article we provide framework for student modeling trained on large sets of data using Hadoop and Mahout. This kind of system would provide insight into each individual student's activity. Based on that information, professors could adjust course materials according to student interest and knowledge.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7314114/",
        "year": "2015"
    },
    {
        "title": "Predicting the Political Sentiment of Web Log Posts Using Supervised Machine Learning Techniques Coupled with Feature Selection",
        "abstract": "As the number of web logs dramatically grows, readers are turning to them as an important source of information. Automatic techniques that identify the political sentiment of web log posts will help bloggers categorize and filter this exploding information source. In this paper we illustrate the effectiveness of supervised learning for sentiment classification on web log posts. We show that a Na√Øve Bayes classifier coupled with a forward feature selection technique can on average correctly predict a posting‚Äôs sentiment 89.77% of the time with a standard deviation of 3.01. It significantly outperforms Support Vector Machines at the 95% confidence level with a confidence interval of [1.5, 2.7]. The feature selection technique provides on average an 11.84% and a 12.18% increase for Na√Øve Bayes and Support Vector Machines results respectively. Previous sentiment classification research achieved an 81% accuracy using Na√Øve Bayes and 82.9% using SVMs on a movie domain corpus.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-77485-3_11",
        "year": "2007"
    },
    {
        "title": "Predicting user behavior through sessions using the web log mining",
        "abstract": "It is the method to extract the user sessions from the given log files. Initially, each user is identified according to his/her IP address specified in the log file and corresponding user sessions are extracted. Two types of logs ie., server-side logs and client-side logs are commonly used for web usage and usability analysis. Server-side logs can be automatically generated by web servers, with each entry corresponding to a user request. Client-side logs can capture accurate, comprehensive usage data for usability analysis. Usability is defined as the satisfaction, efficiency and effectiveness with which specific users can complete specific tasks in a particular environment. This process includes 3 stages, namely Data cleaning, User identification, Session identification. In this paper, we are implementing these three phases. Depending upon the frequency of users visiting each page mining is performed. By finding the session of the user we can analyze the user behavior by the time spend on a particular page.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7449167/",
        "year": "2016"
    },
    {
        "title": "Predicting visualization of hospital clinical reports using survival analysis of access logs from a virtual patient record",
        "abstract": "The amount of data currently being produced, stored and used in hospital settings is stressing information technology infrastructure, making clinical reports to be stored in secondary memory devices. The aim of this work was to develop a model that predicts the probability of visualization, within a certain period after production, of each clinical report. We collected log data, from January 2013 till May 2011, from an existing virtual patient record, in a tertiary university hospital in Porto, Portugal, with information on report creation and report first-time visualization dates, along with contextual information. The main factors associated with visualization were defined using logistic regression. These factors were then used as explanatory variables for predicting the probability of a piece of information being accessed after production, using Kaplan-Meier analysis and the Weibull probability distribution. Clinical department, type of encounter and report type were found significantly associated with time-to-visualization and probability of visualization.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6627841/",
        "year": "2013"
    },
    {
        "title": "Prediction of Web User Behavior by Discovering Temporal Relational Rules from Web Log Data",
        "abstract": "The Web has become a very popular and interactive medium in our lives. With the rapid development and proliferation of e-commerce and Web-based information systems, web mining has become an essential tool for discovering specific information on the Web. There are a lot of previous web mining techniques have been proposed. In this paper, an approach of temporal interval relational rule mining is applied to discover knowledge from web log data. Comparing our proposed approach and previous web mining techniques, the attribute of timestamp in web log data is considered in our approach. Firstly, temporal intervals of accessing web pages are formed by folding over a periodicity. And then discovery of relational rules is performed based on constraint of these temporal intervals. In the experiment, we analyze the result of relational rules and the effect of important parameters used in the mining approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-32597-7_3",
        "year": "2012"
    },
    {
        "title": "Preprocessing and mining web log data for web personalization",
        "abstract": "We describe the web usage mining activities of an on-going project, called ClickWorld, that aims at extracting models of the navigational behaviour of a web site users. The models are inferred from the access logs of a web server by means of data and web mining techniques. The extracted knowledge is deployed to the purpose of offering a personalized and proactive view of the web services to users. We first describe the preprocessing steps on access logs necessary to clean, select and prepare data for knowledge extraction. Then we show two sets of experiments: the first one tries to predict the sex of a user based on the visited web pages, and the second one tries to predict whether a user might be interested in visiting a section of the site.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-3-540-39853-0_20",
        "year": "2003"
    },
    {
        "title": "Preprocessing DNS Log Data for Effective Data Mining",
        "abstract": "The domain name service (DNS) provides a critical function in directing Internet traffic. Defending DNS servers from bandwidth attacks is assisted by the ability to effectively mine DNS log data for statistical patterns. Processing DNS log data can be classified as a data-intensive problem, and as such presents challenges unique to this class of problem. When problems occur in capturing log data, or when the DNS server experiences an outage (scheduled or unscheduled), the normal pattern of traffic for that server becomes clouded. Simple linear interpolation of the holes in the data does not preserve features such as peaks in traffic (which can occur during an attack, making them of particular interest). We demonstrate a method for estimating values for missing portions of time sensitive DNS log data. This method would be suitable for use with a variety of datasets containing time series values where certain portions are missing.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/5199359/",
        "year": "2009"
    },
    {
        "title": "Principles of Inductive Reasoning on the Semantic Web: A Framework for Learning in ${\\mathcal AL}$ -Log",
        "abstract": "The design of the logical layer of the Semantic Web, and subsequently of the mark-up language SWRL, has renewed the interest in hybrid knowledge representation and reasoning. In this paper we discuss principles of inductive reasoning for this layer. To this aim we provide a general framework for learning in {\\mathcal AL}-log, a hybrid language that integrates the description logic {\\mathcal ALC} and the function-free Horn clausal language Datalog, thus turning out to be a small yet sufficiently expressive subset of SWRL. In this framework inductive hypotheses are represented as constrained Datalog clauses, organized according to the {\\mathcal B}-subsumption relation, and evaluated against observations by applying coverage relations that depend on the representation chosen for the observations. The framework is valid whatever the scope of induction (description vs. prediction) is. Yet, for illustrative purposes, we concentrate on an instantiation of the framework which supports description.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11552222_12",
        "year": "2005"
    },
    {
        "title": "Privacy in web search query log mining",
        "abstract": "Web search engines have changed our lives - enabling instant access to information about subjects that are both deeply important to us, as well as passing whims. The search engines that provide answers to our search queries also log those queries, in order to improve their algorithms. Academic research on search queries has shown that they can provide valuable information on diverse topics including word and phrase similarity, topical seasonality and may even have potential for sociology, as well as providing a barometer of the popularity of many subjects. At the same time, individuals are rightly concerned about what the consequences of accidental leaking or deliberate sharing of this information may mean for their privacy. In this talk I will cover the applications which have benefited from mining query logs, the risks that privacy can be breached by sharing query logs, and current algorithms for mining logs in a way to prevent privacy breaches.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-04180-8_4",
        "year": "2009"
    },
    {
        "title": "Probing Experiences: Logs, Traces, Self-Report and A Sense of Wonder",
        "abstract": "Two studies are described in which logging electronic data in conjunction with self-report provided a deeper insight into user experiences. During a field trial, interaction with a ‚Äúvisual‚Äù radio was electronically logged throughout and shed light on subjects‚Äô daily diaries. In a location-based audio play, users‚Äô tracks and traces were used in conjunction with interview data. In a third study, participants wore fashionable communicating garments; Galvanic Skin Response data triggered a sense of wonder. The studies highlight the ad-hoc nature of our use of electronic data to probe experiences and the need for a more rigid theoretical framework.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4020-6593-4_5",
        "year": "2008"
    },
    {
        "title": "Process Discovery from Low-Level Event Logs",
        "abstract": "The discovery of a control-flow model for a process is here faced in a challenging scenario where each trace in the given log L_E encodes a sequence of low-level events without referring to the process‚Äô activities. To this end, we define a framework for inducing a process model that describes the process‚Äô behavior in terms of both activities and events, in order to effectively support the analysts (who typically would find more convenient to reason at the abstraction level of the activities than at that of low-level events). The proposed framework is based on modeling the generation of L_E with a suitable Hidden Markov Model (HMM), from which statistics on precedence relationships between the hidden activities that triggered the events reported in L_E are retrieved. These statistics are passed to the well-known Heuristics Miner algorithm, in order to produce a model of the process at the abstraction level of activities. The process model is eventually augmented with probabilistic information on the mapping between activities and events, encoded in the discovered HMM. The framework is formalized and experimentally validated in the case that activities are ‚Äúatomic‚Äù (i.e., an activity instance triggers a unique event), and several variants and extensions (including the case of ‚Äúcomposite‚Äù activities) are discussed.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-91563-0_16",
        "year": "2018"
    },
    {
        "title": "Process discovery in event logs: An application in the telecom industry",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1568494610001055",
        "year": "2011"
    },
    {
        "title": "Process Mining as First-Order Classification Learning on Logs with Negative Events",
        "abstract": "Process mining is the automated construction of process models from information system event logs. In this paper we identify three fundamental difficulties related to process mining: the lack of negative information, the presence of history-dependent behavior and the presence of noise. These difficulties can elegantly dealt with when process mining is represented as first-order classification learning on event logs supplemented with negative events. A first set of process discovery experiments indicates the feasibility of this learning technique.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-78238-4_6",
        "year": "2008"
    },
    {
        "title": "Process Mining Event Logs from FLOSS Data: State of the Art and Perspectives",
        "abstract": "Free/Libre Open Source Software (FLOSS) is a phenomenon that has undoubtedly triggered extensive research endeavors. At the heart of these initiatives is the ability to mine data from FLOSS repositories with the hope of revealing empirical evidence to answer existing questions on the FLOSS development process. In spite of the success produced with existing mining techniques, emerging questions about FLOSS data require alternative and more appropriate ways to explore and analyse such data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-15201-1_12",
        "year": "2015"
    },
    {
        "title": "Process Mining of Event Log from Web Information and Administration System for Management of Student‚Äôs Computer Networks",
        "abstract": "Process mining is relatively new approach which is often using for performance managing and optimizing of the most important business processes. Process mining analysis allows extracting information from event logs. The main purpose of this paper is to describe advantages of process mining, current trends and provide process mining analysis of event log from web information and administration system for management of student‚Äôs computer networks in case study. The case study deals with using Disco software tool for process mining of mentioned event log. Case study will provide information about processes such as automatic discovery of process model based on imported data, process map with detail information about activities and paths (frequency, repetitions and duration), number of events, overview about events and active cases over time and finally also using resources.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-52015-5_57",
        "year": "2017"
    },
    {
        "title": "Process Mining on Databases: Unearthing Historical Data from Redo Logs",
        "abstract": "Process Mining techniques rely on the existence of event data. However, in many cases it is far from trivial to obtain such event data. Considerable efforts may need to be spent on making IT systems record historic data at all. But even if such records are available, it may not be possible to derive an event log for the case notion one is interested in, i.e., correlating events to form process instances may be challenging. This paper proposes an approach that exploits a commonly available and versatile source of data, i.e. database redo logs. Such logs record the writing operations performed in a general-purpose database for a range of objects, which constitute a collection of events. By using the relations between objects as specified in the associated data model, it is possible to turn such events into an event log for a wide range of case types. The resulting logs can be analyzed using existing process mining techniques.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-23063-4_25",
        "year": "2015"
    },
    {
        "title": "Process Mining Reloaded: Event Structures as a Unified Representation of Process Models and Event Logs",
        "abstract": "Process mining is a family of methods to analyze event logs produced during the execution of business processes in order to extract insights regarding their performance and conformance with respect to normative or expected behavior. The landscape of process mining methods and use cases has expanded considerably in the past decade. However, the field has evolved in a rather ad hoc manner without a unifying foundational theory that would allow algorithms and theoretical results developed for one process mining problem to be reused when addressing other related problems. In this paper we advocate a foundational approach to process mining based on a well-known model of concurrency, namely event structures. We outline how event structures can serve as a unified representation of behavior captured in process models and behavior captured in event logs. We then sketch how process mining operations, specifically automated process discovery, conformance checking and deviance mining, can be recast as operations on event structures.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-19488-2_2",
        "year": "2015"
    },
    {
        "title": "Process mining: Discovering direct successors in process logs",
        "abstract": "Workflow management technology requires the existence of explicit process models, i.e. a completely specified workflow design needs to be developed in order to enact a given workflow process. Such a workflow design is time consuming and often subjective and incomplete. We propose a learning method that uses the workflow log, which contains information about the process as it is actually being executed. In our method we will use a logistic regression model to discover the direct connections between events of a realistic not complete workflow log with noise. Experimental results are used to show the usefulness and limitations of the presented method.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/3-540-36182-0_37",
        "year": "2002"
    },
    {
        "title": "Process Mining: Extending Œ±-Algorithm to Mine Duplicate Tasks in Process Logs",
        "abstract": "Process mining is a new technology which can distill workflow models from a set of real executions. However, the present research in process mining still meets many challenges. The problem of duplicate tasks is one of them, which refers to the situation that the same task can appear multiple times in one workflow model. The ‚ÄúŒ±-algorithm‚Äù is proved to mine sound Structured Workflow nets without task duplication. In this paper, basing on the ‚ÄúŒ±-algorithm‚Äù, a new algorithm (the ‚ÄúŒ±*-algorithm‚Äù) is presented to deal with duplicate tasks and has been implemented in a research prototype. In eight scenarios, the ‚ÄúŒ±*-algorithm‚Äù is evaluated experimentally to show its validity.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-72909-9_43",
        "year": "2007"
    },
    {
        "title": "Process Model Matching Using Event-Log Information",
        "abstract": "Alignments between processes provide an important basis for a variety of application scenarios and techniques. These alignments are, among others, used for the detection of differences between models¬†[148], the harmonization of process model variants¬†[149], process querying¬†[123], and the propagation of process changes¬†[265]. The accuracy and, therefore, usefulness of such techniques is highly dependent on the correctness and completeness of the alignments that are established by process model matching techniques. However, despite the existence of a plethora of matching techniques, it has been shown that their results leave room for improvement¬†[54]. A possible cause for this is that existing process model matching techniques focus exclusively on information related to the specification of processes, typically by just considering the information contained in process models themselves. Therefore, they ignore information that relates to the actual execution of the processes, as captured in event logs. These logs provide valuable information on data attributes, event durations, and other aspects specifically associated with the enactment of a process.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-94634-4_7",
        "year": "2018"
    },
    {
        "title": "Process Modeling Leveraged by Workflow Structure and Running Logs Analysis",
        "abstract": "The reality of big data opens up a new world for business process modeling. Omnipresent cases and workflow logs are getting accessible, which implies the chance to exploit important patterns hidden in them so as to cut down the modeling cost or to improve the quality of process models. To take the full advantage of big data in process-aware systems (PASs), we propose a novel business process modeling technique that leverages the modeling by cases and workflow logs analysis. It uses the average perceptron to analyze both of existing process structure of cases and co-occurrence relation of activities in workflow logs. In contrast to traditional manual efforts, it improves the performance significantly by recommending proved working patterns. Comparing to recent process mining strategies, it serves the modeling online with meaningful process segments. We evaluate our approach against a synthesis dataset (100 processes and 10,000 log items generated by the plugin PLG in ProM) and real data from public business processes (77 processes in the package Paul Fisher workflows for benchmarks PR and CA2 from the website myExperiment). The study reveals that 9.46¬†% improvement in precision can be gained by considering both case structure and log items in contrast to the structure only, or 5.94¬†% gaining in contrast to mere logs. Our evaluation validates the effectiveness of the proposed technique and efficiency when we applying it on real modeling scenarios.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-981-10-1019-4_3",
        "year": "2016"
    },
    {
        "title": "Profile Mining in CVS-Logs and Face-to-Face Contacts for Recommending Software Developers",
        "abstract": "In order to support a software development team in its day-to-day operations, different data sources can be exploited. In this paper, we focus on CVS logs and communication profiles between developers provided by RFID-proximity information. We provide a novel approach for combining the data sources into a graph, and apply the page rank algorithm for capturing interesting knowledge about resource and developer profiles. Additionally, we discuss the application in the software developer setting, and also for project management. The proposed approach is evaluated in the context of a real-world developer setting.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6113122/",
        "year": "2011"
    },
    {
        "title": "Profiling distributed systems in lightweight virtualized environments with logs and resource metrics",
        "abstract": "Understanding and troubleshooting distributed systems in the cloud is considered a very difficult problem because the execution of a single user request is distributed to multiple machines. Further, the multi-tenancy nature of cloud environments further introduces interference that causes performance issues. Most existing troubleshooting tools either focus on log analysis or intrusive tracing methods, leaving resource usage monitoring unexplored.We propose and implement LRTrace, a non-intrusive tracing and feedback control tool for distributed applications in lightweight virtualized environments. LRTrace profiles both log messages and actual resource consumptions of an application at runtime in a fine-grained manner, which is made possible by lightweight container-based virtualization. By correlating these two kinds of information, LRTrace provides users the ability to build the relationship between changes in resource consumption and application events. Furthermore, LRTrace allows users to define and implement their own feedback control plug-ins to manage the cluster in a semi-automatic manner. In system evaluation, we run Spark and MapReduce applications in a multi-tenant cluster and show that LRTrace can diagnose performance issues caused by either interference or bugs, or both. It also helps users to understand the workflows of data-parallel applications.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3208044",
        "year": "2018"
    },
    {
        "title": "Profiling Event Logs to Configure Risk Indicators for Process Delays",
        "abstract": "Risk identification is one of the most challenging stages in the risk management process. Conventional risk management approaches provide little guidance and companies often rely on the knowledge of experts for risk identification. In this paper we demonstrate how risk indicators can be used to predict process delays via a method for configuring so-called Process Risk Indicators (PRIs). The method learns suitable configurations from past process behaviour recorded in event logs. To validate the approach we have implemented it as a plug-in of the ProM process mining framework and have conducted experiments using various data sets from a major insurance company.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-38709-8_30",
        "year": "2013"
    },
    {
        "title": "Property Driven Mining in Workflow Logs",
        "abstract": "We present a language for property specification for workflows and a tool for property checks. The language is based on the Propositional Linear Temporal Logic and the structure of workflow logs. These language and tool help companies to diagnose business processes, react to changes in business environment and collect formal definitions of business properties. We give examples of specifications of business properties that set relations between events of business processes",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-32392-9_55",
        "year": "2005"
    },
    {
        "title": "Prototyping of a portable data logging embedded system for naturalistic motorcycle study",
        "abstract": "The primary objective of this work is to design a highly portable data logging embedded system for naturalistic motorcycle study with capability of collecting many types of data such as images, speed, acceleration, time, location, distance approximation, etc. The proposed embedded system design is based on an Arduino microcontroller and is capable of storing up to 220 hours of text/image data during a one month study period. We have successfully designed and implemented the system. The data acquired has been validated and found to be accurate.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6799838/",
        "year": "2013"
    },
    {
        "title": "Provable Unlinkability Against Traffic Analysis Already After $\\mathcal{O}(\\log(n))~$ Steps!",
        "abstract": "We consider unlinkability of communication problem: given n users, each sending a¬†message to some destination, encode and route the messages so that an adversary analyzing the traffic in the communication network cannot link the senders with the recipients. A solution should have a¬†small communication overhead, that is, the number of additional messages should be kept low.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-30144-8_30",
        "year": "2004"
    },
    {
        "title": "Query Log Analysis",
        "abstract": "¬†Weblog Analysis",
        "include": false,
        "url": "http://link.springer.com/referenceworkentry/10.1007/978-1-4939-7131-2_100936",
        "year": "2018"
    },
    {
        "title": "Query log analysis in the context of information retrieval for children",
        "abstract": "In this paper we analyze queries and sessions intended to satisfy children's information needs using a large-scale query log. The aim of this analysis is twofold: i) To identify differences between such queries and sessions, and general queries and sessions; ii) To enhance the query log by including annotations of queries, sessions, and actions for future research on information retrieval for children. We found statistically significant differences between the set of general purpose and queries seeking for content intended for children. We show that our findings are consistent with previous studies on the physical behavior of children using Web search engines.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1835646",
        "year": "2010"
    },
    {
        "title": "Query log analysis: social and technological challenges",
        "abstract": "Analysis of search engine query logs is an important tool for developers and researchers. However, the potentially personal content of query logs raises a number of questions about the use of that data. Privacy advocates are concerned about potential misuse of personal data; search engine providers are interested in protecting their users while maintaining a competitive edge; and academic researchers are frustrated by barriers to shared learning though shared data analysis. This paper reports on a workshop held at the WWW 2007 Conference to foster dialogue on the social and technical challenges that are posed by the content of query logs and the analysis of that content.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1328985",
        "year": "2007"
    },
    {
        "title": "Query log driven web search results clustering",
        "abstract": "Different important studies in Web search results clustering have recently shown increasing performances motivated by the use of external resources. Following this trend, we present a new algorithm called Dual C-Means, which provides a theoretical background for clustering in different representation spaces. Its originality relies on the fact that external resources can drive the clustering process as well as the labeling task in a single step. To validate our hypotheses, a series of experiments are conducted over different standard datasets and in particular over a new dataset built from the TREC Web Track 2012 to take into account query logs information. The comprehensive empirical evaluation of the proposed approach demonstrates its significant advantages over traditional clustering and labeling techniques.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2609583",
        "year": "2014"
    },
    {
        "title": "Query Log Mining for Inferring User Tasks and Needs",
        "abstract": "Search behavior, and information seeking behavior more generally, is often motivated by tasks that prompt search processes that are often lengthy, iterative, and intermittent, and are characterized by distinct stages, shifting goals and multitasking. Current search systems do not provide adequate support for users tackling complex tasks due to which the cognitive burden of keeping track of such tasks is placed on the searcher. In this note, we summarize our recent efforts towards extracting search tasks from search logs. Based on recent advancements in Bayesian Nonparametrics and distributional semantics, we propose novel algorithms to extract task and subtasks from a query collection. The models discussed can inform the design of the next generation of task-based search systems that leverage user‚Äôs task behavior for better support and personalization.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-46131-1_36",
        "year": "2016"
    },
    {
        "title": "Query-Log Based Authority Analysis for Web Information Search",
        "abstract": "The ongoing explosion of web information calls for more intelligent and personalized methods towards better search result quality for advanced queries. Query logs and click streams obtained from web browsers or search engines can contribute to better quality by exploiting the collaborative recommendations that are implicitly embedded in this information. This paper presents a new method that incorporates the notion of query nodes into the PageRank model and integrates the implicit relevance feedback given by click streams into the automated process of authority analysis. This approach generalizes the well-known random-surfer model into a random-expert model that mimics the behavior of an expert user in an extended session consisting of queries, query refinements, and result-navigation steps. The enhanced PageRank scores, coined QRank scores, can be computed offline; at query-time they are combined with query-specific relevance measures with virtually no overhead. Our preliminary experiments, based on real-life query-log and click-stream traces from eight different trial users indicate significant improvements in the precision of search results.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-30480-7_11",
        "year": "2004"
    },
    {
        "title": "Quickdraw: Generating Security Log Events for Legacy SCADA and Control System Devices",
        "abstract": "Security event logs play a role in the early detection of attacks and in after incident investigations. Controllers used in SCADA, DCS and other control systems log almost no security events. This deficiency is addressed by the Quickdraw application, which is a passive security log generator for controllers. Quickdraw monitors communication like a network IDS, detects events that should be logged in a controller, creates the security events, and then sends the event to a historian, SEM or other log aggregator.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/4804448/",
        "year": "2009"
    },
    {
        "title": "Radiation patterns of a log-periodic antenna in the VHF band: comparison between simulations and measurements",
        "abstract": "The aim of this paper is to compare the output of two models used to determine the radiation patterns of log-periodic antennas placed over a ground plane, with measurements performed in the VHF band. In the first part, we describe the simulation models: the reflection coefficient method based on the moment method, and the R/sub V//R/sub H/ method, based on the Fresnel reflection coefficients. In the second part, we present the method used to measure the radiation patterns. These measurements were realized at several frequencies and several antenna heights. Then, we compare the experimental and the simulated results and we conclude with the capabilities of each model.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/630216/",
        "year": "1997"
    },
    {
        "title": "Radiation patterns of paraboloid with log-periodic dipole feed",
        "abstract": "The effect of axial displacement of phase centre on the secondary performance of a parabolic reflector is described, with particular reference to a log-periodic dipole-array feed. Computed and experimental results are presented.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4235361/",
        "year": "1971"
    },
    {
        "title": "RALD: Reliable Transactional Software Controller for Redundancy Array of Log Structured Disks",
        "abstract": "An inconsistent update in a parity-based RAID can cause data loss when a disk failure occurs. There are two ways to restore the consistency: 1, Re-calculating parities, which does not work with disk failures. 2, Using transactions to record and replay updated data-contents in the transaction log, which is a single point of failure. It needs much bandwidth to replicate a transaction log of data-contents over network. We proposed the Above-Logging Transaction (ALT), and designed the Redundancy Array of Log-structured Disks (RALD). All updates histories of data-contents were on log-structured disks. ALTs recorded updated data's addresses on those log-structured disks into the transaction log by which ALTs mapped the consistent blocks into the read-only snapshot's space after they finished. To avoid the single point of failure, the RALD copied the ALTs log into those log-structured disks. By using flushes, the RALD utilized write-buffers safely. We had evaluated the RALD on SATA3 HDDs. The RALD can restore consistency from disk failures plus system crashes. The ALT's logs consumes little bandwidth, less than 0.5MB/s per replica. Compared to the Linux MD RAID: on write-dominant traces, the RALD has 20% to 190% more IOPS, on read-dominant traces, the RALD has 30% to 160% more IOPS if its internal caches have effective read-ahead, otherwise it has 10% to 50% less IOPS.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7336152/",
        "year": "2015"
    },
    {
        "title": "RALFIE: a life-logging system for reducing potential radiation exposures",
        "abstract": "After the nuclear power plant accident in Fukushima, a significant amount of radioactive materials were released into the environment. The radiation exposure has brought lots of concerns about environments contamination, as well as economic and social consequences. Japan governmental agencies have started to continuously monitor and collect radiation levels by monitoring posts, probe cars and airborne surveys. These monitoring data contribute to estimate the external dose levels of radioactive materials and analyze human effects in the future. In this paper, we introduce the RALFIE (RAdiation Exposure LiFelog Indicator) system to help residents and experts to search for contaminated spots in daily activities and develop a reference guideline for reducing external exposures for individuals with their own data. First we design a model to integrate personal spatio-temporal positions with air dose rates on the real-time radiation monitoring data. Then we compare the relationship between ambient dose equivalent and individuals from D-shuttle sensors. Finally, we discuss how to create semantic trajectories with users' real-time annotation and geographic features with future directions.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2835615",
        "year": "2015"
    },
    {
        "title": "Ranking Entities Using Web Search Query Logs",
        "abstract": "Searching for entities is an emerging task in Information Retrieval for which the goal is finding well defined entities instead of documents matching the query terms. In this paper we propose a novel approach to Entity Retrieval by using Web search engine query logs. We use Markov random walks on (1) Click Graphs ‚Äì built from clickthrough data ‚Äì and on (2) Session Graphs ‚Äì built from user session information. We thus provide semantic bridges between different query terms, and therefore indicate meaningful connections between Entity Retrieval queries and related entities.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-15464-5_28",
        "year": "2010"
    },
    {
        "title": "Real time system for acquiring and logging the plan position using NI MyRIO controller",
        "abstract": "Present paper proposes a system for establishing in real time the current geographical positioning with high accuracy. The system is composed by a NI (National Instruments) MyRIO controller equipped with FPGA (Field Programmable Gate Array) and Real time technologies which communicates with a laptop as the host computer and a GPS (Global Positioning System) module. Using an appropriate soft application accomplished in LabVIEW, the GPS data is processed with 10 Hz and having Internet connection, the application may also set the current positioning on Google Earth maps. This is possible by designing shared variables which provide communication in real time between files. The update rate is set to 10Hz. The LabVIEW application can save the GPS data into documents in order the information to be post-processed.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8019278/",
        "year": "2017"
    },
    {
        "title": "Real-Time Log Analysis Using Hitachi uCosminexus Stream Data Platform",
        "abstract": "In this demo, we present real-time log analysis using Hitachi uCosminexus Stream Data Platform, uCSDP for short. Real-time log analysis is one of the key applications that offers preventive measures to detect irregular manipulations and human mistakes in system management, and reduces the risk and loss caused by such operations to the minimum in advance. uCSDP is the stream data processing system featuring its declarative query processing language, flexible time management, RAS support for high-available processing, and eager scheduling for ultra low latency processing. This demo highlights the uCSDP features for realizing real-time log analysis very easily and effectively.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-12098-5_45",
        "year": "2010"
    },
    {
        "title": "Real-time monitoring of SDN networks using non-invasive cloud-based logging platforms",
        "abstract": "The Software Defined Networking (SDN) paradigm enables quick deployment of software controlled network infrastructures, however new approaches to system monitoring are required to provide network administrators with instant feedback on a network's health. This paper details the deployment of an SDN system architecture featuring the integration of a cloud-based, real-time log-analysis platform. The proposed architecture uses log data collected from host machines, OpenFlow switches and the SDN controllers in a non-invasive style. This work uses a commercially available correlation platform to provide network administrators with a real-time view of the network status and the approach is validated under two scenarios:network overload and a security attack.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7794973/",
        "year": "2016"
    },
    {
        "title": "Recognition method of behavioral pattern by sequential learning for life logs",
        "abstract": "We proposed a new pattern recognition method suitable for sequential learning and self adapting. which meets the requirement of Web behavioral analysis on large Life Log databases. Even if sparse data, such as web page keywords, a user's web sequence can be efficiently compared with the statistical sequence. If a user's Web behavior does not change suddenly, our method will be accurate. For future work, we will evaluate the above metioned details in terms of performance and accuracy.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5532056/",
        "year": "2010"
    },
    {
        "title": "Recommendation of Process Discovery Algorithms Through Event Log Classification",
        "abstract": "Process mining is concerned with the extraction of knowledge about business processes from information system logs. Process discovery algorithms are process mining techniques focused on discovering process models starting from event logs. The applicability and effectiveness of process discovery algorithms rely on features of event logs and process characteristics. Selecting a suitable algorithm for an event log is a tough task due to the variety of variables involved in this process. The traditional approaches use empirical assessment in order to recommend a suitable discovery algorithm. This is a time consuming and computationally expensive approach. The present paper evaluates the usefulness of an approach based on classification to recommend discovery algorithms. A knowledge base was constructed, based on features of event logs and process characteristics, in order to train the classifiers. Experimental results obtained with the classifiers evidence the usefulness of the proposal for recommendation of discovery algorithms.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-19264-2_1",
        "year": "2015"
    },
    {
        "title": "Recommending join queries via query log analysis",
        "abstract": "Complex ad hoc join queries over enterprise databases are commonly used by business data analysts to understand and analyze a variety of enterprise-wide processes. However, effectively formulating such queries is a challenging task for human users, especially over databases that have large, heterogeneous schemas. In this paper, we propose a novel approach to automatically create join query recommendations based on input-output specifications (i.e.,input tables on which selection conditions are imposed, and output tables whose attribute values must be in the result of the query).The recommended join query graph includes (i) \"intermediate'' tables, and (ii) join conditions that connect the input and output tables via the intermediate tables. Our method is based on analyzing an existing query log over the enterprise database. Borrowing from program slicing techniques, which extract parts of a program that affect the value of a given variable, we first extract \"query slices'' from each query in the log. Given a user specification, we then re-combine appropriate slices to create a new join query graph, which connects the sets of input and output tables via the intermediate tables. We propose and study several quality measures to enable choosing a good join query graph among the many possibilities. Each measure expresses an intuitive notion that there should be sufficient evidence in the log to support our recommendation of the join query graph. We conduct an extensive study using the log of an actual enterprise database system to demonstrate the viability of our novel approach for recommending join queries.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/abstract/document/4812469/",
        "year": "2009"
    },
    {
        "title": "Reconstructing IMS LD Units of Learning from Event Logs",
        "abstract": "In this paper a novel approach to facilitate the reuse of units of learning (UoLs) is presented. Typically, e-learning platforms do not provide the means to retrieve designed UoLs in a standardized format to be reused in a different platform, but they have in common that the students and teachers interaction with the system is logged to files. Taking this into account, we propose to use these logs and apply a three steps re-engineering approach to translate these UoLs into an accepted educational modelling language, specifically IMS LD. In the first step, the sequence of activities and their functional dependencies are learned by a process mining algorithm. In the second step, another algorithm analyses the variables and their value change in order to learn the adaptation rules that may have been defined in the UoL. And finally, in the last step the inferred process structure and rules are matched with the typical structure of activities, acts, and plays defined by IMS LD.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-11200-8_26",
        "year": "2014"
    },
    {
        "title": "Recovering deleted browsing artifacts from web browser log files in Linux environment",
        "abstract": "In the present day scenario when it comes to Information interchange no one can even think about it without the use of Internet. Today there are uncountable numbers of websites in existence with loads of information present in them. To acquire this information one has to use a web browser in which these websites can be browsed according to our need. These web browsers store the users browsing history in specified log files which can be used in digital forensic investigation to acquire lots of information regarding the suspects browsing history. But these log files can be manipulated and cleaned by the users as and when required. In such a situation when suspects involved in any cyber crime deliberately cleans the traces of their browsing activity from any computer system further in depth examination of the crime becomes challenging and typical for the investigator concerned. This research is mainly focused on recovering such deleted browsing artifacts from the installed browser (Google Chrome in Linux environment) by an in depth analysis of the image of the hard disk used by the suspects involved in the crime.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7570957/",
        "year": "2016"
    },
    {
        "title": "Recovery in distributed systems using asynchronous message logging and checkpointing",
        "abstract": "No abstract available.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=62575",
        "year": "1988"
    },
    {
        "title": "Redo Log Process Mining in Real Life: Data Challenges \u0026 Opportunities",
        "abstract": "Data extraction and preparation are the most time-consuming phases of any process mining project. Due to the variability on the sources of event data, it remains a highly manual process in most of the cases. Moreover, it is very difficult to obtain reliable event data in enterprise systems that are not process-aware. Some techniques, like redo log process mining, try to solve these issues by automating the process as much as possible, and enabling event extraction in systems that are not process aware. This paper presents the challenges faced by redo log, and traditional process mining, comparing both approaches at theoretical and practical levels. Finally, we demonstrate that the data obtained with redo log process mining in a real-life environment is, at least, as valid as the one extracted by the traditional approach.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-74030-0_45",
        "year": "2018"
    },
    {
        "title": "Relationships among the Computational Powers of Breaking Discrete Log Cryptosystems",
        "abstract": "We investigate the complexity of breaking cryptosystems of which security is based on the discrete logarithm problem. We denote the algorithms of breaking the Diffie-Hellman‚Äôs key exchange scheme by DH, the Bellare-Micali‚Äôs non-interactive oblivious transfer scheme by BM, the ElGamal‚Äôs public-key cryptosystem by EG, the Okamoto‚Äôs conference- key sharing scheme by CONF, and the Shamir‚Äôs 3-pass key-transmission scheme by 3PASS, respectively. We show a relation among these cryp- tosystems that 3PASS ‚â§ m FP CONF ‚â§ m FP EG ‚â° m FP DH, where ‚â§ m FP denotes the polynomial-time functionally many-to-one re- ducibility, i.e. a function version of the ‚â§ m p -reducibility. We further give some condition in which these algorithms have equivalent difficulty. Namely,1.If the complete factorization of p ‚àí 1 is given, i.e. if the the dis- crete logarithm problem is a certified one, then these cryptosystems are equivalent w.r.t. expected polynomial-time functionally Turing reducibility.¬†2.If the underlying group is the Jacobian of an elliptic curve over Z p with a prime order, then these cryptosystems are equivalent w.r.t. polynomial-time functionally many-to-one reducibility.¬†",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-49264-X_28",
        "year": "1995"
    },
    {
        "title": "Remote sensing of selective logging in Amazonia: Assessing limitations based on detailed field observations, Landsat ETM+, and textural analysis",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0034425701003261",
        "year": "2002"
    },
    {
        "title": "Research on Application of Web Log Analysis Method in Agriculture Website Improvement",
        "abstract": "With the advance of agricultural modernization, agriculture website was increasingly becoming a major tool for farmers getting information about life and production. How to make the analysis of the needs of farmers effectively to help them to find the information from the ocean of information and resources of the Internet they were interested in had become an urgent and important issue. In this paper, we used the website of Agridata as an example and focused on the solution for the problem. A way was proposed for analyzing and mining the web log of Agridata, which integrated statistical analysis and cluster analysis. By this method, could information behaviors of users be grasped when browsing the website. It was important significance in improving the structure and content of agriculture website, which could provide better services for farmers and improve the level of modernization of agricultural production.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-27278-3_30",
        "year": "2012"
    },
    {
        "title": "Research on audit log association rule mining based on improved Apriori algorithm",
        "abstract": "Aimed at solving the problem of low-level intelligence and low utilization of audit logs of the security audit system, a secure audit system based on association rule mining is proposed in this paper. The system is able to take full advantage of the existing audit logs, establish the behavior pattern database of users and the system with data mining technique, and discover abnormal situation in a timely manner, which improves the security of computer system. We propose an improved E-Apriori algorithm which narrows the scanning range of the transactions, lowers the time complexity, and refines the operating efficiency. Experiment results on the Weka platform indicate that our proposed E-Apriori algorithm clearly outperforms the traditional Apriori algorithm, especially in the large sparse datasets.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/7509792/",
        "year": "2016"
    },
    {
        "title": "Research on log Gabor wavelet and its application in image edge detection",
        "abstract": "Gabor function and log Gabor function are compared in this paper. The log Gabor wavelet is developed and its performance is analyzed. Compared with Gabor wavelet, the excellent performance of log Gabor wavelet is shown. Finally, log Gabor wavelet is applied to image edge detection, and it is shown that using log Gabor wavelet can not only improve the detection property, but also save about half the computational load compared with Gabor wavelet, in the case of similar detection effect.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1181125/",
        "year": "2002"
    },
    {
        "title": "Research on log management system of geographical information sharing platform based on WebGIS",
        "abstract": "This paper illustrates the design ideas of log management system based on WebGIS and implementation methods. Bringing WebGIS technology into the field of log management, giving full play to capabilities of WebGIS graphical information dissemination and retrieval, and achieving real-time monitoring data display and analysis, will greatly enhance the intuitive and convenient level of log management system. But due to achievement mechanism of WebGIS system, as well as external network constraints, the current application system based on WebGIS cannot achieve true on-line monitoring. Introducing RIA technology into the log management system and taking Suzhou Industrial Park geographic information sharing platform for example, we build out a comprehensive and efficient log data management system to achieve spatial information which is relating to users' access visualization. The system can monitor network traffic online efficiently and easily, count and analyze log data, provide a scientific decision-making basis for operation and maintenance of geographic information sharing platform.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6270341/",
        "year": "2012"
    },
    {
        "title": "Research on Log Pre-processing for Exascale System Using Sparse Representation",
        "abstract": "With system size and complexity is growing rapidly, traditional passive fault tolerance can no longer guarantee the reliability of system because of the high overhead and poor scalability of these methods. Active fault tolerance is believed to be the most important fault tolerant approach for exascale systems. Aiming at system failure prediction, this paper proposes a system logs pre-processing method using classification via sparse representation (SRCP). Adopting the idea of vectorization, SRCP removes the details of each log and generates the corresponding Vectors. It uses TF-IDF (term frequency-inverse document frequency) method to Weight each keyword which can reveal more precise information about correlation between log records. In order to improve the accuracy and flexibility of pre-processing method, log vectors are processed by sparse representation classification. For generalization purpose, SRCP does not adopt any expert system or domain knowledge. Experimental results show that, SRCP can not only achieve both outstanding precision and F-measure, but also provide a satisfactory compression ratio.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-38027-3_36",
        "year": "2013"
    },
    {
        "title": "Research on personalized recommendation system for e-commerce based on web log mining and user browsing behaviors",
        "abstract": "Web server log files and customers transaction data can be mined meaningful user access patterns to anticipate potential customers so as to enable personalized information services and targeted e-commerce activities. The paper bases on Clustering technology of Web Mining to provide a personalized solution to implement an e-commerce recommendation system. The paper introduces the UserID-URL associated matrix according to log information, We calculate UserID-URL associated matrix and Distance matrix to cluster users into user groups. Clustering algorithm is simple and easy to achieve due to improve the nature of algorithm, no such the candidate set of Apriori algorithm in association rules. The system can recommend the goods which other users of this cluster browse to the user and achieve the objective of personalized goods recommendation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5622330/",
        "year": "2010"
    },
    {
        "title": "Restoring auditor credibility: tertiary monitoring and logging of continuous assurance systems",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1467089504000193",
        "year": "2004"
    },
    {
        "title": "Reviewing Logs and Monitoring",
        "abstract": "Whether you‚Äôre dealing with a car or a computer, poor maintenance habits lead to the same consequence: disaster. You‚Äôre on the freeway, carefully driving at the posted speed limit, and your engine suddenly dies. You go to the mechanic, who roots out the cause: your timing belt broke. You would have replaced your timing belt, had you kept to the maintenance schedule and taken your car in for service at 60,000 miles. Airline maintenance crews who stick to a steadfast and detailed maintenance schedule rarely have this happen to them, mainly because they know precisely on what date the plane was maintained, at what time, and what maintenance was performed.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4842-1712-2_5",
        "year": "2016"
    },
    {
        "title": "Root Cause Analysis with Enriched Process Logs",
        "abstract": "In the field of process mining, the use of event logs for the purpose of root cause analysis is increasingly studied. In such an analysis, the availability of attributes/features that may explain the root cause of some phenomena is crucial. Currently, the process of obtaining these attributes from raw event logs is performed more or less on a case-by-case basis: there is still a lack of generalized systematic approach that captures this process. This paper proposes a systematic approach to enrich and transform event logs in order to obtain the required attributes for root cause analysis using classical data mining techniques, the classification techniques. This approach is formalized and its applicability has been validated using both self-generated and publicly-available logs.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-36285-9_18",
        "year": "2013"
    },
    {
        "title": "Rule-driven defect detection in CT images of hardwood logs",
        "abstract": "This paper deals with automated detection and identification of internal defects in hardwood logs using computed tomography (CT) images. We have developed a system that employs artificial neural networks (ANNs) to perform tentative classification of logs on a pixel-by-pixel basis. This approach achieves a high level of classification accuracy for several hardwood species (northern red oak, Quercus rubra, L., water oak, Q. nigra, L., yellow poplar, Liriodendron tulipifera, L., and black cherry, Prunus serotina, Ehrh.), and three common defect types (knots, splits, and decay). Although the results are very satisfactory statistically, a subjective examination reveals situations that could be refined in a subsequent post-processing step. We are currently developing a rule-based, contextual approach to region refinement that augments the initial emphasis on local information. The resulting rules are domain dependent, utilizing information that depends on region shape and type of defect. For example splits tend to be long and narrow, and this knowledge can be used to merge smaller, disjoint regions that have tentatively been labeled as splits. Similarly, image regions that represent knots, decay, and clear wood can be refined by removing small, spurious regions and by smoothing the boundaries of these regions. Mathematical morphology operators can be used for most of these tasks. This paper provides details concerning the domain-dependent rules by which morphology operators are chosen, and for merging results from different operations.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0168169903000462",
        "year": "2003"
    },
    {
        "title": "SAHN with SEP/COP and SPADE, to Build a General Web Navigation Adaptation System Using Server Log Information",
        "abstract": "During the last decades, the information on the web has increased drastically but larger quantities of data do not provide added value for web visitors; there is a need of easier access to the required information and adaptation to their preferences or needs. The use of machine learning techniques to build user models allows to take into account their real preferences. We present in this work the design of a complete system, based on the collaborative filtering approach, to identify interesting links for the users while they are navigating and to make the access to those links easier. Starting from web navigation logs and adding a generalization procedure to the preprocessing step, we use agglomerative hierarchical clustering (SAHN) combined with SEP/COP, a novel methodology to obtain the best partition from a hierarchy, to group users with similar navigation behavior or interests. We then use SPADE as sequential pattern discovery technique to obtain the most probable transactions for the users belonging to each group and then be able to adapt the navigation of future users according to those profiles. The experiments show that the designed system performs efficiently in a web-accesible database and is even able to tackle the cold start or 0-day problem.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-25274-7_42",
        "year": "2011"
    },
    {
        "title": "Search log analysis: What it is, what's been done, how to do it",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0740818806000673",
        "year": "2006"
    },
    {
        "title": "Security Incident Detection Using Multidimensional Analysis of the Web Server Log Files",
        "abstract": "The paper presents the results of the research related to security analysis of web servers. The presented method uses the web server log files to determine the type of the attack against the web server. The web server log files are collections of text strings describing users‚Äô requests, so one of the most important part of the work was to propose the method of conversion informative part of the requests, to numerical values to make possible further automatic processing. The vector of values obtained as the result of web server log file processing is used as the input to Self-Organizing Map (SOM) network. Finally, the SOM network has been trained to detect SQL injections and brute force password guessing attack. The method has been validated using the data obtained from a real data center.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-11289-3_67",
        "year": "2014"
    },
    {
        "title": "Sedentary Behavior-Based User Life-Log Monitoring for Wellness Services",
        "abstract": "Ubiquitous computing and smart gadgets have revolutionized the self-quantification in tracking and logging activities for improving daily life and inducing healthy behavior. Life-log monitoring is the process of monitoring the daily life routines of user in an efficient manner in terms of time and amount of activities. The effective utilization of life-log monitoring is to correctly identify and intimate user unhealthy activities in a timely manner. For monitoring life-log, the knowledge of sedentary behavior first need to be formulated by the domain expert in the form of unhealthy situations, these situations are used as the monitoring unit. In this study we proposed a method for automatically monitoring users‚Äô unhealthy situations in the domain of sedentary behavior with prolonged activities. The proposed method simultaneously filters out multiple sedentary activities of users simultaneously while ignoring the activities having no situations. The results depict that the monitoring method intimates the stakeholder with delay less than the monitoring interval cycle.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39601-9_25",
        "year": "2016"
    },
    {
        "title": "Semantic Query Federation for Scalable Security Log Analysis",
        "abstract": "The digitalization of business processes increasingly exposes organizations to sophisticated cyber-security threats. To contain attacks and minimize their impact, it is essential to detect them early. To this end, it is necessary to analyze a wide range of log files that potentially provide clues about malicious activity. However, these logs are typically voluminous, heterogeneous, difficult to interpret, and stored in disparate locations, which makes it difficult to analyze them. Current approaches to analyze security logs mainly focus on regular expressions and statistical indicators and do not directly provide actionable insight to security analysts. To address these limitations, we propose a distributed approach that enables semantic querying of dispersed log sources in large-scale infrastructures. To automatically integrate and reason about security log information, we will leverage linked data technologies and state-of-the-art federated query processing systems. In this proposal, we discuss the research problem, methodology, approach and evaluation plan for scalable federated semantic security log analysis.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-98192-5_48",
        "year": "2018"
    },
    {
        "title": "Semi-supervised Log Pattern Detection and Exploration Using Event Concurrence and Contextual Information",
        "abstract": "Process mining offers a variety of techniques for analyzing process execution event logs. Although process discovery algorithms construct end-to-end process models, they often have difficulties dealing with the complexity of real-life event logs. Discovered models may contain either complex or over-generalized fragments, the interpretation of which is difficult, and can result in misleading insights. Detecting and visualizing behavioral patterns instead of creating model structures can reduce complexity and give more accurate insights into recorded behaviors. Unsupervised detection techniques, based on statistical properties of the log only, generate a multitude of patterns and lack domain context. Supervised pattern detection requires a domain expert to specify patterns manually and lacks the event log context. In this paper, we reconcile supervised and unsupervised pattern detection. We visualize the log and help users extract patterns of interest from the log or obtain patterns through unsupervised learning automatically. Pattern matches are visualized in the context of the event log (also showing concurrency and additional contextual information). Earlier patterns can be extended or modified based on the insights. This enables an interactive and iterative approach to identify complex and concrete behavioral patterns in event logs. We implemented our approach in the ProM framework and evaluated the tool using both the BPI Challenge 2012 log of a loan application process and an insurance claims log from a major Australian insurance company.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-69462-7_11",
        "year": "2017"
    },
    {
        "title": "SePMa: An Algorithm That Mining Sequential Processes from Hybrid Log",
        "abstract": "To accommodate ourselves to the changeful and complex business environment, we should be able adjust the business processes within the enterprise whenever changes happen. However, the work to design and redesign the processes is far from trivial, the designers are required to have deep knowledge of the business processes at hand, in traditional approaches it means long term investigation and high cost. To automate the procedure of process discovery, process mining is introduced. Process mining takes the run-time log generated by the process management system as its input, and outputs the process models defined for the system. Unfortunately, current work on process mining often assumes that the input log is generated by the same process, but in many occasions this requisition is hard to be satisfied. In this paper, we propose SePMa, an algorithm that mining sequential processes from hybrid log. SePMa aims at discovering sequential processes from log generated by multiple processes, both of theoretical analysis and experimental results show that SePMa has very high efficiency and effectiveness.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-77018-3_30",
        "year": "2007"
    },
    {
        "title": "Session Identification Based on Time Interval in Web Log Mining",
        "abstract": "In this paper, we calculate the time intervals of page views, and analyze the time intervals to obtain a certain threshold, which is then used to break the web logs into sessions. Based on the time intervals, frequencies for each interval are counted and frequency vectors are obtained for each IP. Some IPs with special features of frequency distributions can be deemed as single users. For these IPs, we can define threshold for each individual IP, and separate sessions at the points of long access time intervals.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/0-387-23152-8_50",
        "year": "2005"
    },
    {
        "title": "Shedding Light on Log Correlation in Network Forensics Analysis",
        "abstract": "Presently, forensics analyses of security incidents rely largely on manual, ad-hoc, and very time-consuming processes. A security analyst needs to manually correlate evidence from diverse security logs with expertise on suspected malware and background on the configuration of an infrastructure to diagnose if, when, and how an incident happened. To improve our understanding of forensics analysis processes, in this work we analyze the diagnosis of 200 infections detected within a large operational network. Based on the analyzed incidents, we build a decision support tool that shows how to correlate evidence from different sources of security data to expedite manual forensics analysis of compromised systems. Our tool is based on the C4.5 decision tree classifier and shows how to combine four commonly-used data sources, namely IDS alerts, reconnaissance and vulnerability reports, blacklists, and a search engine, to verify different types of malware, like Torpig, SbBot, and FakeAV. Our evaluation confirms that the derived decision tree helps to accurately diagnose infections, while it exhibits comparable performance with a more sophisticated SVM classifier, which however is much less interpretable for non statisticians.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-37300-8_14",
        "year": "2013"
    },
    {
        "title": "Sliding window technique for the web log analysis",
        "abstract": "The results of the Web query log analysis may be significantly shifted depending on the fraction of agents (non-human clients), which are not excluded from the log. To detect and exclude agents the Web log studies use threshold values for a number of requests submitted by a client during the observation period. However, different studies use different observation periods, and a threshold assigned to one period is usually incomparable with the threshold assigned to the other period. We propose the uniform method equally working on the different observation periods. The method bases on the sliding window technique: a threshold is assigned to the sliding window rather than to the whole observation period. Besides, we determine the sub-optimal values of the parameters of the method: a window size and a threshold and recommend 5-7 unique queries as an upper bound of the threshold for 1-hour sliding window.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1242771",
        "year": "2007"
    },
    {
        "title": "Small mammals and retention islands: an experimental study of animal response to alternative logging practices",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0378112710005189",
        "year": "2010"
    },
    {
        "title": "SnortView: visualization system of snort logs",
        "abstract": "False detection is a major issue in deploying and maintaining Network-based Intrusion Detection Systems (NIDS). Traditionally, it is recommended to customize its signature database (DB) to reduce false detections. However, it requires quite deep knowledge and skills to appropriately customize the signature DB. Inappropriate customization causes the increase of false negatives as well as false positives. In this paper, we propose a visualization system of a NIDS log, named SnortView, which supports administrators in analyzing NIDS alerts much faster and much more easily. Instead of customizing the signature DB, we propose to utilize visualization to recognize not only each alert but also false detections. The system is based on a 2-D time diagram and alerts are shown as icons with different styles and colors. In addition, the system introduces some visualization techniques such as overlayed statistical information, source-destination matrix, and so on. The system was used to detect real attacks while recognizing some false detections.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1029232",
        "year": "2004"
    },
    {
        "title": "Source rock evaluation by well log analysis (Lower Toarcian, Hils syncline)",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/B9780080372365500178",
        "year": "1988"
    },
    {
        "title": "Spectral shape analysis for contaminant logging at the Hanford site",
        "abstract": "A spectral shape factor log has been developed to provide information on the spatial distribution of gamma-ray-emitting contaminants detected by passive logging of boreholes that surround the buried high-level nuclear waste tanks at the U.S. Department of Energy's Hanford Site near Richland, Washington. Many boreholes intersect plumes of contaminants that leaked from the tanks. Spectral shape analysis has been able to distinguish /sup 137/Cs and /sup 60/Co uniformly distributed in the formation from borehole-localized and radially remote distributions of these contaminants. A uniform distribution is evidence for a formation-migration plume while a borehole-localized contaminant distribution indicates that the borehole was a preferential pathway for contaminant migration. Knowledge of the contaminant distribution assists the process of formation contamination assessment and of estimation of remediation costs. In a few cases, the shape factor log has identified the bremsstrahlung-producing contaminant /sup 90/Sr//sup 90/Y. Shape factor data processing and interpretation methods were optimized using results of Monte Carlo simulations of the logging measurements. The accuracy of the Monte Carlo simulations was confirmed through experiments with known source distributions.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/682695/",
        "year": "1998"
    },
    {
        "title": "Speech-to-Text-Based Life Log System for Smartphones",
        "abstract": "In this paper, we propose a life log system which provides a real-time voice recording with a speech-to-text feature over smartphone environments. The proposed system records data of user life using a microphone of smartphone. Recorded data are sent to a server, analyzed, and stored. Recorded data are dictated using a speech-to-text service, and saved as text files. The proposed system is implemented as a prototype system and evaluated. Users of our system are able to search their life log sound files using text.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-981-10-0281-6_90",
        "year": "2015"
    },
    {
        "title": "Speed Limit Sign Recognition Using Log-Polar Mapping and Visual Codebook",
        "abstract": "Traffic sign recognition is one of the hot issues on the modern driving assistance. In recent years, the method using Bag-of-Word (BOW) model for image recognition has gained its popularity upon its simplicity and efficiency. The conventional approach based on BOW requires nonlinear classifiers to get a good image recognition accuracy. Instead, a method called Locality-constrained Linear Coding(LLC) presents an effective strategy for coding, and only with a simple linear classifier could achieve a good effect. LLC uses uniform sampling for feature extraction, but allowing for features of traffic signs, the central vision information of the image is more important than the surroundings. Fortunately, log-polar mapping to preprocess image samples before coding is helpful for traffic sign recognition. In this paper, a combination method of log-polar mapping and LLC algorithm is presented to achieve a high image classification performance up to 97.3141% on speed limit sign in the GTSRB dataset.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-31362-2_28",
        "year": "2012"
    },
    {
        "title": "SPTrack: Visual Analysis of Information Flows within SELinux Policies and Attack Logs",
        "abstract": "Analyzing and administrating system security policies is difficult as policies become larger and more complex every day. The paper present work toward analyzing security policies and sessions in terms of security properties. Our intuition was that combining both visualization tools that could benefit from the expert‚Äôs eyes, and software analysis abilities, should lead to a new interesting way to study and manage security policies as well as users‚Äô sessions. Rather than trying to mine large and complex policies to find possible flaws within, work may concentrate on which potential flaws are really exploited by attackers.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-35236-2_60",
        "year": "2012"
    },
    {
        "title": "Stability analysis of the turbo decoding algorithm using Max-Log-MAP",
        "abstract": "In this paper we investigate analytically the convergence properties of the iterative turbo decoding with the Max-Log-MAP algorithm for soft-output generation. With respect to the iterative decoding scheme, several equivalent mathematical formulations are derived to facilitate the stability analysis, based on which we show that turbo decoding with Max-Log-MAP has at least one fixed point but is in general suboptimal and may not even converge. Finally, some conditions are given to guarantee the convergence of the iterative turbo decoding.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1023413/",
        "year": "2002"
    },
    {
        "title": "State-space synthesis and analysis of log-domain filters",
        "abstract": "The general state-space synthesis of linear filters is reviewed including the use of linear transformations. Then the use of time-varying nonlinear mappings on the state variables is shown to produce equations for the synthesis of linear, linear companding, and log-domain filters. Formulas are developed which underlie the realization of log-domain filters. Design issues relating to realizability, noise, and nonideal performance are discussed using the state-space mathematical formulation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/abstract/document/718587/",
        "year": "1998"
    },
    {
        "title": "Statistical analysis of the power sum of multiple correlated log-normal components",
        "abstract": "A statistical method is presented for the analysis of the power sum of multiple correlated log-normal random components. The results are compared with those of S.C. Schwartz and Y.S. Yeh (1982) and the Monte Carlo simulation. The effect of correlation on the mean and the variance of the combined multiple correlated log-normal components is also investigated. The presented method allows quick and accurate calculations for various system performance parameters of radio networks for high-capacity cellular telephony, two-way paging, packet radio, mobile data networks, and radar detection systems.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/192387/",
        "year": "1993"
    },
    {
        "title": "Statistical Model-Based Voice Activity Detection Using Spatial Cues and Log Energy for Dual-Channel Noisy Speech Recognition",
        "abstract": "In this paper, a voice activity detection (VAD) method for dual-channel noisy speech recognition is proposed on the basis of statistical models constructed by spatial cues and log energy. In particular, spatial cues are composed of the interaural time differences and interaural level differences of dual-channel speech signals, and the statistical models for speech presence and absence are based on a Gaussian kernel density. In order to evaluate the performance of the proposed VAD method, speech recognition is performed using only speech signals segmented by the proposed VAD method. The performance of the proposed VAD method is then compared with those of conventional methods such as a signal-to-noise ratio variance based method and a phase vector based method. It is shown from the experiments that the proposed VAD method outperforms conventional methods, providing the relative word error rate reductions of 19.5% and 12.2%, respectively.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-17604-3_19",
        "year": "2010"
    },
    {
        "title": "Status of preparation of IAEA handbook on nuclear data for well logging and mineral analysis",
        "abstract": "The preliminary scope and contents of the IAEA (International Atomic Energy Agency) handbook are presented. The work consists of two tasks. One task is the book itself, containing graphs, tables, and the necessary explanatory texts, and the second is a computer tape containing nuclear data: cross sections, spectra of neutron sources, and decay data for a number of selected isotopes. The data will be supplemented by a number of codes to process cross-section data. The need for covariance information is discussed and some doubts are raised about the benefits of using this kind of information in treating well logging data.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/57392/",
        "year": "1990"
    },
    {
        "title": "Stereo System for Tracking Moving Object Using Log-Polar Transformation and Zero Disparity Filtering",
        "abstract": "An active stereo-vision system enables a target object to be localized based on passing small disparities without heavy computation to identify the target. However, this simple method is not applicable to situations where a distracting background is included or the target and other objects are simultaneously located in the zero disparity area. Accordingly, to alleviate these problems, the current study combined filtering and foveation, which employs high resolution in the center of the visual field, while suppressing the periphery. An image pyramid and log-polar transformation are compared for the foveated image representation. The stereo disparity of the target is also extracted using projection to maintain a small stereo disparity during tracking. Experiments demonstrated that a log-polar transformation was superior to both the image pyramid and the traditional method for separating a target from a distracting background, and comparatively enhanced the tracking performance.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-45179-2_23",
        "year": "2003"
    },
    {
        "title": "Storage and Querying of Large Web-Logs",
        "abstract": "Web access-logs record the access history of users that visit a Web site. The entries of the log are collected automatically and, for this reason, their size tends to grow very rapidly. Recent work has proposed the application of web-log mining methods[4, 7, 12, 15, 16, 17], which search for access-patterns. Some examples include methods based on clustering [20] and sequence mining [2].",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4419-8636-8_8",
        "year": "2003"
    },
    {
        "title": "Storage and retrieval of system log events using a structured schema based on message type transformation",
        "abstract": "Message types are semantic groupings of the free form messages in system log events. The message types that exist in a log file, if known, can be used in several log management and analysis tasks. In this work, we explore the use of message types as a schema definition for the storage and retrieval of messages in event logs. We show how message types can be used to impose structure on the unstructured content of event logs and how this structured representation can provide a usable index for searching the contents of the log file. As a side benefit, the structured representation that message types impose also leads to the removal of redundant information in the event logs that leads to space savings on disk.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1982298",
        "year": "2011"
    },
    {
        "title": "Stratified analysis of AOL query log",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0020025509000516",
        "year": "2009"
    },
    {
        "title": "Strong agile metrics: mining log data to determine predictive power of software metrics for continuous delivery teams",
        "abstract": "ING Bank, a large Netherlands-based internationally operating bank, implemented a fully automated continuous delivery pipe-line for its software engineering activities in more than 300 teams, that perform more than 2500 deployments to production each month on more than 750 different applications. Our objective is to examine how strong metrics for agile (Scrum) DevOps teams can be set in an iterative fashion. We perform an exploratory case study that focuses on the classification based on predictive power of software metrics, in which we analyze log data derived from two initial sources within this pipeline. We analyzed a subset of 16 metrics from 59 squads. We identified two lagging metrics and assessed four leading metrics to be strong.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3117779",
        "year": "2017"
    },
    {
        "title": "Structural Event Detection from Log Messages",
        "abstract": "A wide range of modern web applications are only possible because of the composable nature of the web services they are built upon. It is, therefore, often critical to ensure proper functioning of these web services. As often, the server-side of web services is not directly accessible, several log message based analysis have been developed to monitor the status of web services. Existing techniques focus on using clusters of messages (log patterns) to detect important system events. We argue that meaningful system events are often representable by groups of cohesive log messages and the relationships among these groups. We propose a novel method to mine structural events as directed workflow graphs (where nodes represent log patterns, and edges represent relations among patterns). The structural events are inclusive and correspond to interpretable episodes in the system. The problem is non-trivial due to the nature of log data: (i) Individual log messages contain limited information, and (ii) Log messages in a large scale web system are often interleaved even though the log messages from individual components are ordered. As a result, the patterns and relationships mined directly from the messages and their ordering can be erroneous and unreliable in practice. Our solution is based on the observation that meaningful log patterns and relations often form workflow structures that are connected. Our method directly models the overall quality of structural events. Through both qualitative and quantitative experiments on real world datasets, we demonstrate the effectiveness and the expressiveness of our event detection method.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3098124",
        "year": "2017"
    },
    {
        "title": "Structural Feature Selection for Event Logs",
        "abstract": "We consider the problem of classifying business process instances based on structural features derived from event logs. The main motivation is to provide machine learning based techniques with quick response times for interactive computer assisted root cause analysis. In particular, we create structural features from process mining such as activity and transition occurrence counts, and ordering of activities to be evaluated as potential features for classification. We show that adding such structural features increases the amount of information thus potentially increasing classification accuracy. However, there is an inherent trade-off as using too many features leads to too long run-times for machine learning classification models. One way to improve the machine learning algorithms‚Äô run-time is to only select a small number of features by a feature selection algorithm. However, the run-time required by the feature selection algorithm must also be taken into account. Also, the classification accuracy should not suffer too much from the feature selection. The main contributions of this paper are as follows: First, we propose and compare six different feature selection algorithms by means of an experimental setup comparing their classification accuracy and achievable response times. Second, we discuss the potential use of feature selection results for computer assisted root cause analysis as well as the properties of different types of structural features in the context of feature selection.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-74030-0_2",
        "year": "2018"
    },
    {
        "title": "Struts+Spring+Hibernate Integrated Framework and Its Use in Log Accounting and Analyzing System",
        "abstract": "According to the accretion of business affairs of modern company, it becomes very important to construct a reusable and maintainable application software framework. Based on the concise analyzation of Struts and Spring and Hibernate modern frameworks, this paper discusses the merit of the Struts/Spring/Hibernate integrated framework, then describes its specific use in a CDN network log accounting and analyzing system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5671204/",
        "year": "2010"
    },
    {
        "title": "Students Effort vs. Outcome: Analysis Through Moodle Logs",
        "abstract": "Previous research shows how student's effort is found in Learning Management Systems (LMS). In this paper, we verified if positive effort in face-to-face courses supported by Moodle LMS carries on to their completion. We used logs of several courses and compared them with teachers' diaries. We developed an algorithm to analyze data and retrieve information about students. The results confirm that students with good or high effort are more able to complete the course.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7757000/",
        "year": "2016"
    },
    {
        "title": "Study on behavioral biometrics to predict user's interest level using web access log",
        "abstract": "The value of the user's access log in the website has been improved tremendously. Moreover, web analytics service has been widely used, and the web analytics has become commonly effective. The objective of this study is to propose the behavioral biometrics to predict the user's interest level in a specific product using access log of websites. We conducted the experiment in which the subjects were asked to perform the shopping task on websites. The comparative analysis is conducted between the interest level of one category product getting through the interview and the access log during the purchasing process in websites. The results show that the behavioral patterns of the web searching and some parameters based on the access log were clearly different depending on the interest level.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8090645/",
        "year": "2017"
    },
    {
        "title": "Study on the relationship among multiple websites using access log analysis",
        "abstract": "The present study attempts to clarify a relationship among multiple websites using access log analysis. A lot of information is widely and deeply clarified than a previous access log analysis and is useful for administrators.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5532021/",
        "year": "2010"
    },
    {
        "title": "Subgraph Mining for Anomalous Pattern Discovery in Event Logs",
        "abstract": "Conformance checking allows organizations to verify whether their IT system complies with the prescribed behavior by comparing process executions recorded by the IT system against a process model (representing the normative behavior). However, most of the existing techniques are only able to identify low-level deviations, which provide a scarce support to investigate what actually happened when a process execution deviates from the specification. In this work, we introduce an approach to extract recurrent deviations from historical logging data and generate anomalous patterns representing high-level deviations. These patterns provide analysts with a valuable aid for investigating nonconforming behaviors; moreover, they can be exploited to detect high-level deviations during conformance checking. To identify anomalous behaviors from historical logging data, we apply frequent subgraph mining techniques together with an ad-hoc conformance checking technique. Anomalous patterns are then derived by applying frequent items algorithms to determine highly-correlated deviations, among which ordering relations are inferred. The approach has been validated by means of a set of experiments.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-61461-8_12",
        "year": "2017"
    },
    {
        "title": "Successive cancellation list polar decoder using log-likelihood ratios",
        "abstract": "Successive cancellation list (SCL) decoding algorithm is a powerful method that can help polar codes achieve excellent error-correcting performance. However, the current SCL algorithm and decoders are based on likelihood or log-likelihood forms, which render high hardware complexity. In this paper, we propose a log-likelihood-ratio (LLR)-based SCL (LLR-SCL) decoding algorithm, which only needs half the computation and storage complexity than the conventional one. Then, based on the proposed algorithm, we develop low-complexity VLSI architectures for LLR-SCL decoders. Analysis results show that the proposed LLR-SCL decoder achieves 50% reduction in hardware and 98% improvement in hardware efficiency.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7094505/",
        "year": "2014"
    },
    {
        "title": "Sufficiency of Windows Event Log as Evidence in Digital Forensics",
        "abstract": "The prevalence of computer and the internet has brought forth the increasing spate of cybercrime activities; hence the need for evidence to attribute a crime to a suspect. The research therefore, centres on evidence, the legal standards applied to digital evidence presented in court and the main sources of evidence in the Windows OS, such as the Registry, slack space and the Windows event log. In order to achieve the main aim of this research, cybercrime activities such as automated password guessing attack and hacking was emulated on to a Windows OS within a virtual network environment set up using VMware workstation. After the attack the event logs on the victim system was analysed and assessed for its admissibility (evidence must conform to certain legal rules), and weight (evidence must convince the court that the accused committed the crime).",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-33448-1_34",
        "year": "2012"
    },
    {
        "title": "Supporting seamless learning using ubiquitous learning log system",
        "abstract": "This chapter describes a learning log system named SCROLL (System for Capturing and Reminding of Learning Log), which helps users to share and remind ubiquitous learning experiences. It is expected to powerfully assist the implementation of seamless learning. It proposes that seamless learning can be classified into seven types according to the mobile connectivity, and an empirical study in the context of English vocabulary learning at the university level is introduced. The results show that the test group registered fewer words, but learned more words than the control group, and that the linking in-class and outside-class function showed statistically significant effectiveness when excluding two exceptional cases. The participants predominantly used mobile devices during outside-class learning. Therefore, mobile connectivity is, undoubtedly, contributing to the realization of seamless learning in the sense of linking in-class and outside-class learning.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-981-287-113-8_9",
        "year": "2015"
    },
    {
        "title": "Survival analysis of click logs",
        "abstract": "Click logs from search engines provide a rich opportunity to acquire implicit feedback from users. Patterns derived from the time between a posted query and a click provide information on the ranking quality, reflecting the perceived relevance of a retrieved URL. This paper applies the Kaplan-Meier estimator to study click patterns. The visualization of click curves demonstrates the interaction between the relevance and the rank position of URLs. The observed results demonstrate the potential of using click curves to predict the quality of the top-ranked results.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2348513",
        "year": "2012"
    },
    {
        "title": "Syaritar intelligent system for the detection of illegal logging in the river basin Jeneberang",
        "abstract": "The purpose of this study is to classify the use of intelligent Syaritar system, methods for the detection of illegal logging and changes in forest area in the water sed. The research of intelligent hybrid system methods simulate Syaritar to know and analyze the logging on a sample image of the area of protected forest in the Jeneberang basin river by using sample image pair years 2007 to 2009. On the methods off intelligent system is an digital image, then cropped and will be classified in order to obtain a picture of the two parts of the forest area and the area is not a forest. Futher input parameter in the form of finding the average value of R (Red), G (Green), and B (Blue) for each sample pair the image of the beginning and end. This parameter will be the input for a method of intelligent system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7067328/",
        "year": "2014"
    },
    {
        "title": "Symbolic analysis of the Tau Cell log-domain filter using affine MOSFET models",
        "abstract": "This paper analyses a filter known as the Tau Cell using symbolic methods and shows that the operation of this filter is independent of the magnitude of the input DC offset. This means that the circuit places no restrictions on whether the input DC offset is a sub-threshold current or not. The circuit behaviour predicted from symbolic analysis was observed in similar circuits on a chip fabricated using MOSIS AMI 1.6 Œºm technology. This paper highlights the utility of symbolic analysis and shows that it is a powerful tool for circuit analysis and design.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5774809/",
        "year": "2010"
    },
    {
        "title": "System Log-Based Android Root State Detection",
        "abstract": "Android rooting enables device owners to freely customize their own devices. However, rooting system weakens the security of Android devices and opens the backdoor for malware to obtain privileged access easily. For this reason, some developers have introduced detection mechanisms for sensitive or high-value mobile apps to mitigate the potential security risks. Nevertheless, the existing root prevention and detection methods generally lack universality. In this paper, we studied the existing Android root detection methods and found the both parties have ignored the traces of the relevant behavior in the log. Thus, we proposed the system log based root state detection method. In the method, we directly use the existing log information to find clues to verify the system root state on one hand, on the other hand, to use the triggering features of some special operations to update and enrich the log information. The results show that, even be deliberately erased, some log information is still remained which can be used to verify whether system was rooted or not.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-68542-7_69",
        "year": "2017"
    },
    {
        "title": "System Problem Detection by Mining Process Model from Console Logs",
        "abstract": "Given the explosive growth of large-scale services, manually detecting problems from console logs is infeasible. In the current study, we propose a novel process mining algorithm to discover process model from console logs, and further use the obtained process model to detect anomalies. In brief, the console logs are first parsed into events, and the events from one single session are further grouped to event sequences. Then, a process model is mined from the event sequences to describe the main system behaviors. At last, we use the process model to detect anomalous log information. Experiments on Hadoop File System log dataset show that this approach can detect anomalies from log messages with high accuracy and few false positives. Compared with previously proposed automatic anomaly detection methods, our approach can provide intuitive and meaningful explanations to human operators as well as identify real problems accurately. Furthermore, the process model is easy to understand.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-68210-5_16",
        "year": "2017"
    },
    {
        "title": "Systematic use of trace element distribution patterns in log-log diagrams for plutonic suites",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0016703786900347",
        "year": "1986"
    },
    {
        "title": "Take Command: The System Logging Daemons, syslogd and klog",
        "abstract": "Take command of your log fileslearning to handle those pesky logging daemons.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=349586",
        "year": "2000"
    },
    {
        "title": "Technical and financial analysis of enrichment planting in logging gaps as a potential component of forest management in the eastern Amazon",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0378112707007645",
        "year": "2008"
    },
    {
        "title": "Temperature controlling and data logging using embedded system",
        "abstract": "The goal is to read the analogue signals from temperature sensors and convert it into a digital form and which is available on board as a real time signals. Also this signal can be store in the on chip memory of Micro controller along with the time stamp for further use. This can be achieve with, real time clock, voltage converter to transfer this data to pc or through serial portIn second part software is running on PC side which can request this temperature value in periodic basis and can store the different values with respect to time in PC. We are proposing RS232 serial interface to connect the PC with microcontroller",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1742124",
        "year": "2010"
    },
    {
        "title": "Temporal Network Representation of Event Logs for Improved Performance Modelling in Business Processes",
        "abstract": "Analysing performance of business processes is an important vehicle to improve their operation. Specifically, an accurate assessment of sojourn times and remaining times enables bottleneck analysis and resource planning. Recently, methods to create respective performance models from event logs have been proposed. These works are severely limited, though: They either consider control-flow and performance information separately, or rely on an ad-hoc selection of temporal relations between events. In this paper, we introduce the Temporal Network Representation (TNR) of a log, based on Allen‚Äôs interval algebra, as a complete temporal representation of a log, which enables simultaneous discovery of control-flow and performance information. We demonstrate the usefulness of the TNR for detecting (unrecorded) delays and for probabilistic mining of variants when modelling the performance of a process. In order to compare different models from the performance perspective, we develop a framework for measuring performance fitness. Under this framework, we provide guarantees that TNR-based process discovery dominates existing techniques in measuring performance characteristics of a process. To illustrate the practical value of the TNR, we evaluate the approach against three real-life datasets. Our experiments show that the TNR yields an improvement in performance fitness over state-of-the-art algorithms.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-65000-5_1",
        "year": "2017"
    },
    {
        "title": "Temporal query log profiling to improve web search ranking",
        "abstract": "Temporal information can be leveraged and incorporated to improve web search ranking. In this work, we propose a method to improve the ranking of search results by identifying the fundamental properties of temporal behavior of low-quality hosts and spam-prone queries in search logs and modeling those properties as quantifiable features. In particular, we introduce the concepts of host churn, a measure of changes in host visibility for user queries, and query volatility, a measure of semantic instability of query results, and propose the methods for construction of temporal profiles from search query logs that can be used for estimation of a set of features based on the introduced concepts. The utility of the proposed concepts has been experimentally demonstrated for two language-independent search tasks: the regression-based ranking of search results and a novel classification problem of detecting spam-prone queries introduced in this work.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1871583",
        "year": "2010"
    },
    {
        "title": "Test software evaluation using data logging [missile testing]",
        "abstract": "As the missile technology and capability become increasingly sophisticated, the test system software has become more complex. Because of this, the degree of difficulty has also increased in the area of verification and validation of the test software. This paper proposes that a test software evaluation tool (TSET) be developed to use data logged during the execution of test programs, minimizing the traditional manual code review, to perform software evaluation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1243646/",
        "year": "2003"
    },
    {
        "title": "The ‚ÄúWeb Log‚Äù",
        "abstract": "By this point in the book, you should know more than enough about blogging to decide which solution to investigate. Obviously, if you bought this book, you‚Äôve decided that you‚Äôre ready to enter what some see as a world of pain and get your hands dirty with some code.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4302-0127-4_1",
        "year": "2006"
    },
    {
        "title": "The Algorithm of Data Preprocessing in Web Log Mining Based on Cloud Computing",
        "abstract": "In the structure of distributed cluster server, web log data mining model based on data warehouse has the defects of bottlenecks in the network and computing, transmission errors caused by the large data transmission, the paper makes use of the advantages of cloud computing, distributed processing and virtualization technology, designs a type of Web log analysis platform based on cloud computing Hadoop cluster framework, finally, a new hybrid algorithm of distributed procession in the cloud computing environment is proposed. To further verify the efficiency of the platform, we use the improved data pretreatment algorithm on the platform of processing large number of Web logs, experimental results show that it can improve the efficiency of Web data mining.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-34910-2_54",
        "year": "2013"
    },
    {
        "title": "The Algorithm Research Based on Parallel Web Log Mining",
        "abstract": "A data preprocessing Model is discussed in this paper, which included processes of source data preparing, data cleaning, user recognition, dialog recognition, path supplementation and data formatting. In order to promote compute efficiency and apply parallel technique to the algorithm, a parallel mining algorithm fitted to web log is put forward. And in the last, the feasibility of algorithm is verified by an instance.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5600879/",
        "year": "2010"
    },
    {
        "title": "The Application and Improvement of ID3 Algorithm in WEB Log Data Mining",
        "abstract": "Data mining comes into being as a new area of research, WEB data mining technology is known as one of the major information processing technology in the future. ID3 algorithm is a often used classical algorithm in data mining technology, which is mainly applied to the implementation of data mining. It always creates the smallest tree structure and is proved that the system design has good effect to transaction analysis of log files by proofing instances, this system is effective in the log files analysis and improvement of ID3 algorithm.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-981-10-1536-6_75",
        "year": "2016"
    },
    {
        "title": "The application of matrix Apriori algorithm in web log mining",
        "abstract": "With the advent of the big data era, data mining technology has gradually become mature, association rules analysis is also applied in many fields. Web log mining is an important way to do some personalized services and achieve Web personalize. Apriori algorithm is a classical algorithm of association rules, but it has a lot of shortcomings. In recent years, the improvement about Apriori algorithm emerges in endlessly. In this paper, we mainly discuss the application of Matrix Apriori algorithm in Web log mining based on matrix storage. First, we analyze the improvements of Matrix Apriori and describe the process of the algorithm. We make some comparisons of several association rules algorithms. Then, Matrix Apriori algorithm is applied to Sogou search log and shoes website search log. Finally, according to the results of the Web log mining, we can make personalized recommendation and optimize site settings.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/8078821/",
        "year": "2017"
    },
    {
        "title": "The Application of User Log for Online Business Environment using Content-Based Image Retrieval System",
        "abstract": "Over the past few years, inter-query learning has gained much attention in the research and development of content-based image retrieval (CBIR) systems. This is largely due to the capability of inter-query approach to enable learning from the retrieval patterns of previous query sessions. However, much of the research works in this field have been focusing on analyzing image retrieval patterns stored in the database. This is not suitable for a dynamic environment such as the World Wide Web (WWW) where images are constantly added or removed. A better alternative is to use an image's visual features to capture the knowledge gained from the previous query sessions. Based on the previous work (Chung et al., 2006), the aim of this paper is to propose a framework of inter-query learning for the WWW-CBIR systems. Such framework can be extremely useful for those online companies whose core business involves providing multimedia content-based services and products to their customers",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4031661/",
        "year": "2006"
    },
    {
        "title": "The Combined Influence Assessment of Plantation Logging on Environment-Resource-Cost System Based on Grey Cluster Method",
        "abstract": "A combined influence assessment model is established based on grey cluster method in this paper to analyze the combined influence of plantation logging on the environment-resource-cost system. The modeling results show that in the process of skidding and hauling, the higher the degree of mechanization, the less the compositive consumption. Moreover, the compositive consumption of small equipment is lower than that of large equipment under the same condition. As a result, the proper equipment should be chosen according to environmental condition during the process of plantation logging, and traditional manual work should be avoided as much as possible. Furthermore, the selection of logging equipments should follow the rules of reasonable and sufficient application. Efficient and small equipments are suitable choice such as using farm vehicles instead of trucks in hauling.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5748937/",
        "year": "2011"
    },
    {
        "title": "The cost of checkpointing, logging and recovery for the mobile agent systems",
        "abstract": "The reliable execution of a mobile agent is a very important design issue to build a mobile agent system and many fault-tolerant schemes have been proposed. Hence, we present the experimental evaluation of the performance of the fault-tolerant schemes for the mobile agent environment. Our evaluation focuses on the checkpointing schemes and deals with the cooperating agents.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1185617/",
        "year": "2002"
    },
    {
        "title": "The Data-Logging System of the Trigger and Data Acquisition for the ATLAS Experiment at CERN",
        "abstract": "The ATLAS experiment is getting ready to observe collisions between protons at a centre of mass energy of 14 TeV. These will be the highest energy collisions in a controlled environment to-date, to be provided by the Large Hadron Collider at CERN by mid 2008. The ATLAS Trigger and Data Acquisition (TDAQ) system selects events online in a three level trigger system in order to keep those events promising to unveil new physics at a budgeted rate of ~200 Hz for an event size of ~1.5 MB. This paper focuses on the data-logging system on the TDAQ side, the so-called ldquoSub-Farm Outputrdquo (SFO) system. It takes data from the third level trigger, and it streams and indexes the events into different files, according to each event's trigger path. The data files are moved to CASTOR, the central mass storage facility at CERN. The final TDAQ data-logging system has been installed using 6 Linux PCs, holding in total 144 disks of 500 GB each, managed by three RAID controllers on each PC. The data-writing is managed in a controlled round-robin way among three independent filesystems associated to a distinct set of disks, managed by a distinct RAID controller. This novel design allows fast I/O, which together with a high speed network permits to minimize the number of SFO nodes. We report here on the functionality and performance requirements on the system, our experience with commissioning it and on the performance achieved.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4696569/",
        "year": "2008"
    },
    {
        "title": "The Dependence of Log Periodic Dipole Antenna (LPDA) and e-CALLISTO Software to Determine the Type of Solar Radio Burst (I -V)",
        "abstract": "Solar radio burst originated at the layer of the atmosphere where the Geo-effective disturbance occurred which energy will be released in solar flares and Coronal Mass Ejections (CMEs) will be launched. Solar Radio Burst can be divided into 5 types and determined by using the Log Periodic Dipole Antenna (LPDA) and e-CALLISTO system. The LPDA was set up in a 45-870 MHz range in frequency and has maximum boom length 5.45m. Besides that, it has minimum scale factor, œÑ=0.76 and maximum at œÑ=0.98. We put some effort to construct suitable with designs, high specification and practical enough with the size of boom length as the conclusion the scale factor that suitable with this design is 0.8118 as a directivity of an antenna. LPDA has 19 elements which using two (2) aluminium rod with 7.01dB gain. The antenna has a function to receive the signals then connected to the low noise amplifier and e-CALLISTO spectrometer completes it as a system. A CALLISTO (Compound Astronomical Low-Cost- Low-Frequency Instrument for Spectroscopy Transportable Observatory) spectrometer was used to figure out the dynamic of solar corona which in metric and decimetric wavelength radio observation and the main objective of this study was to study how the solar radio burst can be detected by using LPDA (Malaysia) and e-CALLISTO (ETH Zurich, Switzerland) which were set up in a different location.In this paper, the potential of Malaysia be one of the candidates to contribute a good data will be highlighted and we will focus more on performance evaluation and visualization data.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7504039/",
        "year": "2016"
    },
    {
        "title": "The design and implement of the centralized log gathering and analysis system",
        "abstract": "Logs generated by network devices and systems provide important information for network management. In this paper, we describe a centralized syslog system which gathers and analyzes log messages from a number of routers, switches and firewalls. The gathered logs are filtered and categorized with regular expression, and finally stored in a MySQL database with format. Through the statistics analysis, feature-based detection on security events, the system can effectively find out abnormal behavior of network devices and ensure the network security. Some methods are found out to allow us to check if the network behavior is unusual. These perspective methods also provide the basis of network management and security strategy design for administrators, thereby strengthen further network management.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6272772/",
        "year": "2012"
    },
    {
        "title": "The design and implementation of a log-structured file system",
        "abstract": "This paper presents a new technique for disk storage management called a log-structured file system. A log-structured file system writes all modifications to disk sequentially in a log-like structure, thereby speeding up both file writing and crash recovery. The log is the only structure on disk; it contains indexing information so that files can be read back from the log efficiently. In order to maintain large free areas on disk for fast writing, we divide the log intosegmentsand use a segment cleaner to compress the live information from heavily fragmented segments. We present a series of simulations that demonstrate the efficiency of a simple cleaning policy based on cost and benefit. We have implemented a prototype log-structured file system called Sprite LFS; it outperforms current Unix file systems by an order of magnitude for small-file writes while matching or exceeding Unix performance for reads and large writes. Even when the overhead for cleaning is included, Sprite LFS can use 70% of the disk bandwidth for writing, whereas Unix file systems typically can use only 5‚Äì10%.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=146943",
        "year": "1992"
    },
    {
        "title": "The Design of efficient initialization and crash recovery for log-based file systems over flash memory",
        "abstract": "While flash memory has been widely adopted for storage systems for various embedded systems, issues of performance and reliability have started receiving growing attention in recent years. How to provide efficient roll back and quick mounting for flash-memory file systems has become an important research topic in recent years, in addition to the work on effective garbage collection and superb runtime performance. Such an observation motivates our work on the investigation of efficient initialization and crash recovery of flash-memory file systems based on log structures. A methodology is proposed for the acceleration of mounting and crash recovery for log-based file systems. A system prototype based on a well-known flash-memory file system, YAFFS, was implemented with performance evaluation. Experimental results show that the proposed methodology can reduce mounting time significantly, regardless of whether the file system is properly unmounted.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1210600",
        "year": "2006"
    },
    {
        "title": "The Design of Log Analysis Mechanism in SDN Quarantined Network System",
        "abstract": "Due to the development of information and communication technologies, there is a growing dependence on the computer. Based on the accidents occurred in the country, this dependence means that if people have a problem in terms of cyber security, it will cause great confusion nationally. Even though there are a lot of security solutions to prevent it, security incidents have continued to occur because the number of solutions confined to a specific function have duplication and vulnerability. In order to overcome the limitation of the information security environment, it has been proposed an inspection system using the SDN of the new concept. In this paper, we will propose log analysis system to be applied within the inspection system using these SDN.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-981-10-0281-6_48",
        "year": "2015"
    },
    {
        "title": "The development of web log mining based on improve-K-means clustering analysis",
        "abstract": "The main objective of Web log mining is to extract interesting patterns from the Web access to records. Web log mining has been successfully applied to a personalized recommendation system improvement and business intelligence. This paper presents the development of Web log mining based on improve-K-Means clustering analysis. K-Means clustering algorithm is analyzed and the paper proposes effective index of the K-Means clustering algorithm and verified by experiment, and proposes automatically selected based on the initial cluster centers that this selection method can reduce the outlier and improve the clustering results.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-30223-7_97",
        "year": "2012"
    },
    {
        "title": "The effects of reduced-impact logging practices on soil animal communities in the Deramakot Forest Reserve in Borneo",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0929139313001959",
        "year": "2014"
    },
    {
        "title": "The Effects of Web Logs and the Semantic Web on Autonomous Web Agents",
        "abstract": "Search engines exploit the Web‚Äôs hyperlink structure to help infer information content. The new phenomenon of personal Web logs, or ‚Äòblogs‚Äô, encourage more extensive annotation of Web content. If their resulting link structures bias the Web crawling applications that search engines depend upon, there are implications for another form of annotation rapidly on the rise, the Semantic Web. We conducted a Web crawl of 160 000 pages in which the link structure of the Web is compared with that of several thousand blogs. Results show that the two link structures are significantly different. We analyse the differences and infer the likely effect upon the performance of existing and future Web agents. The Semantic Web offers new opportunities to navigate the Web, but Web agents should be designed to take advantage of the emerging link structures, or their effectiveness will diminish.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-30182-0_68",
        "year": "2004"
    },
    {
        "title": "The Failure Prediction of Cluster Systems Based on System Logs",
        "abstract": "The failure prediction of cluster systems is an effective approach to improve the reliability of the cluster systems, which is becoming a new research hotspot of high performance computing, especially with the growth of cluster systems and applications both in scale and complexity. A classification sequential rule model is proposed to predict cluster system failures. The system logs of BlueGene/L, Red Storm, and Spirit are used as experimental datasets to predict cluster system failures. The results show that sequential rule approach outperforms SVM and HSMM in terms of precision and F-measure in 5hr prediction window, and in 1hr or 12hr prediction window, sequential rules, SVM and HSMM have their own strengths and weaknesses respectively.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-39787-5_44",
        "year": "2013"
    },
    {
        "title": "The Log-polar Image Representation in Pattern Recognition Tasks",
        "abstract": "This paper is a review of works about the use of the log-polar image model for pattern recognition purposes. Particular attention is paid to the rotation- and scale-invariant pattern recognition problem, which is simplified by the log-polar mapping. In spite of this advantage, ordinary translations become a complicated image transform in the log-polar domain. Two approaches addressing the estimation of translation, rotation and scaling are compared. One of them, developed by the authors, takes advantage of the principles of the active vision paradigm.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-44871-6_119",
        "year": "2003"
    },
    {
        "title": "The prevalence of kleptographic attacks on discrete-log based cryptosystems",
        "abstract": "The notion of a Secretly Embedded Trapdoor with Universal Protection (SETUP) and its variations on attacking black-box cryptosystems has been recently introduced. The basic definitions, issues, and examples of various setup attacks (called Kleptographic attacks) have also been presented. The goal of this work is to describe a methodological way of attacking cryptosystems which exploits certain relations between cryptosystem instances which exist within cryptosystems. We call such relations ‚Äúkleptograms‚Äù. The identified kleptogram is used as the base for searching for a setup.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/BFb0052241",
        "year": "1997"
    },
    {
        "title": "The RP-HPLC measurement and QSPR analysis of log Po/w values of several Pt (II) complexes",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0162013406000468",
        "year": "2006"
    },
    {
        "title": "The use of a personal digital assistant Patient Encounter Log System to track procedures performed by students during a mandatory emergency medicine ‚Ä¶",
        "abstract": "ctives: Before graduation, medical schools expect their students to successfully perform basic procedures either on a patient or on a model. In addition, many medical schools are developing systems that track the type and number of procedures performed by their students. This study describes the number of times certain basic procedures were performed by medical students rotating on a mandatory emergency medicine clerkship as documented in their personal digital assistant (PDA) Patient Encounter Log System (PELS).Methods: Starting in July 2003, all fourth-year medical students began rotating through a new 4-week emergency medicine clerkship. During this clerkship, students were taught and performed the following basic procedures: airway maintenance (AM), central venous catheter insertion (CVCI), intravenous catheter insertion (IVI), laceration repair (LR), and lumbar puncture (LP). Students were expected to document all performed procedures into their PDA-based PELS. PELS cumulates the type and number of procedures performed. After completion of the clerkship, every student was expected to download their PELS data into the central base run by the medical school's Center for Research in Medical Education. Data were collected from the first 5 emergency medicine clerkships.Results: Fifty-five students downloaded PELS data (Table).Conclusion: A mandatory fourth-year emergency medicine clerkship provides medical students the opportunity to perform many basic procedures at least once before graduation. A PDA system that facilitates students' documentation of basic procedures can provide data necessary for the successful completion of medical school. Future data from this instrument can also be used to assess the clinical and procedural experience of a fourth-year emergency medicine clerkship compared with third-year clerkships.Table, abstract 154..\tNo. of Times Procedure Performed\t\tProcedure\t0\t1\t2\t3\t4\t5+\tTotal Students\t% at Least 1AM\t1\t13\t19\t10\t8\t4\t55\t98.2CVCI\t15\t27\t7\t4\t1\t1\t55\t90.9IVI\t5\t23\t9\t6\t5\t7\t55\t98.2LR\t1\t6\t13\t9\t6\t20\t55\t98.2LP\t12\t24\t13\t4\t1\t1\t55\t78.2",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0196064404008820",
        "year": "2004"
    },
    {
        "title": "The use of OPAC in a large academic library: a transactional log analysis study of subject searching",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0099133307000377",
        "year": "2007"
    },
    {
        "title": "Three centuries of categorical data analysis: Log-linear models and maximum likelihood estimation",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0378375807001073",
        "year": "2007"
    },
    {
        "title": "Three Views of Log Trace Triaging",
        "abstract": "This paper extends previous work on execution trace triaging. We examine the problem of trace triaging along three of the four views used in the study of temporal properties, namely the automata-theoretic view, the temporal logic view and the set-theoretic view. For each case, we propose several partitions of universe of possible traces into equivalence classes, which follow naturally from the chosen view and form the basis for trace triaging.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-51966-1_12",
        "year": "2017"
    },
    {
        "title": "Time series analysis of a Web search engine transaction log",
        "abstract": "In this paper, we use time series analysis to evaluate predictive scenarios using search engine transactional logs. Our goal is to develop models for the analysis of searchers‚Äô behaviors over time and investigate if time series analysis is a valid method for predicting relationships between searcher actions. Time series analysis is a method often used to understand the underlying characteristics of temporal data in order to make forecasts. In this study, we used a Web search engine transactional log and time series analysis to investigate users‚Äô actions. We conducted our analysis in two phases. In the initial phase, we employed a basic analysis and found that 10% of searchers clicked on sponsored links. However, from 22:00 to 24:00, searchers almost exclusively clicked on the organic links, with almost no clicks on sponsored links. In the second and more extensive phase, we used a one-step prediction time series analysis method along with a transfer function method. The period rarely affects navigational and transactional queries, while rates for transactional queries vary during different periods. Our results show that the average length of a searcher session is approximately 2.9 interactions and that this average is consistent across time periods. Most importantly, our findings shows that searchers who submit the shortest queries (i.e., in number of terms) click on highest ranked results. We discuss implications, including predictive value, and future research.",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0306457308000800",
        "year": "2009"
    },
    {
        "title": "Topic Analysis of Web User Behavior Using LDA Model on Proxy Logs",
        "abstract": "We propose a web user profiling and clustering framework based on LDA-based topic modeling with an analogy to document analysis in which documents and words represent users and their actions. The main technical challenge addressed here is how to symbolize web access actions, by words, that are monitored through a web proxy. We develop a hierarchical URL dictionary generated from Yahoo! Directory and a cross-hierarchical matching method that provides the function of automatic abstraction. We apply the proposed framework to 7500 students in Osaka University. The results include, for example, 24 topics such as ‚ÄùTechnology Oriented‚Äù, ‚ÄùJob Hunting‚Äù, and ‚ÄùSNS-addict.‚Äù The results reflect the typical interest profiles of University students, while perplexity analysis is employed to confirm the optimality of the framework.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-20841-6_43",
        "year": "2011"
    },
    {
        "title": "Towards Event Log Querying for Data Quality",
        "abstract": "Process mining is, by now, a well-established discipline focussing on process-oriented data analysis. As with other forms of data analysis, the quality and reliability of insights derived through analysis is directly related to the quality of the input (garbage in - garbage out). In the case of process mining, the input is an event log comprised of event data captured (in information systems) during the execution of the process. It is crucial then that the event log be treated as a first-class citizen. While data quality is an easily understood concept little effort has been directed towards systematically detecting data quality issues in event logs. Analysts still spend a large proportion of any project in ‚Äòdata cleaning‚Äô, often involving manual and ad hoc tasks, and requiring more than one tool. While there are existing tools and languages that query event logs, the problem of different approaches for different log imperfections remains. In this paper we take the first steps to developing QUELI (Querying Event Log for Imperfections) a log query language that provides direct support for detecting log imperfections. We develop an approach that identifies capabilities required of QUELI and illustrate the approach by applying it to 5 of the 11 event log imperfection patterns described in [29]. We view this as a first step towards operationalising systematic, automated support for log cleaning.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-02610-3_7",
        "year": "2018"
    },
    {
        "title": "Towards More Effective Solution Retrieval in IT Support Services Using Systems Log",
        "abstract": "Technical support agents working in the IT support services field resolve IT problems. They are often faced with the daunting task of identifying the correct solution document through a search system from large corpora of IT support documents. Based on the observation that system logs may contain critical information for identifying the root cause of IT problems, we explore the idea of automatic query expansion by using system logs as a bridge to link queries with the most relevant documents. Given the original query from a user such as a technical support agent, an intermediate query is first formed by adding key terms extracted from system logs using domain-specific rules. Based on topic models, further key terms are selected from corpora of IT support documents, which are combined with the intermediate query to form the final query. Our experimental results show that expanding queries using system logs together with topic models yields better performance in retrieving relevant IT support documents than using topic models only.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-46295-0_52",
        "year": "2016"
    },
    {
        "title": "Towards Personalized Context-Aware Recommendation by Mining Context Logs through Topic Models",
        "abstract": "The increasing popularity of smart mobile devices and their more and more powerful sensing ability make it possible to capture rich contextual information and personal context-aware preferences of mobile users by user context logs in devices. By leveraging such information, many context-aware services can be provided for mobile users such as personalized context-aware recommendation. However, to the best knowledge of ours, how to mine user context logs for personalized context-aware recommendation is still under-explored. A critical challenge of this problem is that individual user‚Äôs historical context logs may be too few to mine their context-aware preferences. To this end, in this paper we propose to mine common context-aware preferences from many users‚Äô context logs through topic models and represent each user‚Äôs personal context-aware preferences as a distribution of the mined common context-aware preferences. The experiments on a real-world data set contains 443 mobile users‚Äô historical context data and activity records clearly show the approach is effective and outperform baselines in terms of personalized context-aware recommendation.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-30217-6_36",
        "year": "2012"
    },
    {
        "title": "Towards User Context Enhance Search Engine Logs Mining",
        "abstract": "Making search engines responsive to human needs requires understanding user intention when submitting a query. Intention, context and situation are intimately connected [8]. Thus context modelling is paramount when mining search engine logs but the process should be sistematized and standarized for the future generation of autonomous data mining components. In this paper, we propose to integrate context as metadata represented in PMML. The complete knowledge discovery process would benefit from the metadata and in particular final and intermediate evaluations can use this information for interpretation. The paper also presents results of integration of GUMO ontology to conceptualize user context to improve interpretation of query mining on the weblogs of the site search engine.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-72575-6_15",
        "year": "2007"
    },
    {
        "title": "Towards Visual Analysis of Usability Test Logs Using Task Models",
        "abstract": "In this paper we discuss techniques on how task models can enhance visualization of the usability test log. Evaluation of the usability tests is a traditional method in user centered design. It is a part of the methodology for design of usable products. We developed a tool for visualization of usability logs that uses the hierarchical structure of the task models to group and visualize observers‚Äô annotations. This way of visualization is very natural and allows investigation of the test dynamics and comparison of participant‚Äôs behavior in various parts of the test. We also describe methods of visualization of multiple logs that allow for comparison of the test results between participants. For that purpose we present a new visualization method based on alignment of the visual representations of tasks, where binding between the task model and the log visualization is used. Finally, we present an evaluation of our tool on two usability tests, which were conducted in our laboratory and we discuss observed findings.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-70816-2_3",
        "year": "2007"
    },
    {
        "title": "Trace rewriting: Computing normal forms in time O(n log n)",
        "abstract": "We develop an O(n log n) algorithm for computing normal forms in the case of finite weight-reducing trace rewriting systems with connected left-hand sides. The time complexity of previously known algorithms solving this problem has been square time in the worst-case.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-60922-9_23",
        "year": "1996"
    },
    {
        "title": "Truncated Log Shaped Type Software Reliability Growth Model",
        "abstract": "Due to the large scale application of software systems, software reliability plays an important role in software developments. In this paper, a software reliability growth model (SRGM) is proposed. The testing time on the right is truncated in this model. The intensity function, mean-value function, reliability of the software, estimation of parameters and the special applications of this model are discussed using real data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-32645-5_78",
        "year": "2012"
    },
    {
        "title": "Trusted Log Management System",
        "abstract": "With the many accounting scandals that have been reported in companies around the world, the need for internal control has steadily grown. Many different kinds of logs exist, and storing them over the long-term is necessary to realize internal control systems based on logs. Previously, we proposed a low-cost system to store logs semi-permanently using a Virtual Large Scale Disk. However, as this log system cannot guarantee the transfer of trusted logs across a vulnerable transfer path, it is unacceptable for use in digital forensics.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.2991/978-94-91216-71-8_5",
        "year": "2012"
    },
    {
        "title": "Tuning Your Java Virtual Machine: Finding Your Ideal JVM Settings Through Metrics Log Analysis",
        "abstract": "The Java Virtual Machine underlying the ColdFusion server is probably one of the least understood things about ColdFusion. Yet this mysterious ‚Äúblack box‚Äù is key to keeping ColdFusion operating efficiently. In the ColdFusion world, Mike Brunt is one of the best-known experts on taming this beast, and he presents many insights in this article.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4302-7214-4_8",
        "year": "2010"
    },
    {
        "title": "A Tool for Integrating Log and Video Data for Exploratory Analysis and Model Generation",
        "abstract": "Analysis of students‚Äô log data to understand their process as they solve problems is an essential part of educational technology research. Models of correct and buggy student behavior can be generated from this log data and used as a basis for intelligent feedback. Another important technique for understanding problem-solving process is video protocol analysis, but historically, this has not been well integrated with log data. In this paper, we describe a tool to 1) facilitate the annotation of log data with information from video data, and 2) automatically generate models of student problem-solving process that include both video and log data. We demonstrate the utility of the tool with analysis of student use of a teachable robot system for geometry.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07221-0_9",
        "year": "2014"
    },
    {
        "title": "A Top Down Algorithm for Mining Web Access Patterns from Web Logs",
        "abstract": "This paper proposes a new algorithm, called TAM WAP(the shorthand forTop down Algorithm for Mining Web AccessPatterns), to mine interesting WAP from Web logs. TAM WAP searches the P tree database in the top down manner to mine WAP. By selectively building intermediate data according to the features of current area to be mined, it can avoid stubbornly building intermediate data for each step of mining process. The experiments for both real data and artificial data show that our algorithm outperforms conventional methods.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11430919_99",
        "year": "2005"
    },
    {
        "title": "A Type System for Counting Logs of Multi-threaded Nested Transactional Programs",
        "abstract": "We present a type system to estimate an upper bound for the resource consumption of nested and multi-threaded transactional programs. The resource is abstracted as transaction logs. In comparison to our previous work on type and effect systems for Transactional Featherweight Java, this work exploits the natural composition of thread creation to give types to sub-terms. As a result, our new type system is simpler and more effective than our previous one. More important, it is more precise than our previous type system. We also show a type inference algorithm that we have implemented in a prototype tool.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-28034-9_21",
        "year": "2016"
    },
    {
        "title": "A User-Friendly Log Viewer for Storage Systems",
        "abstract": "System log files contains messages emitted from several modules within a system and carries valuable information about the system state such as device status and error conditions and also about the various tasks within the system such as program names, execution path, including function names and parameters, and the task completion status. For customers with remote support, the system collects and transmits these logs to a central enterprise repository, where these are monitored for alerts, problem forecasting, and troubleshooting.Very large log files limit the interpretability for the support engineers. For an expert, a large volume of log messages may not pose any problem; however, an inexperienced person may get flummoxed due to the presence of a large number of log messages. Often it is desired to present the log messages in a comprehensive manner where a person can view the important messages first and then go into details if required.In this article, we present a user-friendly log viewer where we first hide the unimportant or inconsequential messages from the log file. A user can then click a particular hidden view and get the details of the hided messages. Messages with low utility are considered inconsequential as their removal does not impact the end user for the aforesaid purpose such as problem forecasting or troubleshooting. We relate the utility of a message to the probability of its appearance in the due context. We present machine-learning-based techniques that computes the usefulness of individual messages in a log file. We demonstrate identification and discarding of inconsequential messages to shrink the log size to acceptable limits. We have tested this over real-world logs and observed that eliminating such low value data can reduce the log files significantly (30% to 55%), with minimal error rates (7% to 20%). When limited user feedback is available, we show modifications to the technique to learn the user intent and accordingly further reduce the error.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2846101",
        "year": "2016"
    },
    {
        "title": "A vision-based postproduction tool for footage logging, analysis, and annotation",
        "abstract": "We report a new method for logging and annotating video footage directed towards the needs of professional postproduction and archivist end users. SALSA‚ÄîSemi-Automated Logging with Semantic Annotation‚Äîis a hybrid system that uses automated footage analysis for cut detection and camera movement classification, and a stenographic-like keyboard input system for the logging of higher-level semantic information. Output is presented both in standard printed log form, with the addition of mosaic visual representations of shots, and in a fully searchable database. Experimental comparisons of SALSA with conventional hand analysis show a significant increase in the logger‚Äôs speed with no reduction in accuracy or semantic detail.",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S1524070305000123",
        "year": "2005"
    },
    {
        "title": "A Visual Approach to Spot Statistically-Significant Differences in Event Logs Based on Process Metrics",
        "abstract": "This paper addresses the problem of comparing different variants of the same process. We aim to detect relevant differences between processes based on what was recorded in event logs. We use transition systems to model behavior and to highlight differences. Transition systems are annotated with measurements, used to compare the behavior in the variants. The results are visualized as transitions systems, which are colored to pinpoint the significant differences. The approach has been implemented in ProM, and the implementation is publicly available. We validated our approach by performing experiments using real-life event data. The results show how our technique is able to detect relevant differences undetected by previous approaches while it avoids detecting insignificant differences.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-39696-5_10",
        "year": "2016"
    },
    {
        "title": "A web interface for XALT log data analysis",
        "abstract": "XALT is a job-monitoring tool to collect accurate, detailed, and continuous job level and link-time data on all MPI jobs running on a computing cluster. Due to its usefulness and complementariness to other system logs, XALT has been deployed on Stampede at Texas Advanced Computing Center and other high performance computing resources around the world. The data collected by XALT can be extremely valuable to help resource providers understanding resources usages and identify patterns and insights for future improvements. However, the volume of data collected by XALT grows quickly over time on large system and presents challenges for access and analysis. In this paper, we describe development of a prototype tool to analyze and visualize XALT data. The application utilizes Spark for efficient data processing over large volume of log data and R for interactive visualization of the results over the web. The application provides an easy to use interface for users to conveniently share and communicate executable usage and patterns without prerequisite knowledge on big data technology. In this paper, we detail the features of this tool, current development status, performance evaluation and exemplar use cases.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2949560",
        "year": "2016"
    },
    {
        "title": "A well logging technique for the in situ determination of 90Sr",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0029554X8090782X",
        "year": "1980"
    },
    {
        "title": "Abducing Compliance of Incomplete Event Logs",
        "abstract": "The capability to store data about business processes execution in so-called Event Logs has brought to the diffusion of tools for the analysis of process executions and for the assessment of the goodness of a process model. Nonetheless, these tools are often very rigid in dealing with Event Logs that include incomplete information about the process execution. Thus, while the ability of handling incomplete event data is one of the challenges mentioned in the process mining manifesto, the evaluation of compliance of an execution trace still requires an end-to-end complete trace to be performed. This paper exploits the power of abduction to provide a flexible, yet computationally effective, framework to deal with different forms of incompleteness in an Event Log. Moreover it proposes a refinement of the classical notion of compliance into strong and conditional compliance to take into account incomplete logs.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-49130-1_16",
        "year": "2016"
    },
    {
        "title": "Abstracting log lines to log event types for mining software system logs",
        "abstract": "Log files contain valuable information about the execution of a system. This information is often used for debugging, operational profiling, finding anomalies, detecting security threats, measuring performance etc. The log files are usually too big for extracting this valuable information manually, even though manual perusal is still one of the more widely used techniques. Recently a variety of data mining and machine learning algorithms are being used to analyze the information in the log files. A major road block for the efficient use of these algorithms is the inherent variability present in every log line of a log file. Each log line is a combination of a static message type field and a variable parameter field. Even though both these fields are required, the analyses algorithm often requires that these be separated out, in order to find correlations in the repeating log event types. This disentangling of the message and parameter fields to find the event types is called abstraction of log lines. Each log line is abstracted to a unique ID or event type and the dynamic parameter value is extracted to give an insight on the current state of the system. In this paper we present a technique based on a clustering technique used in the Simple Log file Clustering Tool for log file abstraction. This solution is especially useful when we don't have access to the source code of the application or when the lines in the log file do not conform to a rigid structure. We evaluated our implementation on log files from the Virtual Computing Lab, a cloud computer management system at North Carolina State University, and abstracted it to 727 unique event types.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/5463281/",
        "year": "2010"
    },
    {
        "title": "Accountable logging in operating systems",
        "abstract": "In this paper, study how to achieve accountable logging for operating system using the flow-net logging and its implementation in current operating system such as Linux. We demonstrate that the flow-net logging technique is capable of preserving event relationship. The performance for the flow-net logging implementation in Linux operation system is evaluated.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7249469/",
        "year": "2015"
    },
    {
        "title": "Actively Mining Search Logs for Diverse Tags",
        "abstract": "Social tagging has become a very important mechanism for organizing information on the Web. Usually, people tag a web page manually, just as what they do on a social bookmarking web site. In this paper, we will demonstrate a brand-new perspective - tagging web pages automatically by mining search logs. In order to keep diversity, we first classify web queries into different categories and then extract tags from queries to depict each category. Thereafter we describe a web page with all queries which are related to this page, and finally we get the recommended tags for each web page after mapping the related queries into corresponding diverse tags. The experiments conducted on a real search log show that our method can dig out accurate and meaningful diverse tags for web pages more effectively.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-35341-3_48",
        "year": "2012"
    },
    {
        "title": "Adaptable technique for collecting MWD density logging research data using Windows software",
        "abstract": "The goal of the Density Project at Develco is to further develop gamma-gamma technology and support a commercial instrument that measures bulk density of Earth formations while drilling. A large database is automatically generated by the data collection system developed in the R\u0026D department. This data collection system also performs several quality assurance functions for the data as they are collected. Commercial software applications are used in lieu of software generated in-house. Real-time analysis can be accomplished with any software application that supports the Dynamic Data Exchange (DDE) features of Windows applications. Mathcad was used initially to develop an algorithm for locating photo peaks and subsequently fitting to the peak a Gaussian function.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/301360/",
        "year": "1992"
    },
    {
        "title": "Agent-Mining of Grid Log-Files: A Case Study",
        "abstract": "Grid monitoring requires analysis of large amounts of log files across multiple domains. An approach is described for automated extraction of job-flow information from large computer grids, using software agents and genetic computation. A prototype was created as a first step towards communities of agents that will collaborate to learn log-file structures and exchange knowledge across organizational domains.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-36288-0_15",
        "year": "2013"
    },
    {
        "title": "ALERT-ID: Analyze Logs of the Network Element in Real Time for Intrusion Detection",
        "abstract": "The security of the networking infrastructure (e.g., routers and switches) in large scale enterprise or Internet service provider (ISP) networks is mainly achieved through mechanisms such as access control lists (ACLs) at the edge of the network and deployment of centralized AAA (authentication, authorization and accounting) systems governing all access to network devices. However, a misconfigured edge router or a compromised user account may put the entire network at risk. In this paper, we propose enhancing existing security measures with an intrusion detection system overseeing all network management activities. We analyze device access logs collected via the AAA system, particularly TACACS+, in a global tier-1 ISP network and extract features that can be used to distinguish normal operational activities from rogue/anomalous ones. Based on our analyses, we develop a real-time intrusion detection system that constructs normal behavior models with respect to device access patterns and the configuration and control activities of individual accounts from their long-term historical logs and alerts in real-time when usage deviates from the models. Our evaluation shows that this system effectively identifies potential intrusions and misuses with an acceptable level of overall alarm rate.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-33338-5_15",
        "year": "2012"
    },
    {
        "title": "Aligning Event Logs and Declarative Process Models for Conformance Checking",
        "abstract": "Process mining can be seen as the ‚Äúmissing link‚Äù between data mining and business process management. Although nowadays, in the context of process mining, process discovery attracts the lion‚Äôs share of attention, conformance checking is at least as important. Conformance checking techniques verify whether the observed behavior recorded in an event log matches a modeled behavior. This type of analysis is crucial, because often real process executions deviate from the predefined process models. Although there exist solid conformance checking techniques for procedural models, little work has been done to adequately support conformance checking for declarative models. Typically, traces are classified as fitting or non-fitting without providing any detailed diagnostics. This paper aligns event logs and declarative models, i.e., events in the log are related to activities in the model if possible. The alignment provides then sophisticated diagnostics that pinpoint where deviations occur and how severe they are. The approach has been implemented in ProM and has been evaluated using both synthetic logs and real-life logs from Dutch municipalities.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-32885-5_6",
        "year": "2012"
    },
    {
        "title": "Alternative Approach to Tree-Structured Web Log Representation and Mining",
        "abstract": "More recent approaches to web log data representation aim to capture the user navigational patterns with respect to the overall structure of the web site. One such representation is tree-structured log files which is the focus of this work. Most existing methods for analyzing such data are based on the use of frequent sub tree mining techniques to extract frequent user activity and navigational paths. In this paper we evaluate the use of other standard data mining techniques enabled by a recently proposed structure preserving flat data representation for tree-structured data. The initially proposed framework was adjusted to better suit the web log mining task. Experimental evaluation is performed on two real world web log datasets and comparisons are made with an existing state-of-the-art classifier for tree-structured data. The results show the great potential of the method in enabling the application of a wider range of data mining/analysis techniques to tree-structured web log data.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2052355",
        "year": "2011"
    },
    {
        "title": "An adaptive data logging system for animal power studies",
        "abstract": "The development of a micro-processor based modular data logging system is described and features of the hardware, software and sensors are given. The reasons for including certain features are discussed in the context of draught animal power investigations. A number of advanced electronic techniques have been incorporated, providing a high degree of versatility and data handling power in a compact, user friendly system. The system is capable of logging 160 channels/second with simple processing, with more complex processing the logging speed is reduced.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/0021863489900449",
        "year": "1989"
    },
    {
        "title": "An algorithm for generating n Huffman codes with a worst case of (n log/sub 2/ n+n log/sub 2/ log/sub 2/ n) comparisons",
        "abstract": "The method is based on a Heapsort algorithm for sorting a sequence of n data elements with a worst case of (n log/sub 2/ n+n log/sub 2/ log/sub 2/ n) comparisons. The idea of heap restoring in the Heapsort algorithm is incorporated into the Huffman tree building process. The performance of the proposed algorithm in execution time is evaluated with a list of ordered, random, and reverse ordered data containing from 5000 to 500000 items.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/515711/",
        "year": "1993"
    },
    {
        "title": "An Analysis of Main-Memory and Log Space Usage in Extended Ephemeral Logging",
        "abstract": "Extended Ephemeral Logging (XEL) is a database logging and recovery technique which manages a log of recovery data by partitioning it into a series of logically circular generations. XEL copies longer-lived log data from one generation to another in order to reclaim more quickly the space occupied by shorter-lived log data. As a result of copying, records in the log lose their original ordering; this leads to main-memory and log space overhead for obsolete recovery data. In this paper, we quantify the effects of reordering log records by introducing the notion of Garbage Removal Dependencies (GRDs). We develop a classification of log records based on GRDs and use it to characterize main-memory and log space allocation during normal system operation. Through simulation, we demonstrate how main-memory and log space allocation vary with changes in database and workload parameters.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/3-540-45754-2_3",
        "year": "2001"
    },
    {
        "title": "Uncovering the Relationship Between Event Log Characteristics and Process Discovery Techniques",
        "abstract": "The research field of process mining deals with the extraction of knowledge from event logs. Event logs consist of the recording of activities that took place in a certain business environment and as such, one of process mining‚Äôs main goals is to get an insight on the execution of business processes. Although a substantial effort has been put on developing techniques which are able to mine event logs accurately, it is still unclear how exactly characteristics of the latter influence a technique‚Äôs performance. In this paper, we provide a robust methodology of analysis and subsequently derive useful insights on the role of event log characteristics in process discovery tasks by means of an exhaustive comparative study.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-06257-0_4",
        "year": "2014"
    },
    {
        "title": "Understand and Assess People‚Äôs Procrastination by Mining Computer Usage Log",
        "abstract": "Although the computer and Internet largely improve the convenience of life, they also result in various problems to our work, such as procrastination. Especially, today‚Äôs easy access to Internet makes procrastination more pervasive for many people. However, how to accurately assess user procrastination is a challenging problem. Traditional approaches are mainly based on questionnaires, where a list of questions are often created by experts and presented to users to answer. But these approaches are often inaccurate, costly and time-consuming, and thus can not work well for a large number of ordinary people. In this paper, to the best of our knowledge, we are the first to propose to understand and assess people‚Äôs procrastination by mining user‚Äôs behavioral log on computer. Specifically, as the user‚Äôs behavior log is time-series, we first propose a simple procrastination identification model based on the Markov Chain to assess user procrastination. While the simple model can not directly depict reasons of user procrastination, we extract some features from computer logs, which successfully bridge the gap between user behaviors on computer and psychological theories. Based on the extracted features, we design a more sophisticated model, which can accurately identify user procrastination and reveal factors that may cause user‚Äôs procrastination. The revealed factors could be used to further develop programs to mitigate user‚Äôs procrastination. To validate the effectiveness of our model, we conduct experiments on a real-world dataset and procrastination questionnaires with 115 volunteers. The results are consistent with psychological findings and validate the effectiveness of the proposed model. We believe this work could provide valuable insights for researchers to further exploring procrastination.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-99365-2_17",
        "year": "2018"
    },
    {
        "title": "Understanding academic information seeking habits through analysis of web server log files: the case of the teachers college library website",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S009913330800195X",
        "year": "2009"
    },
    {
        "title": "Understanding user behavior through log data and analysis",
        "abstract": "HCI researchers are increasingly collecting rich behavioral traces of user interactions with online systems in situ at a scale not previously possible. These logs can be used to characterize user interactions with existing systems and compare different designs. Large-scale log studies give rise to new challenges in experimental design, data collection and interpretation, and ethics. The chapter discusses how to address these challenges using search engine logs, but the methods are applicable to other types of log data.",
        "include": false,
        "url": "https://link.springer.com/chapter/10.1007/978-1-4939-0378-8_14",
        "year": "2014"
    },
    {
        "title": "Unhealthy Dietary Behavior Based User Life-Log Monitoring for Wellness Services",
        "abstract": "Unhealthy behavior, constitutes of unhealthy diet, smoking, physical inactivity and alcohol intake, increases the risk of chronic diseases and premature mortality. These unhealthy behaviors can be avoided by little intention¬†and guidance. Diet is an influential factor of healthcare. Healthy and balanced diet selection is related to the better life expectancy and decreases the chances of chronic diseases. The Ubiquitous computing revolutionized the wellness domain towards user centric preference based health management. In this study we proposed a method for monitoring and indication of users‚Äô unhealthy nutrition consumption. We evaluated 3 different timings of indication to user for induction of healthy dietary pattern. The ‚Äúlocation and time based indication‚Äù depicts very promising result of 78% in the adoption of healthy diet pattern and has positive impact on the intake of fat nutrient in diet.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-66188-9_7",
        "year": "2017"
    },
    {
        "title": "Unsupervised Anomaly Detection in Noisy Business Process Event Logs Using Denoising Autoencoders",
        "abstract": "Business processes are prone to subtle changes over time, as unwanted behavior manifests in the execution over time. This problem is related to anomaly detection, as these subtle changes start of as anomalies at first, and thus it is important to detect them early. However, the necessary process documentation is often outdated, and thus not usable. Moreover, the only way of analyzing a process in execution is the use of event logs coming from process-aware information systems, but these event logs already contain anomalous behavior and other sorts of noise. Classic process anomaly detection algorithms require a dataset that is free of anomalies; thus, they are unable to process the noisy event logs. Within this paper we propose a system, relying on neural network technology, that is able to deal with the noise in the event log and learn a representation of the underlying model, and thus detect anomalous behavior based on this representation. We evaluate our approach on five different event logs, coming from process models with different complexities, and demonstrate that our approach yields remarkable results of 97.2¬†% F1-score in detecting anomalous traces in the event log, and 95.6¬†% accuracy in detecting the respective anomalous activities within the traces.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-46307-0_28",
        "year": "2016"
    },
    {
        "title": "User Activity Investigation of a Web CRM System Based on the Log Analysis",
        "abstract": "There are many tools for the analysis of Web system log files based on statistical or web mining methods. However they do not always provide information specific for a given system. In the paper special method for investigation of the activity of web CRM system users is presented. The method has been designed and implemented in the web CRM system at a debt vindication company. There was basic foundation, that analysis should be carried in three time groups, i.e. working days in hours of work, working days beyond hours of work and idle days. Besides, system should make it possible to perform analysis for a chosen employee, position, day, hour and CRM system file. The results of investigation allowed to reveal anomalies in staff activity, what was not possible using common web log analyzer.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11495772_66",
        "year": "2005"
    },
    {
        "title": "User-Level Secure Deletion on Log-Structured File Systems",
        "abstract": "This chapter presents our research into user-level secure deletion for flash memory, with a concrete example of an Android-based mobile phone. We show that these systems provide no timely data deletion, and that the time data remains increases with the storage medium‚Äôs size. We propose two user-level solutions that achieve secure deletion as well as a hybrid of them, which guarantees periodic, prompt secure data deletion regardless of the storage medium‚Äôs size. We also develop a model of the writing behaviour on a mobile device that we use to quantify our solution‚Äôs performance.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-28778-2_5",
        "year": "2016"
    },
    {
        "title": "Using Event Logs to Model Interarrival Times in Business Process Simulation",
        "abstract": "The construction of a business process simulation (BPS) model requires significant modeling efforts. This paper focuses on modeling the interarrival time (IAT) of entities, i.e. the time between the arrival of consecutive entities. Accurately modeling entity arrival is crucial as it influences process performance metrics such as the average waiting time. In this respect, the analysis of event logs can be useful. Given the limited process mining support for this BPS modeling task, the contribution of this paper is twofold. Firstly, an IAT input model taxonomy for process mining is introduced, describing event log use depending on process and event log characteristics. Secondly, ARPRA is introduced and operationalized for gamma distributed IATs. This novel approach to mine an IAT input model is the first to explicitly integrate the notion of queues. ARPRA is shown to significantly outperform a benchmark approach which ignores queue formation.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-42887-1_21",
        "year": "2016"
    },
    {
        "title": "Using Location, Bearing and Motion Data to Filter Video and System Logs",
        "abstract": "In evaluating and analysing a pervasive computing system, it is common to log system use and to create video recordings of users. A lot of data will often be generated, representing potentially long periods of user activity. We present a procedure to identify sections of such data that are salient given the current context of analysis; for example analysing the activity of a particular person among many trial participants recorded by multiple cameras. By augmenting the cameras used to capture a mobile experiment, we are able to establish both a location and heading for each camera, and thus model the field of view for each camera over time. Locations of trial participants are also recorded and compared against camera views, to determine which periods of user activity are likely to have been recorded in detail. Additionally the stability of a camera can be tracked and video can be subsequently filtered to exclude footage of unacceptable quality. These techniques are implemented in an extension to Replayer: a software toolkit for use in the development cycle of mobile applications. A report of initial testing is given, whereby the technique‚Äôs use is demonstrated on a representative mobile application.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-72037-9_7",
        "year": "2007"
    },
    {
        "title": "Using Suffix-Tree to Identify Patterns and Cluster Traces from Event Log",
        "abstract": "Process mining refers to the extraction process models from event logs. Traditional process mining algorithms have problems dealing with event logs that are produced from unstructured real-life processes and generate spaghetti-like and incomprehensible process models. One means making traces more structural is to extract commonly used process model constructs (common patterns) in the event log and transform traces basing on such constructs. Another way of pre-processing traces is to categorize traces in event log into clusters such that process traces in each cluster can be adequately represented by a process model. Nevertheless, current approaches for trace clustering have many problems such as ignoring context process and huge computational overhead. In this paper, suffix-tree is firstly utilized for discovering common patterns. The traces in event log are transformed with common patterns. Thereafter suffix-trees are applied to categorize transformed traces. The trace clustering algorithm has a linear-time computational complexity. The process models mined from the clustered traces show a high degree of fitness and comprehensibility.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-32573-1_20",
        "year": "2012"
    },
    {
        "title": "Vegetation development of boreal riparian plant communities after flooding, fire, and logging, Peace River, Canada",
        "abstract": "In this study we compare and contrast vegetation development following natural and logging disturbances in a major boreal river valley. Permanent sample plots and releves were established and sampled for vegetation and landscape attributes in June and July of 1993 and 1994 in the Peace River Lowlands, Wood Buffalo National Park, Canada. In the Peace River Lowlands, primary succession is a flood-origin process. Secondary succession may be either autogenic through gap dynamics mediated by nursery logs, buried wood, and suckering, or allogenic, following fire or logging. Flood origin accounts for 72% and fire origin for 29% of the undisturbed forests. From 1951‚Äì1995, 24% of the forest land burned, yielding a fire return interval of 186 years. Forest successional trajectories are set soon after flood, logging, or fire, with little evidence of gradual replacement of one forest type by another. Vegetation composition and relative species abundance are strongly correlated with living moss depth, moss-lichen total cover, total tree cover, herb cover, and canopy height. Species with high indicator value are Hylocomium splendens, Picea glauca, Pyrola chlorantha, Equisetum pratense, and Epilobium angustifolium. Strong correlations exist between white spruce tree density and canopy height, total tree cover and canopy height, total tree cover and basal area per hectare, basal area and canopy height, and between canopy height and surface age. Clearcuts are initially dominated by rose-raspberry followed by balsam poplar (with lesser amounts of Alaska birch and aspen). After logging, temporal changes in composition and dominance occur more rapidly than during natural succession. There is no evidence of post-logging convergence toward the original white spruce and mixedwood forests; a long-term deciduous disclimax is predicted. Vegetation associations, successional pathways, landscape relationships, and ecological benchmarks are identified.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0378112796039291",
        "year": "1997"
    },
    {
        "title": "Visual Mining of Web Logs with DataTube2",
        "abstract": "We present in this paper a new method for the visual and interactive exploration of Web sites logs. Web usage data is mapped onto a 3D tube which axis represents time and where each facet corresponds to the hits of a given page and for a given time interval. A rearrangement clustering algorithm is used to create groups among pages. Several interactions have been implemented within this visualization such as the possibility to add annotations or the use of a virtual reality equipment. We present results for two Web sites (1148 pages over 491 days, and 107 pages over 625 days). We highlight the actual limits of our system (9463 pages over 153 days) and show that it outperforms similar existing approaches.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-04409-0_53",
        "year": "2009"
    },
    {
        "title": "Visualization of System Log Files for Post-incident Analysis and Response",
        "abstract": "Post-incident analysis of a security event is a complex task due to the volume of data that must be assessed, often within tight temporal constraints. System software, such as operating systems and applications, provide a range of opportunities to record data in log files about interactions with the computer that may provide evidence during an investigation. Data visualization can be used to aid data set interpretation and improve the ability of the analyst to make sense of information. This paper posits a novel methodology that visualizes data from a range of log files to aid the investigation process. In order to demonstrate the applicability of the approach, a case study of identification and analysis of attacks is presented.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07620-1_3",
        "year": "2014"
    },
    {
        "title": "Web Log Data Analysis and Mining",
        "abstract": "Log files contain information about User Name, IP Address, Time Stamp, Access Request, number of Bytes Transferred, Result Status, URL that Referred and User Agent. The log files are maintained by the web servers. By analysing these log files gives a neat idea about the user. This paper gives a detailed discussion about these log files, their formats, their creation, access procedures, their uses, various algorithms used and the additional parameters that can be used in the log files which in turn gives way to an effective mining. It also provides the idea of creating an extended log file.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-17881-8_44",
        "year": "2011"
    },
    {
        "title": "Web log data clustering for a multi-agent recommendation system",
        "abstract": "In this paper, we propose an automatic way of recommending information to be visualized by users. The list of information to be recommended is generated based on the web logs of the users stored by the system in a multi relational database. This system is a web-based multi-agent system which provides geographical information and monitors the actions of the users by generating logs. Frequently, the system joins the relational web log data and runs a clustering algorithm in order to recommend a list of most accessed information up to that moment to registered users who log in the system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5581017/",
        "year": "2010"
    },
    {
        "title": "Web log data warehousing and mining for intelligent web caching",
        "abstract": "",
        "include": true,
        "url": "https://www.sciencedirect.com/science/article/pii/S0169023X01000386",
        "year": "2001"
    },
    {
        "title": "Web log mining",
        "abstract": "In the design and implementation of an Intelligent Web Information System (IWIS), it is necessary to consider the learning and discovery functionalities that produce the required knowledge of the system. Web log files provide a useful resource for the discovery of useful knowledge. In the context of IWIS, we present a brief survey of Web log mining. An overview of the more general topic known as Web mining is given first. Web log mining is then reviewed by focusing on three important aspects, namely, data preparation, Web log mining, and applications.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-662-05320-1_9",
        "year": "2003"
    },
    {
        "title": "Web Log Mining and Parallel SQL Based Execution",
        "abstract": "We performed association rule mining and sequence pattern mining against the access log which was accumulated at NTT Software Mobile Info Search portal site. Detail web log mining process and the rules we derived are reported in this paper. The integration of web data and relational database enables better management of web data. Some researches have even tried to implement applications such as web mining with SQL. Commercial RDBMSs support parallel execution of SQL. Parallelism is key to improve the performance. We showed that commercial RDBMS can achieve substantial speed up for web mining.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-44431-9_2",
        "year": "2000"
    },
    {
        "title": "Web Logs",
        "abstract": "You don‚Äôt have to write for a major newspaper to have your opinions shared with the world. And you definitely don‚Äôt need to come up with 300 pages for a best-selling book covering your favorite hobby. I could have titled this chapter ‚ÄúBecome Famous Using Web Logs.‚Äù You might be surprised by the number of people who have built a reputation (and a fortune) by creating and writing a web log (or blog for short) of their own. Do a Google search for Matt Drudge, Paul Kos, and glenn Reynolds to see how a few individuals turned their opinions into fame and fortune.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4302-1864-7_15",
        "year": "2009"
    },
    {
        "title": "Web Objects Clustering Using Transaction Log",
        "abstract": "In this paper, we present a novel method for clustering web objects. Most of existing methods aren't sufficient to explore similar objects, because the basic data, which include attributes of objects, click-through data, and link data, are often sparse, scarce or difficult to obtain. In contrast, the information we exploit is transaction log, which is more common, denser as well as noisier. To reduce the influence of the noises, we calculate the similarity in two steps. Firstly, we use a basic similarity to discover objects' neighbors. The objects are represented by vectors consisting of their neighbors. Secondly, the cosine similarity of the object vectors is calculated for clustering. Experiments on synthetic data show that our method is robust against noises. Using noisy data, we increase the precision by 10%. Finally, we show real clustering results based on a movie dataset and achieve the coverage of 76% and the precision of 60%.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/5432677/",
        "year": "2010"
    },
    {
        "title": "Web Search and Browse Log Mining: Challenges, Methods, and Applications",
        "abstract": "Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users‚Äô permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve search results as well as online advertisement. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-20152-3_42",
        "year": "2011"
    },
    {
        "title": "Web Site Auditing Using Web Access Log Data",
        "abstract": "This paper applies a method to use the access log data to audit Web sites. It studies website auditing by (1) proposing a new fuzzy clustering algorithm that combines standard fuzzy C-means and the artificial fish swarm algorithm; (2) presenting a new measurement index for similarities between user sessions; and (3) providing an experiment on the execution of this new method. The results are encouraging and show the potential of our fuzzy clustering approach to assist in auditing Web site.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4939112/",
        "year": "2009"
    },
    {
        "title": "Web usage pattern analysis through web logs: A review",
        "abstract": "Web server log repositories are great source of knowledge, which keeps the record of web usage patterns of different web users. The Web usage pattern analysis is the process of identifying browsing patterns by analyzing the user's navigational behavior. The web server log files which store the information about the visitors of web sites is used as input for the web usage pattern analysis process. First these log files are preprocessed and converted into required formats so web usage mining techniques can apply on these web logs. This paper reviews the process of discovering useful patterns from the web server log file of an academic institute. The obtained results can be used in different applications like web traffic analysis, efficient website administration, site modifications, system improvement and personalization and business intelligence etc.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6261924/",
        "year": "2012"
    },
    {
        "title": "Web user log mining for Web retrieval",
        "abstract": "As information on the Internet expands rapidly, we can get more information than before. However, how to find user-intended information from the Internet including text, images, and video is not easy. In this paper, we use relevance feedback and build user space by an improved Bayesian algorithm to mine the log of user's feedback to improve retrieval performance. Data mining is used to remove clutter and irrelevant text information, and help to eliminate mismatch between the page author's expression and the user's understanding and expectation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1181223/",
        "year": "2002"
    },
    {
        "title": "Web User Profiling on Proxy Logs and Its Evaluation in Personalization",
        "abstract": "We propose a web user profiling and clustering framework based on LDA-based topic modeling with an analogy to document analysis in which documents and words represent users and their actions. The main technical challenge addressed here is how to symbolize web access actions, by words, that are monitored through a web proxy. We develop a hierarchical URL dictionary generated from Yahoo! Directory and a cross-hierarchical matching method that provides the function of automatic abstraction. We apply the proposed framework to 7500 students in Osaka University. The framework is used to analyze their 40GB click streams over a 4 month period. We evaluate clustering-based recommendation effectiveness to confirm the optimality of the framework. The results show high hit precision compared with existing methods.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-20291-9_13",
        "year": "2011"
    },
    {
        "title": "Web-Log Trend Pattern Analysis with Temporal Approach",
        "abstract": "Web mining uses data mining's techniques and its algorithms to explore the interesting patterns from the web access log on server data to withdraw out better knowledge of user attractions or users activity over the website. This paper gives the idea that how an industry conclude or examine their user's behavior on their websites, which is one of the important topics in today's time using the time stamp of the IP address. This paper deals with the temporal approach of weblog mining on NASA's website based on temporal trend analysis by analyzing the extracted interesting patterns. A weblog of NASA has been analyzed using R-Studio to extract the trend pattern based on the time of stay of users, which creates a particular pattern. There are many methods for data mining extractions, but this technique of trend analysis can provide more useful information for web mining.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8442783/",
        "year": "2018"
    },
    {
        "title": "Website Community Mining from Query Logs with Two-Phase Clustering",
        "abstract": "A website community refers to a set of websites that concentrate on the same or similar topics. There are two major challenges in website community mining task. First, the websites in the same topic may not have direct links among them because of competition concerns. Second, one website may contain information about several topics. Accordingly, the website community mining method should be able to capture such phenomena and assigns such website into different communities. In this paper, we propose a method to automatically mine website communities by exploiting the query log data in Web search. Query log data can be regarded as a comprehensive summarization of the real Web. The queries that result in a particular website clicked can be regarded as the summarization of that website content. The websites in the same topic are indirectly connected by the queries that convey information need in this topic. This observation can help us overcome the first challenge. The proposed two-phase method can tackle the second challenge. In the first phase, we cluster the queries of the same host to obtain different content aspects of the host. In the second phase, we further cluster the obtained content aspects from different hosts. Because of the two-phase clustering, one host may appear in more than one website communities.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-54903-8_17",
        "year": "2014"
    },
    {
        "title": "Website Privacy Preservation for Query Log Publishing",
        "abstract": "In this paper we study privacy preservation for the publication of search engine query logs. We introduce a new privacy concern, website privacy as a special case of business privacy. We define the possible adversaries who could be interested in disclosing website information and the vulnerabilities in the query log, which they could exploit. We elaborate on anonymization techniques to protect website information, discuss different types of attacks that an adversary could use and propose an anonymization strategy for one of these attacks. We then present a graph-based heuristic to validate the effectiveness of our anonymization method and perform an experimental evaluation of this approach. Our experimental results show that the query log can be appropriately anonymized against the specific attack, while retaining a significant volume of useful data.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-78478-4_5",
        "year": "2008"
    },
    {
        "title": "Werewolf Game Modeling Using Action Probabilities Based on Play Log Analysis",
        "abstract": "In this study, we construct a non-human agent that can play the werewolf game (i.e., AI wolf) with aims of creating more advanced intelligence and acquire more advanced communication skills for AI-based systems. We therefore constructed a behavioral model using information regarding human players and the decisions made by such players; all such information was obtained from play logs of the werewolf game. To confirm our model, we conducted simulation experiments of the werewolf game using an agent based on our proposed behavioral model, as well as a random agent for comparison. Consequently, we obtained an 81.55% coincidence ratio of agent behavior versus human behavior.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-50935-8_10",
        "year": "2016"
    },
    {
        "title": "What‚Äôs in a Step? Toward General, Abstract Representations of Tutoring System Log Data",
        "abstract": "The Pittsburgh Science of Learning Center (PSLC) is developing a data storage and analysis facility, called DataShop. It currently handles log data from 6 full-year tutoring systems and dozens of smaller, experimental tutoring systems. DataShop requires a representation of log data that supports a variety of tutoring systems, atheoretical analyses and theoretical analyses. The theory-based analyses are strongly related to student modeling, so the lessons learned in developing the DataShop‚Äôs representation may apply to student modeling in general. This report discusses the representation originally used by the DataShop, the problems encountered, and how the key concept of ‚Äústep‚Äù evolved to meet these challenges.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-73078-1_64",
        "year": "2007"
    },
    {
        "title": "Why policy reforms fail to improve logging practices: The role of governance and norms in Peru",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S1389934105001048",
        "year": "2006"
    },
    {
        "title": "Workflow mining and outlier detection from clinical activity logs",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S153204641200127X",
        "year": "2012"
    },
    {
        "title": "WUML: A Web Usage Manipulation Language for Querying Web Log Data",
        "abstract": "In this paper, we develop a novel Web Usage Manipulation Language (WUML) which is a declarative language for manipulating Web log data. We assume that a set of trails formed by users during the navigation process can be identified from Web log files. The trails are dually modelled as a transition graph and a navigation matrix with respect to the underlying Web topology. A WUML expression is executed by transforming it into Navigation Log Algebra (NLA), which consists of the sum, union, difference, intersection, projection, selection, power and grouping operators. As real navigation matrices are sparse, we perform a range of experiments to study the impact of using different matrix storage schemes on the performance of the NLA.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-30464-7_43",
        "year": "2004"
    },
    {
        "title": "XML Based Pre-processing and Analysis of Log Data in Adaptive E-Learning System: An Algorithmic Approach",
        "abstract": "E-learning has become the most popular way of delivering education and learning. Adaptive E-learning systems are the systems that adapt according to the requirements of the user. These systems should be capable of capturing the user preferences in terms of their learning styles and adapt the user interface accordingly. Web log analysis of the usage data can provide useful information regarding the learning styles. This analysis is extremely useful to develop an adaptive environment for the learner and at the same time for instructors to see how often their course contents are being used. In this paper a modified literature based approach is proposed where the learner‚Äôs behavior is tracked by capturing the interactions with e-learning portal. The captured behavior will be stored in the form of sessions which will be grouped together to generate the sequence files in the XML formats. The learning styles have been identified by an algorithmic approach based on the frequency and time that the learners spend on various learning components on the portal. The approach is useful to provide an adaptive user interface which includes adaptive contents and recommendations in learning environment to improve the efficiency of e-learning. The learning style model used is Felder-Silverman Learning Style Model (FSLSM) to fit the learning styles into an adaptive environment.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-28883-3_17",
        "year": "2016"
    },
    {
        "title": "Z-log: Applying System-Z",
        "abstract": "We present Z-log ‚Äì a practical system that employs the system-Z [13] semantics. Z-log incurs polynomial cost for compilation and entailment in the horn and q-horn [2] cases. Z-log‚Äôs complexity is intractable in the unrestricted case ‚Äì but intractable in the number of defaults that cause the violation of the q-horn property. We present here initial performance results over two alternative rulesbases. The results indicate that Z-log currently scales to problems on the order of 1000‚Äôs of propositional rules when the rules are in q-Horn form. We shall be applying Z-log in cognitive disease diagnosis.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-45757-7_52",
        "year": "2002"
    },
    {
        "title": "An analysis of matching in the Tau cell log-domain filter",
        "abstract": "Using various layout techniques and circuit configurations, the effects of matching on a log-domain filter were analyzed. It is shown here that one of three possible Tau cell configurations to implement the same 2nd order low pass filter clearly outperforms the others. Furthermore, application of a common centroid layout technique has had no noticeable improvement on filter matching",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1692612/",
        "year": "2006"
    },
    {
        "title": "An Approach to Identifying False Traces in Process Event Logs",
        "abstract": "By means of deriving knowledge from event logs, the application of process mining algorithms can provide valuable insight into the actual execution of business processes and help identify opportunities for their improvement. The event logs may be collected by people manually or generated by a variety of software applications, including business process management systems. However logging may not always be done in a reliable manner, resulting in events being missed or interchanged. Consequently, the results of the application of process mining algorithms to such ‚Äúpolluted‚Äù logs may not be so reliable and it would be preferable if false traces, i.e. polluted traces which are not possibly valid as regards the process model to be discovered, could be identified first and removed before such algorithms are applied. In this paper an approach is proposed that assists with identifying false traces in event logs as well as the cause of their pollution. The approach is empirically validated.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-37456-2_45",
        "year": "2013"
    },
    {
        "title": "An e-shop log file analysis toolbox",
        "abstract": "Most e-commerce applications are implemented without any significant means of built-in performance measuring mechanisms, although responsiveness is a major factor for high revenue. Often, overall response times increase without the administrators even noticing. It is therefore crucial to have a precise measuring system that also reports customer behavioral patterns. We created a customizable e-commerce log file analyzer that measures performance mainly through combining a mix of log file data with e-shop information. It is capable of supporting multiple e-shops and can perform cross comparisons between them. It is easy to use and the software is extendable in order to support different web server architectures. It displays patterns taken by the user interaction with the e-shop and allows the administrators to locate less visited pages and make improvements or promote them better. This way an e-shop may benefit since it may lead to higher user satisfaction and profitability. The administrator can regularly study the readings and take the appropriate actions, if overall performance decay is observed.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5580415/",
        "year": "2010"
    },
    {
        "title": "An effective system for mining web log",
        "abstract": "The WWW provides a simple yet effective media for users to search, browse, and retrieve information in the Web. Web log mining is a promising tool to study user behaviors, which could further benefit web-site designers with better organization and services. Although there are many existing systems that can be used to analyze the traversal path of web-site visitors, their performance is still far from satisfactory. In this paper, we propose our effective Web log mining system consists of data preprocessing, sequential pattern mining and visualization. In particular, we propose an efficient sequential mining algorithm (LAPIN_WEB: LAst Position INduction for WEB log), an extension of previous LAPIN algorithm to extract user access patterns from traversal path in Web logs. Our experimental results and performance studies demonstrate that LAPIN_WEB is very efficient and outperforms well-known PrefixSpan by up to an order of magnitude on real Web log datasets. Moreover, we also implement a visualization tool to help interpret mining results as well as predict users‚Äô future requests.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/11610113_5",
        "year": "2006"
    },
    {
        "title": "An enhanced pre-processing technique for web log mining by removing web robots",
        "abstract": "Nowadays, internet becomes useful source of information in day-to-day life. It creates huge development of World Wide Web in its quantity of interchange and its size and difficulty of websites. Web Usage Mining (WUM) is one of the main applications of data mining, artificial intelligence and so on to the web data and forecast the user's visiting behaviors and obtains their interests by investigating the samples. Since WUM directly involves in large range of applications, such as, ecommerce, e-learning, Web analytics, information retrieval etc. Weblog data is one of the major sources which contain all the information regarding the users visited links, browsing patterns, time spent on a particular page or link and this information can be used in several applications like adaptive web sites, modified services, customer summary, pre-fetching, generate attractive web sites etc. There are several problems related with the existing web usage mining approaches. Existing web usage mining algorithms suffer from difficulty of practical applicability. So, a novel research is necessary for the accurate prediction of future performance of web users with rapid execution time. WUM consists of preprocessing, pattern discovery and pattern analysis. Log data is characteristically noisy and unclear. Hence, preprocessing is an essential process for effective mining process. In this paper, a novel pre-processing technique is proposed by removing local and global noise and web robots. Anonymous Microsoft Web Dataset and MSNBC.com Anonymous Web Dataset are used for estimating the proposed preprocessing technique.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6510325/",
        "year": "2012"
    },
    {
        "title": "An Evaluation Performance of Log Periodic Dipole Antenna Based on the Parameter of Flux Density of the Solar Radio Burst Event",
        "abstract": "In this work, an evaluation of the flux density of the solar radio burst is analyzed. We used a nineteenth (19) elements of rod aluminums‚Äô type as a conductor with of different sizes is being prepared to construct a log periodic dipole antenna (LPDA) from 45 - 870 MHz. The testing was carried out at the National Space Agency (PAN), Sg. Lang, Banting Selangor by connecting to the Compound Low Cost Low Frequency Spectroscopy Transportable Observatory (CALLISTO) spectrometer. We choose the input impedance, R0 = 50 ohm for this LPDA antenna. From the analysis, the gain of the antenna is 9.3 dB. This antenna possibly captures a signal that covers about 0.08 m2 area of the Sun. It was found that the temperature of the burst that detected at the feed point of the antenna is 32 K. However, the signal becomes decrease to 28.75K while by the CALLISTO spectrometer as a receiver. We found that the isotropic source spectral power is 1576 W/Hz. Since the burst level above the background sky is 0.41 dB, the flux density of the burst is 5.5 x 10‚àí‚Äâ21 W/m2/Hz. We conclude that this antenna is suitable for to observe the Sun activities at low frequency region.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-662-47200-2_72",
        "year": "2015"
    },
    {
        "title": "An Evolutionary Approach for Clustering User Access Patterns from Web Logs",
        "abstract": "In this paper rough c-means is applied to cluster user access patterns, in which PSO(Particle Swarm Optimization) algorithm is employed to tune the threshold and relative importance of upper and lower approximations. The Davies-Bouldin clustering validity index is used as the fitness function that is minimized while arriving at an optimal clustering. The effectiveness of the algorithm is demonstrated by an experiment.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11941439_145",
        "year": "2006"
    },
    {
        "title": "An Experimental Analysis of Windows Log Events Triggered by Malware",
        "abstract": "According to the 2016 Internet Security Threat Report by Symantec, there are around 431 million variants of malware known. This effort focuses on malware used for spying on user's activities, remotely controlling devices, and identity and credential theft within a Windows based operating system. As Windows operating systems create and maintain a log of all events that are encountered, various malware are tested on virtual machines to determine what events they trigger in the Windows logs. The observations are compiled into Operating System specific lookup tables that can then be used to find the tested malware on other computers with the same Operating System.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3077295",
        "year": "2017"
    },
    {
        "title": "An exploratory web log study of multitasking",
        "abstract": "The Web search multitasking study based on automatic task session detection procedure is described. The results of the study: 1) multitasking is very rare, 2) it usually covers only 2 task sessions, 3) it is frequently formed into a temporal inclusion of an interrupting task session into the interrupted session, 4) the quantitative characteristics of multitasking greatly differ from the characteristics of sequential execution of one and several tasks. A searcher minimizes task switching costs: he avoids multitasking and while multitasking he uses cheapest manner of task switching.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1148286",
        "year": "2006"
    },
    {
        "title": "An Extensible Framework for Analysing Resource Behaviour Using Event Logs",
        "abstract": "Business processes depend on human resources and managers must regularly evaluate the performance of their employees based on a number of measures, some of which are subjective in nature. As modern organisations use information systems to automate their business processes and record information about processes‚Äô executions in event logs, it now becomes possible to get objective information about resource behaviours by analysing data recorded in event logs. We present an extensible framework for extracting knowledge from event logs about the behaviour of a human resource and for analysing the dynamics of this behaviour over time. The framework is fully automated and implements a predefined set of behavioural indicators for human resources. It also provides a means for organisations to define their own behavioural indicators, using the conventional Structured Query Language, and a means to analyse the dynamics of these indicators. The framework‚Äôs applicability is demonstrated using an event log from a German bank.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-07881-6_38",
        "year": "2014"
    },
    {
        "title": "An Improved Algorithm for Session Identification on Web Log",
        "abstract": "As regards session identification method on web mining, an improved one has been put forward. Firstly, considering website structure and its content, page access time threshold will be reached after collecting access time of each page, which should be used to divide sessions into various sets. Then, the session sets will be optimized further, with the help of session reconstruction, namely union and rupture. It has been proved through experiment that the session set which is attained by the above method is more faithful.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-16515-3_8",
        "year": "2010"
    },
    {
        "title": "An improved log-likelihood gradient for continuous-time stochastic systems with deterministic input",
        "abstract": "Using a covariance operator approach, we derive an explicit expression for the log-likelihood ratio gradient for system parameter estimation for continuous-time stochastic systems with deterministic inputs. The gradient formula includes the smoother estimates and derivatives of system matrices with no derivatives of estimates or covariance matrices. A deterministic input is also permitted, and the state noise covariance is not required to be nonsingular. Stable numerical techniques to calculate the gradient are also discussed.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/533686/",
        "year": "1996"
    },
    {
        "title": "An Improved, Feature-Centric LoG Approach for Edge Detection",
        "abstract": "Gaussian filter is used to smooth an input image to prevent false edge detection caused by image noises in the classic LoG edge detector, but it weakens the image features at the same time which results in some edges cannot be detected efficiently. To ameliorate, this paper presents an improved, feature-centric LoG approach for edge detection. It firstly uses non-local means filter based on structural similarity measure to replace Gaussian filter to smooth an input image which enables the image features to be preserved better, and then image edges can be extracted efficiently by the zero-crossing method for the smoothed image operated by Laplacian operator. Experimental results show that the proposed method can improve the edge detection precision of the classic LoG edge detector, and the non-local means filter used in the presented method achieves better results than the other two typical filters with edge-preserving ability.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-42108-7_36",
        "year": "2016"
    },
    {
        "title": "An Interactive Web-Based Toolset for Knowledge Discovery from Short Text Log Data",
        "abstract": "Many companies maintain human-written logs to capture data on events such as workplace incidents and equipment failures. However, the sheer volume and unstructured nature of this data prevent it from being utilised for knowledge acquisition. Our web-based prototype software system provides a cohesive computational methodology for analysing and visualising log data that requires minimal human involvement. It features an interface to support customisable, modularised log data processing and knowledge discovery. This enables owners of event-based datasets containing short textual descriptions, such as occupational health \u0026 safety officers and machine operators, to identify latent knowledge not previously acquirable without significant time and effort. The software system comprises five distinct stages, corresponding to standard data mining milestones: exploratory analysis, data warehousing, association rule mining, entity clustering, and predictive analysis. To the best of our knowledge, it is the first dedicated system to computationally analyse short text log data and provides a powerful interface that visualises the analytical results and supports human interaction.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-69179-4_61",
        "year": "2017"
    },
    {
        "title": "An introduction to log-linear analysis and implementing the Newton-Raphson algorithm in APL2",
        "abstract": "This paper introduces the method of log-linear analysis for performing hypothesis testing on contingency tables. The reader will see a brief development of this topic beginning with Pearson's classic chi-square test and examples of analyses on two and three dimensional tables. The idea of hierarchical models and backward elimination are discussed. Finally, the APL2 implementation of the Newton-Raphson algorithm is described. Newton-Raphson is an iterative procedure for finding the roots of a function. It is used in log-linear analysis to find the maximum-likelihood estimation of expected frequencies that cannot be calculated from expressions in closed form.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=166217",
        "year": "1993"
    },
    {
        "title": "An Investigation on Students' Acceptance of Writing Web Logs: A Test of Technology Acceptance Model",
        "abstract": "The application of computer in education is on the rise recently. However, using Web logs in English language teaching (ELT) is rather new in Malaysia since the students' acceptance of writing web logs has not been extensively examined in previous studies. Therefore, the objective of this study is to measure studentspsila acceptance of writing Web logs employing the Davispsila technology acceptance model (TAM). Two determinants-perceived ease of use (PEOU) and perceived usefulness (PU) are used to determine an individualpsilas behavioural intention (BI) of writing Web log. To investigate the relationships among PEOU, PU, and BI of users with and without hands-on experience of blogging, pre- and post-questionnaires were administered on three intact groups of undergraduate students as the target sample. Three hypotheses were formulated based on the objectives of the study. The result of the study shows that with the support of hands-on experience, the students accepted to write Web logs because they found it was more useful rather than easy to use. Besides, TAM also had been found suitable to be adopted in educational context to predict technology acceptance prior and subsequent to users having any hands-on experience with the technology for pedagogical purpose.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5169477/",
        "year": "2009"
    },
    {
        "title": "An LTE reconfigurable SOVA/log-MAP turbo decoder for uncorrelated Rayleigh fading",
        "abstract": "According to published literature for turbo decoding, SOVA and log-MAP algorithms share common operations. This paper shows that the improved reconfigurable SOVA/log-MAP turbo decoder can be implemented in LTE standard for uncorrelated Rayleigh fading channel. We concentrate on physical shared channel and use pipeline turbo decoder architecture with 12 iterations. We examine two data rates and for each one we consider the nine possible QCIs using five frame lengths. Considering BER and delay limitations for each QCI, SOVA is proposed for most of the cases for low data rates (apart from QCIs with tight BER limitation), whereas log-MAP is proposed for higher data rates.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7297715/",
        "year": "2015"
    },
    {
        "title": "An O(k/sup 2//spl middot/log(n)) algorithm for computing the reliability of consecutive-k-out-of-n: F systems",
        "abstract": "This study presents an O(k/sup 2//spl middot/log(n)) algorithm for computing the reliability of a linear as well as a circular consecutive-k-out-of-n: F system. The proposed algorithm is more efficient and much simpler than the O(k/sup 3//spl middot/log(n/k)) algorithm of Hwang \u0026 Wright.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/1282155/",
        "year": "2004"
    },
    {
        "title": "An O(k/sup 3/spl middot/log(n/k)) algorithm for the consecutive-k-out-of-n:F system",
        "abstract": "The fastest generally-recognized algorithms for computing the reliability of consecutive-k-out-of-n:F systems require O(n) time, for both the linear and circular systems. The authors' new algorithm requires O(k/sup 3/spl middot/log(n/k)) time. The algorithm can be extended to yield an O(n/spl middot/max{k/sup 3/spl middot/log(n/k), log(n))} total time procedure for solving the combinatorial problem of counting the number of working states, with w working and n-w failed components, w=1,2,...,n.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/376533/",
        "year": "1995"
    },
    {
        "title": "An O(qlogq) log-domain decoder for non-binary LDPC over GF(q)",
        "abstract": "This paper presents a log-domain decoder for non-binary LDPC over GF(q). Comparing with the conventional O(q 2 ) decoders, the proposed decoder can efficiently reduce the decoding complexity to O(qlogq) with only negligible degradation in BER. Comparisons on both simulated BER performance and computational complexity between the proposed and existing log-domain decoders are also provided.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4746352/",
        "year": "2008"
    },
    {
        "title": "An object-oriented OSI event and log manager",
        "abstract": "This paper presents an object-oriented design and implementation of the OSI model of event and log manager based on the guidelines of definition of managed objects (GDMO) definitions of event reporting management (ERM) and log control function (LCF). The design is based on an object modelling technique (OMT) and implemented using C++ in an OS/2 2.1 environment. The manager was integrated with the XOM and XMP APIs communication infrastructure.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/526057/",
        "year": "1995"
    },
    {
        "title": "An XML log standard and tool for digital library logging analysis",
        "abstract": "Log analysis can be a primary source ofkno wledge about how digital library patrons actually use DL systems and services and how systems behave while trying to support user information seeking activities. Log recording and analysis allow evaluation assessment, and open opportunities to improvements and enhanced new services. In this paper, we propose an XML-based digital library log format standard that captures a rich, detailed set of system and user behaviors supported by current digital library services. The format is implemented in a generic log component tool, which can be plugged into any digital library system. The focus of the work is on interoperability, reusability, and completeness. Specifications, implementation details, and examples of use within the MARIAN digital library system are described.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/3-540-45747-X_10",
        "year": "2002"
    },
    {
        "title": "Analysing Web search logs to determine session boundaries for user-oriented learning",
        "abstract": "Incremental learning approaches based on user search activities provide a means of building adaptive information retrieval systems. To develop more effective user-oriented learning techniques for the Web, we need to be able to identify a meaningful session unit from which we can learn. Without this, we run a high risk of grouping together activities that are unrelated or perhaps not from the same user. We are interested in detecting boundaries of sequences between related activities (sessions) that would group the activities for a learning purpose. Session boundaries, in Reuters transaction logs, were detected automatically. The generated boundaries were compared with human judgements. The comparison confirmed that a meaningful session threshold for establishing these session boundaries was confined to a 11-15 minute range.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/3-540-44595-1_38",
        "year": "2000"
    },
    {
        "title": "Analysis and implement of PIX firewall syslog log",
        "abstract": "Useful information concerning with the network running status is included in logs generated by firewall, but analyzing large quantity data is very difficult. Therefore, based on Cisco PIX firewall, this paper gathered Syslog logs by employing the thread pool technique, then filtered and categorized them with key words, and finally stored them with format. Through the TopN statistics analysis, research and detection on security event based on feature, it realizes monitoring effectively the network traffic, application service, user behavior and running status, and it also provides the basis of network management and security strategy design for administrator, thereby strengthens further network management.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5477784/",
        "year": "2010"
    },
    {
        "title": "Analysis of a novel V-shape feed line for log-periodic dipole array antenna",
        "abstract": "In this paper, a V-shape feed line for log-periodic dipole array antenna is presented. The design procedure and numerical analysis of the characteristic impedance are obtained. The feed line is analyzed by conformal mapping method and finite integration technique.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7109626/",
        "year": "2015"
    },
    {
        "title": "Analysis of a very large web search engine query log",
        "abstract": "In this paper we present an analysis of an AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents almost 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. We also present results of a correlation analysis of the log entries, studying the interaction of terms within queries. Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. This suggests that traditional information retrieval techniques may not work well for answering web search requests. The correlation analysis showed that the most highly correlated items are constituents of phrases. This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=331405",
        "year": "1999"
    },
    {
        "title": "Analysis of CO2 emissions in APEC countries: A time-series and a cross-sectional decomposition using the log mean Divisia method",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0301421505001266",
        "year": "2006"
    },
    {
        "title": "Analysis of Different Clustering Approaches for Grouping Users on the Basis of GSM Call Logs",
        "abstract": "Analysis of Location-aware data plays an important role in pervasive and mobile computing. In cellular systems (e.g., GSM) the serving cell is easily available as an indication of the user location, without any additional hardware or network services. With this location data and other context variables groups of mobile users who are advertised to the found together can be traced. The objective of this paper is to review the works carried out by different group of researchers using varied techniques in Discovering User context, Mobility Prediction of Mobile users, Discovering social groups, Fraud detection in mobile communications networks etc.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5966392/",
        "year": "2011"
    },
    {
        "title": "Analysis of E-Learning Logs to Estimate Students‚Äô Phonemic Perception Confusion in English Word Recognition",
        "abstract": "In the paper, we analyzed students‚Äô learning log data obtained in a word listening learning system to estimate phonemic perception confusion of students. We had two student groups take a word dictation test and a word choice test. English words in the word dictation test were resolved into phonemes and the abilities of both groups on phonemic perception were inferred by Bayes‚Äô rule. By comparing the results estimated by Bayes‚Äô rule with those obtained in the word choice test, we suggested that the estimation is valid and helpful to build adaptive word recognition training systems.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-41175-5_33",
        "year": "2013"
    },
    {
        "title": "Analysis of electromagnetic well logging tools for oil and gas exploration using finite volume techniques",
        "abstract": "We analyze the response of logging-while-drilling (LWD) tools for oil exploration in complex three-dimensional borehole environments using finite-volume (FV) algorithms in cylindrical coordinates. Both eccentric boreholes and dipping beds scenarios are considered. In order to model eccentric boreholes, a face-based locally-conformal FV scheme is employed. We implement two distinct FV approaches, viz., the direct-field formulation and the coupled vector-scalar potentials formulation. The numerical results show that both FV approaches provide accurate results for the examples considered.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4404343/",
        "year": "2007"
    },
    {
        "title": "Analysis of Event Logs: Behavioral Graphs",
        "abstract": "Analysis of event logs is very important discipline used for the evaluation of performance and control-flow issues within the systems. This type of analysis is typically used in process mining sphere, where information systems, for example workflow management systems, enterprise resource planning systems, customer relationship management, supply chain management systems, and business to business systems record transactions and executed activities in a systematic way. Social network analysis takes part of process mining techniques, focused on activity performers, on users. The authors present a new approach to analysis of user behavior in the systems. The approach allows to find behavioral patterns and to find groups of users with similar behavior. An observer can obtain relations between the users on the basis of their similar behavior. The visualization of relations between the users is then presented by so called behavioral graphs. The approach was tested for event log analysis of a virtual company model developed as a multi-agent system by modeling environment MAREA.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-20370-6_4",
        "year": "2015"
    },
    {
        "title": "Analysis of execution log files",
        "abstract": "Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1810405",
        "year": "2010"
    },
    {
        "title": "Analysis of interaction logs for online tutorials (abstract only)",
        "abstract": "As the use of online interactive tutorials becomes more widespread, there will be more opportunities to use fine-grained interaction log data to deduce student behavior. Log data can help debug usability or pedagogical problems with the tutorials, or guide redesign to discourage pedagogically poor student behavior. OpenDSA is a collection of open source interactive materials for teaching data structures and algorithms. We present a case study analysis of the activity logs from use of OpenDSA tutorials by roughly 150 students over several weeks. We identified clusters of student use based on when they completed exercises, verified the reliability of estimated time requirements for exercises, provided evidence that a majority of students do not read the text, and found evidence that students complete additional exercises after obtaining credit. Furthermore, we determined that slideshow use was fairly high, but that skipping to the end of slideshows was common.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2544274",
        "year": "2014"
    },
    {
        "title": "Analysis of large data logs: an application of Poisson sampling on excite web queries",
        "abstract": "Search engines are the gateway for users to retrieve information from the Web. There is a crucial need for tools that allow effective analysis of search engine queries to provide a greater understanding of Web users' information seeking behavior. The objective of the study is to develop an effective strategy for the selection of samples from large-scale data sets. Millions of queries are submitted to Web search engines daily and new sampling techniques are required to bring these databases to a manageable size, while preserving the statistically representative characteristics of the entire data set. This paper reports results from a study using data logs from the Excite Web search engine. We use Poisson sampling to develop a sampling strategy, and show how sample sets selected by Poisson sampling statistically effectively represent the characteristics of the entire dataset. In addition, this paper discusses the use of Poisson sampling in continuous monitoring of stochastic processes, such as Web site dynamics.",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0306457301000437",
        "year": "2002"
    },
    {
        "title": "Analysis of learners' study logs: mouse trajectories to identify the occurrence of hesitation in solving word-reordering problems",
        "abstract": "In this paper, we describe a Web application we have been developing in order to help both teachers and learners notice the crucial aspects of solving word-reordering problems (WRPs). Also, we discuss ways to analyze the recorded mouse trajectories, response time, and drag and drop (D\u0026D) logs, because these records are potential indicators of the degree of learners' understanding.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2723645",
        "year": "2015"
    },
    {
        "title": "Analysis of Log Files Applying Mining Techniques and Fuzzy Logic",
        "abstract": "With the explosive growth of data available on the Internet, a recent area of investigation called Web Mining has arise. In this paper, we will study general aspects of this area, principally the process of Web Usage Mining where log files are analyzed. These files register the activity of the user when interact with the Web. In the Web Usage Mining, different techniques of mining to discover usage patterns from web data can be applied. We will also study applications of Fuzzy Logic in this area. Specially, we analyze fuzzy techniques such as fuzzy association rules or fuzzy clustering, featuring their functionality and advantages when examining a data set of logs from a web server. Finally, we give initial traces about the application of Fuzzy Logic to personalization and user profile construction.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-73325-6_48",
        "year": "2007"
    },
    {
        "title": "Analysis of long queries in a large scale search log",
        "abstract": "We propose to use the search log to study long queries, in order to understand the types of information needs that are behind them, and to design techniques to improve search effectiveness when they are used. Long queries arise in many different applications, such as CQA (community-based question answering) and literature search, and they have been studied to some extent using TREC data. They are also, however, quite common in web search, as can be seen by looking at the distribution of query lengths in a large scale search log.In this paper we analyze the long queries in the search log with the aim of identifying the characteristics of the most commonly occurring types of queries, and the issues involved with using them effectively in a search engine. In addition, we propose a simple yet effective method for evaluating the performance of the queries in the search log using a combination of the click data in the search log with the existing TREC corpora.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1507511",
        "year": "2009"
    },
    {
        "title": "Analysis of Multilingual Image Search Logs: Users‚Äô Behavior and Search Strategies",
        "abstract": "In this paper we summarize the analysis performed on the logs of multilingual image search provided by iCLEF09 and its comparison with the logs released in the iCLEF08 campaign. We have processed more than one million log lines in order to identify and characterize 5,243 individual search sessions. We focus on the analysis of users‚Äô behavior and their performance trying to find possible correlations between: a) the language skills of the users and the annotation language of the target images; and b) the final outcome of the search session. We have observed that the proposed task can be considered as easy, even though users with no competence in the annotation language of the images tend to perform more interactions and to use cross-language facilities more frequently. Usage of relevance feedback is remarkably low, but successful users use it more often.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-15751-6_3",
        "year": "2010"
    },
    {
        "title": "Analysis of Re-identification Risk Based on Log-Linear Models",
        "abstract": "The number of unique records in a released microdata set which are unique in the population is an important measure of re-identification disclosure risk in microdata. However, the microdata sample contains information about the disclosure risk more than the number of unique records. This paper deals with the development of a technique based on a loglinear models to extract more information from the sample about the disclosure risk not only through the number of sample unique records but also through the number of twin and triple records. These information may help microdata release committee in taking decision about releasing the data for public use. For illustration we apply the proposed method to data from a General Household Survey 1996‚Äì1997.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-25955-8_21",
        "year": "2004"
    },
    {
        "title": "Analysis of recovery in a database system using a write-ahead log protocol",
        "abstract": "In this paper we examine the recovery time in a database system using a Write-Ahead Log protocol, such as ARIES [9], under the assumption that the buffer replacement policy is strict LRU. In particular, analytical equations for log read time, data I/O, log application, and undo processing time are presented. Our initial model assumes a read/write ratio of one, and a uniform access pattern. This is later generalized to include different read/write ratios, as well as a ‚Äúhot set‚Äù model (i.e. x% of the accesses go to y% of the data). We show that in the uniform access model, recovery is dominated by data I/O costs, but under extreme hot-set conditions, this may no longer be true. Furthermore, since we derive anaytical equations, recovery can be analyzed for any set of parameter conditions not discussed here.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=130313",
        "year": "1992"
    },
    {
        "title": "Analysis of Social Networks Extracted from Log Files",
        "abstract": "Each chapter should be preceded by an abstract (10‚Äì15 lines long) that summarizes the content. The abstract will appear online at www.SpringerLink.com and be available with unrestricted access. This allows unregistered users to read the abstract as a teaser for the complete chapter. As a general rule the abstracts will not appear in the printed version of your book unless it is the style of your particular book or that of the series to which your book belongs. Please use the ‚Äústarred‚Äù version of the new Springer abstract command for typesetting the text of the online abstracts (cf. source file of this chapter template abstract) and include them with the source files of your manuscript. Use the plain abstract command if the abstract is also to appear in the printed version of the book.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4419-7142-5_6",
        "year": "2010"
    },
    {
        "title": "Analysis of steady-state segment storage utilizations in a log-structured file system with least-utilized segment cleaning",
        "abstract": "The steady-state distribution of storage utilizations of segments in a log-structured file system with least-utilized (greedy) segment cleaning is found using analytic methods. Furthermore, it is shown that as the number of segments increases, this distribution approaches a limiting continuous distribution, which is also derived. These results could be useful for preliminary performance analysis of LFS-type system designs prior to the development of detailed simulation models or actual implementation.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=240803",
        "year": "1996"
    },
    {
        "title": "Analysis of test log information through interactive visualizations",
        "abstract": "A fundamental activity to achieve software quality is software testing, whose results are typically stored in log files. These files contain the richest and more detailed source of information for developers trying to understand failures and identify their potential causes. Analyzing and understanding the information presented in log files, however, can be a complex task, depending on the amount of errors and the variety of information. Some previously proposed tools try to visualize test information, but they have limited interaction and present a single perspective of such data. This paper presents ASTRO, an infrastructure that extracts information from a number of log files and presents it in multi-perspective interactive visualizations that aim at easing and improving the developers' analysis process. A study carried out with practitioners from 3 software test factories indicated that ASTRO helps to analyze information of interest, with less accuracy in tasks that involved assimilation of information from different perspectives. Based on their difficulties, participants also provided feedback for improving the tool.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3196345",
        "year": "2018"
    },
    {
        "title": "Analysis of visitor's behavior from web log using web log expert tool",
        "abstract": "Web usage mining is a data mining technique. There are large amount of data are stored on the internet. When user search any particular information by search engine like Google, Bing etc. is very difficult because the complexity of web pages is increases day by day. Web usage mining plays an important role to solve this problem. In web usage mining we are creating a suitable pattern according to the user's visiting behavior. The goal of this paper is to implement a web log Expert tool on web server log file (an educational institution web log data) to find the behavioral pattern and profiles of users interacting with a web site. The web mining usage pattern of an Technical Institution web data. Web related data is coteries in to three parts namely web log, access log, error log and proxy log data and collect the data in web server and implemented a web log expert. Our experimental results help to predict and identify the number of visitor for the website and improve the website usability. The web related log data are three types, namely proxy log data, web log data, and error log data. We exploration the activity statistic by daily based hourly based week and monthly based report of web usage pattern. The web usage mining is playing an important role to improve the availability of information of your web site.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8212820/",
        "year": "2017"
    },
    {
        "title": "Analysis of Web Logs: Challenges and Findings",
        "abstract": "Web logs are an important source of information to describe and understand the traffic of the servers and its characteristics. The analysis of these logs is rather challenging because of the large volume of data and the complex relationships hidden in these data. Our investigation focuses on the analysis of the logs of two Web servers and identifies the main characteristics of their workload and the navigation profiles of crawlers and human users visiting the sites. The classification of these visitors has shown some interesting similarities and differences in term of traffic intensity and its temporal distribution. In general, crawlers tend to re-visit the sites rather often, even though they seldom send bursts of requests to reduce their impact on the servers resources. The other clients are also characterized by periodic patterns that can be effectively represented by few principal components.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-25575-5_19",
        "year": "2011"
    },
    {
        "title": "Analysis of Web Proxy Logs",
        "abstract": "Network forensics involves capturing, recording and analysing network audit trails. A crucial part of network forensics is to gather evidence at the server level, proxy level and from other sources. A web proxy relays URL requests from clients to a server. Analysing web proxy logs can give unobtrusive insights to the browsing behavior of computer users and provide an overview of the Internet usage in an organisation. More importantly, in terms of network forensics, it can aid in detecting anomalous browsing behavior. This paper demonstrates the use of a self-organising map (SOM), a powerful data mining technique, in network forensics. In particular, it focuses on how a SOM can be used to analyse data gathered at the web proxy level.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/0-387-36891-4_20",
        "year": "2006"
    },
    {
        "title": "Analyzing and evaluating query reformulation strategies in web search logs",
        "abstract": "Users frequently modify a previous search query in hope of retrieving better results. These modifications are called query reformulations or query refinements. Existing research has studied how web search engines can propose reformulations, but has given less attention to how people perform query reformulations. In this paper, we aim to better understand how web searchers refine queries and form a theoretical foundation for query reformulation. We study users' reformulation strategies in the context of the AOL query logs. We create a taxonomy of query refinement strategies and build a high precision rule-based classifier to detect each type of reformulation. Effectiveness of reformulations is measured using user click behavior. Most reformulation strategies result in some benefit to the user. Certain strategies like add/remove words, word substitution, acronym expansion, and spelling correction are more likely to cause clicks, especially on higher ranked results. In contrast, users often click the same result as their previous query or select no results when forming acronyms and reordering words. Perhaps the most surprising finding is that some reformulations are better suited to helping users when the current results are already fruitful, while other reformulations are more effective when the results are lacking. Our findings inform the design of applications that can assist searchers; examples are described in this paper.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1645966",
        "year": "2009"
    },
    {
        "title": "Analyzing developer sentiment in commit logs",
        "abstract": "The paper presents an analysis of developer commit logs for GitHub projects. In particular, developer sentiment in commits is analyzed across 28,466 projects within a seven year time frame. We use the Boa infrastructure's online query system to generate commit logs as well as files that were changed during the commit. We analyze the commits in three categories: large, medium, and small based on the number of commits using a sentiment analysis tool. In addition, we also group the data based on the day of week the commit was made and map the sentiment to the file change history to determine if there was any correlation. Although a majority of the sentiment was neutral, the negative sentiment was about 10% more than the positive sentiment overall. Tuesdays seem to have the most negative sentiment overall. In addition, we do find a strong correlation between the number of files changed and the sentiment expressed by the commits the files were part of. Future work and implications of these results are discussed.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2903501",
        "year": "2016"
    },
    {
        "title": "Analyzing Malware Log Data to Support Security Information and Event Management: Some Research Results",
        "abstract": "Enterprise information infrastructures are generally characterized by a multitude of information systems which support decision makers in fulfilling their duties. The object of information security management is the protection of these systems, whereas security information and event management (SIEM) addresses those information management tasks which focus on the short term handling of events, as well as on the long term improvement of the entire information security architectures. This is carried out based on those data which can be logged and collected within the enterprise information security infrastructure. An especially interesting type of log data is data created by anti-malware software. This paper demonstrates in the context of a project case study that data mining (DM) is a well suited approach to detect hidden patterns in malware data and thus to support SIEM.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/5071820/",
        "year": "2009"
    },
    {
        "title": "Analyzing the log patterns of adult learners in LMS using learning analytics",
        "abstract": "In this paper, we describe a process of constructing proxy variables that represent adult learners' time management strategies in an online course. Based upon previous research, three values were selected from a data set. According to the result of empirical validation, an (ir)regularity of the learning interval was proven to be correlative with and predict learning performance. As indicated in previous research, regularity of learning is a strong indicator to explain learners' consistent endeavors. This study demonstrates the possibility of using learning analytics to address a learner's specific competence on the basis of a theoretical background. Implications for the learning analytics field seeking a pedagogical theory-driven approach are discussed.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2567616",
        "year": "2014"
    },
    {
        "title": "Anomaly detection algorithms in logs of process aware systems",
        "abstract": "Today, there is a variety of systems that support business process as a whole or partially. However, normative systems are not appropriate for some domains (e.g. health care) as a flexible support is needed to the participants. On the other hand, while it is important to support flexibility in these systems, security requirements can not be met whether these systems do not offer extra control. This paper presents and assesses two anomaly detection algorithms in logs of Process Aware Systems (PAS). The detection of an anomalous trace is based on the \"noise\" which a trace makes in a process model discovered by a process mining algorithm. This paper argues that these methods can support the coexistence of security and flexibility when aggregated to a PAS.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1363904",
        "year": "2008"
    },
    {
        "title": "Anomaly Detection from Network Logs Using Diffusion Maps",
        "abstract": "The goal of this study is to detect anomalous queries from network logs using a dimensionality reduction framework. The fequencies of 2-grams in queries are extracted to a feature matrix. Dimensionality reduction is done by applying diffusion maps. The method is adaptive and thus does not need training before analysis. We tested the method with data that includes normal and intrusive traffic to a web server. This approach finds all intrusions in the dataset.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-23957-1_20",
        "year": "2011"
    },
    {
        "title": "Anvaya: An Algorithm and Case-Study on Improving the Goodness of Software Process Models Generated by Mining Event-Log Data in Issue Tracking Systems",
        "abstract": "Issue Tracking Systems (ITS) such as Bugzilla can be viewed as Process Aware Information Systems (PAIS) generating event-logs during the life-cycle of a bug report. Process Mining consists of mining event logs generated from PAIS for process model discovery, conformance and enhancement. We apply process map discovery techniques to mine event trace data generated from ITS of open source Firefox browser project to generate and study process models. Bug life-cycle consists of diversity and variance. Therefore, the process models generated from the event-logs are spaghetti-like with large number of edges, inter-connections and nodes. Such models are complex to analyse and difficult to comprehend by a process analyst. We improve the Goodness (fitness and structural complexity) of the process models by splitting the event-log into homogeneous subsets by clustering structurally similar traces. We adapt the K-Medoid clustering algorithm with two different distance metrics: Longest Common Subsequence (LCS) and Dynamic Time Warping (DTW). We evaluate the goodness of the process models generated from the clusters using complexity and fitness metrics. We study back-forth and self-loops, bug reopening, and bottleneck in the clusters obtained and show that clustering enables better analysis. We also propose an algorithm to automate the clustering process-the algorithm takes as input the event log and returns the best cluster set.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7551993/",
        "year": "2016"
    },
    {
        "title": "Applicability of Motion Estimation Algorithms for an Automatic Detection of Spiral Grain in CT Cross-Section Images of Logs",
        "abstract": "Techniques for an automatic detection of spiral grain in cross-section CT images of logs are proposed and evaluated on sets of natural and artificial cross-section images. Explicit analysis of global rotation, block matching, and optical flow techniques are compared. Experimental results seem to indicate that spiral grain in fact cannot be modeled by a circular motion of luminance values in gray scale images.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-74272-2_5",
        "year": "2007"
    },
    {
        "title": "Application of automatic topic identification on excite web search engine data logs",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0306457304000482",
        "year": "2005"
    },
    {
        "title": "Application of Multi-Scale Fractal Feature in Defects Detection of Log X-Ray Image",
        "abstract": "The aim of this study is to extract the internal defects information from a log x-ray image and accurately identify the types of the defects in log by digital image processing technology and fractal theory. A method in log x-ray image defects detection based on multi-scale fractal feature (DMF) was applied in this paper. The DMF values of different regions in a log image are normally different. According to the values of DMF, the internal defects in log can be detected and classified. This method has been applied to the log image with crack and knots, the experimental result show that the DMF values of the normal regions are between 0.020 and 0.060; the DMF values of the crack regions are between 0.300 and 0.600; and the DMF values of the knot regions are between 0.100 and 0.200. This study provides a new tool for analyzing and processing log x-ray image.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5209183/",
        "year": "2009"
    },
    {
        "title": "Application of tree-structured data mining for analysis of process logs in XML format",
        "abstract": "Process logs are increasingly being represented using XML based templates such as MXML and XES. Popular XML data mining techniques have had limited application to directly mine such data. The majority of work in the process mining field focuses on process discovery and conformance checking tasks often utilizing visualization and simulation based techniques. In this paper, an approach is proposed within which a wider range of data mining methods can be directly applied on tree-structured process log data. Clustering, classification and frequent pattern mining are used as a case in point and experiments are performed on publicly available real-world and synthetic data. The results indicate the great potential of the proposed approach in adding to the available set of methods for process log analysis. It presents an alternative where process model discovery is not the pre-requisite and a variety of methods can be directly applied.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2525387",
        "year": "2012"
    },
    {
        "title": "Application of well log analysis for source rock evaluation in the Duwi Formation, Southern Gulf of Suez, Egypt",
        "abstract": "",
        "include": false,
        "url": "https://www.sciencedirect.com/science/article/pii/S0926985111002862",
        "year": "2012"
    },
    {
        "title": "Applying Hadoop for log analysis toward distributed IDS",
        "abstract": "In this paper, we apply Hadoop for large-scale log analysis. Our main objective is to efficiently detect an abnormal traffic from high volume data. Due to the high volume of data traffics, the size of traffic logs is usually exceed the capacity of a standalone IDS. Thus, it is practically impossible to perform useful analysis with these data. In most cases, an analysis is usually done when an attack occurred for digital forensics.We proposed applying K-Means algorithm to cluster high volume log data. The resulted clusters are useful in classifying minority as possible intruders. In addition, we proposed IP address summarization method to capture the characteristic of each cluster.Our implementation allows high volume data traffics to be analyzed with a distributed analysis system using K-Means algorithm and data mining. The eventual result is to reduce a chance of being attacked. The prominent points of our implementation are anomaly detection with large file sizes and the distributed processing.However, this paper is just a preliminary study. There exist several opportunities for optimization. Nonetheless, our implementation can point out anomaly. The K-Means Algorithm can provide a new knowledge useful for enhancing security of the system.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=2448559",
        "year": "2013"
    },
    {
        "title": "Applying high-performance bioinformatics tools for outlier detection in log data",
        "abstract": "Most of today's security solutions, such as security information and event management (SIEM) and signature based IDS, require the operator to evaluate potential attack vectors and update detection signatures and rules in a timely manner. However, today's sophisticated and tailored advanced persistent threats (APT), malware, ransomware and rootkits, can be so complex and diverse, and often use zero day exploits, that a pure signature-based blacklisting approach would not be sufficient to detect them. Therefore, we could observe a major paradigm shift towards anomaly-based detection mechanisms, which try to establish a system behavior baseline - either based on netflow data or system logging data - and report any deviations from this baseline. While these approaches look promising, they usually suffer from scalability issues. As the amount of log data generated during IT operations is exponentially growing, high-performance analysis methods are required that can handle this huge amount of data in real-time. In this paper, we demonstrate how high-performance bioinformatics tools can be applied to tackle this issue. We investigate their application to log data for outlier detection to timely reveal anomalous system behavior that points to cyber attacks. Finally, we assess the detection capability and run-time performance of the proposed approach.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7985760/",
        "year": "2017"
    },
    {
        "title": "Approximation to Probability of Detection for Log-Normal Target Fluctuation",
        "abstract": "A relatively simple expression for the approximation to the probability of detection of a log-normally fluctuating target is provided. Absolute errors normally have a magnitude less than 0.01.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4102117/",
        "year": "1979"
    },
    {
        "title": "Assessing the scenic route: measuring the value of search trails in web logs",
        "abstract": "Search trails mined from browser or toolbar logs comprise queries and the post-query pages that users visit. Implicit endorsements from many trails can be useful for search result ranking, where the presence of a page on a trail increases its query relevance. Follow-ing a search trail requires user effort, yet little is known about the benefit that users obtain from this activity versus, say, sticking with the clicked search result or jumping directly to the destination page at the end of the trail. In this paper, we present a log-based study estimating the user value of trail following. We compare the relevance, topic coverage, topic diversity, novelty, and utility of full trails over that provided by sub-trails, trail origins (landing pages), and trail destinations (pages where trails end). Our findings demonstrate significant value to users in following trails, especially for certain query types. The findings have implications for the design of search systems, including trail recommendation systems that display trails on search result pages.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1835548",
        "year": "2010"
    },
    {
        "title": "Assessment of water logged cables by very low frequency tan delta testing technique",
        "abstract": "Distribution cables rated 22 kV which were submerged under water for more than 45 days due to flash floods in one of the processing industry were assessed using VLF technique in addition to other techniques like insulation resistance and dc voltage withstand test. Special technique used for extraction of water from the cable termination is discussed. The consequences of bad crimping of cable lugs and improper cable terminations on the penetration of water into the cable length are highlighted. The study showed abnormal dielectric losses in the cable insulation as a result of highly polar contaminants in the cable. The precaution to prevent water entry into the cable are suggested in this paper.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/6318956/",
        "year": "2012"
    },
    {
        "title": "Association Rule Retrieved from Web Log Based on Rough Set Theory",
        "abstract": "With the development of Internet, data mining in Web log has been a hot spot of research work. Many data mining methods based on association rule have been proposed. In this paper, a novel association rule prediction model based on rough set theory is advanced, it can deal with no-contiguous access sequence and get explicit expression of rules without prior knowledge, the result of experiment prove that it can achieve more prediction accuracy than existing methods.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/4406215/",
        "year": "2007"
    },
    {
        "title": "Asymptotic Log-Loss of Prequential Maximum Likelihood Codes",
        "abstract": "We analyze the Dawid-Rissanen prequential maximum likelihood codes relative to one-parameter exponential family models ÓàπM. If data are i.i.d. according to an (essentially) arbitraryP, then the redundancy grows at rate 12clnùëõ12clnn. We show that c = œÉ 2112/ œÉ 2222, where œÉ 2112 is the variance of P, and œÉ 2222 is the variance of the distribution ùëÄ‚àó‚ààÓàπM‚àó‚ààM that is closest to P in KL divergence. This shows that prequential codes behave quite differently from other important universal codes such as the 2-part MDL, Shtarkov and Bayes codes, for which c = 1. This behavior is undesirable in an MDL model selection setting.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11503415_44",
        "year": "2005"
    },
    {
        "title": "Attack pattern mining algorithm based on security log",
        "abstract": "This paper proposes an attack pattern mining algorithm to extract attack pattern in massive security logs. The improved fuzzy clustering algorithm is used to generate sequence set. Then PrefixSpan is used to mine frequent sequence from the sequence set. The experimental results show that this algorithm can effectively mine the attack pattern, improve the accuracy and generate more valuable attack pattern.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8004918/",
        "year": "2017"
    },
    {
        "title": "Automated Detection of Epidemics from the Usage Logs of a Physicians‚Äô Reference Database",
        "abstract": "Epidemics of infectious diseases are usually recognized by an observation of an abnormal cluster of cases. Usually, the recognition is not automated, and relies on the alertness of human health care workers. This can lead to significant delays in detection. Since real-time data from the physicians‚Äô offices is not available. However, in Finland a Web-based collection of guidelines for primary care exists, and increases in queries concerning certain disease have been shown to correlate to epidemics. We introduce a simple method for automated online mining of probable epidemics from the log of this database. The method is based on deriving a smoothed time series from the data, on using a flexible selection of data for comparison, and on applying randomization statistics to estimate the significance of findings. Experimental results on simulated and real data show that the method can provide accurate and early detection of epidemics.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-39804-2_18",
        "year": "2003"
    },
    {
        "title": "Automated Log Analysis of Infected Windows OS Using Mechanized Reasoning",
        "abstract": "Malware (Malicious Software) of Windows OS has become more sophisticated. To take some countermeasures for recent infection, more intelligent and automated system log analysis is necessary. In this paper we propose an automated log analysis of infected Windows OS using mechanized reasoning. We apply automated deduction system for gathering events of malware and extract the behavior of infection over large scale system logs. In experiment, we cope with four kinds of resolution strategies to detect the malicious behavior. It is shown that automation of analyzing system logs is possible for detecting actual malicious software.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/978-3-642-10684-2_60",
        "year": "2009"
    },
    {
        "title": "Automated Performance Model Construction through Event Log Analysis",
        "abstract": "Locating performance bottlenecks in modern distributed systems can be difficult because direct measurement is often not available. Performance models are often very beneficial in these situations because far more information can be extracted as a result. This work demonstrates the generation of a Layered Queueing Network performance model through the analysis of trace information from a live system. The model can then be analyzed to locate bottlenecks in both the hardware and software.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/6200164/",
        "year": "2012"
    },
    {
        "title": "Automatic Irrigation System with Data Log Creation",
        "abstract": "Water being a crucial resource should be judiciously used, especially when it comes to agriculture. It has been seen that right amount of water helps the farmer to have a good yield and the quality is also better. The paper proposes a technique to automate the irrigation system using Arduino Mega 2560 along with GSM 800L which is controlled through a mobile application module and create a log of the sensor data collected from the field on a webpage.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8473250/",
        "year": "2018"
    },
    {
        "title": "Automatic labeling of software components and their evolution using log-likelihood ratio of word frequencies in source code",
        "abstract": "As more and more open-source software components become available on the Internet we need automatic ways to label and compare them. For example, a developer who searches for reusable software must be able to quickly gain an understanding of retrieved components. This understanding cannot be gained at the level of source code due to the semantic gap between source code and the domain model. In this paper we present a lexical approach that uses the log-likelihood ratios of word frequencies to automatically provide labels for software components. We present a prototype implementation of our labeling/comparison algorithm and provide examples of its application. In particular, we apply the approach to detect trends in the evolution of a software system.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5069499/",
        "year": "2009"
    },
    {
        "title": "Automatic logging of operating system effects to guide application-level architecture simulation",
        "abstract": "Modern architecture research relies heavily on application-level detailed pipeline simulation. A time consuming part of building a simulator is correctly emulating the operating system effects, which is required even if the goal is to simulate just the application code, in order to achieve functional correctness of the application's execution. Existing application-level simulators require manually hand coding the emulation of each and every possible system effect (e.g., system call, interrupt, DMA transfer) that can impact the application's execution. Developing such an emulator for a given operating system is a tedious exercise, and it can also be costly to maintain it to support newer versions of that operating system. Furthermore, porting the emulator to a completely different operating system might involve building it all together from scratch.In this paper, we describe a tool that can automatically log operating system effects to guide architecture simulation of application code. The benefits of our approach are: (a) we do not have to build or maintain any infrastructure for emulating the operating system effects, (b) we can support simulation of more complex applications on our application-level simulator, including those applications that use asynchronous interrupts, DMA transfers, etc., and (c) using the system effects logs collected by our tool, we can deterministically re-execute the application to guide architecture simulation that has reproducible results.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=1140303",
        "year": "2006"
    },
    {
        "title": "Automatic Segmentation of Wood Logs by Combining Detection and Segmentation",
        "abstract": "The segmentation of cut surfaces from a stack of wood logs is a challenging task and leads to many problems. Wood logs theoretically have a certain shape and color, which is the main reason to apply object detection methods. But in real world images there are many disturbing factors, such as defects, dirt or non-elliptical logs. In this paper we mainly address the problem of wood and wood log segmentation by combining object detection with a graph-cut segmentation. We introduce an iterative segmentation procedure, which detects the stack of wood, segments foreground and background, and separates the logs. Our novel approach works fully automatically and has no restrictions on the image acquisition other than well visible log cut surfaces. All three steps of our approach are novel and could be applied on similar problems. We implemented and evaluated different methods and show that of these approaches, our methods leads to the best results.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-33179-4_25",
        "year": "2012"
    },
    {
        "title": "Automatic Task Detection in the Web Logs and Analysis of Multitasking",
        "abstract": "In this paper, we describe the conceptual basis and results of the Web search task detection study with emphasis on multitasking. The basis includes: logical structure of a search process, a space of physical realizations, mapping of a logical structure into the space of realizations. Questions on the users‚Äô manners of search realization are formulated, with emphasis on multiple tasks execution. An automatic analysis of the Web logs shows that multitasking is rare, usually it includes only two task sessions and is formed into a temporal inclusion of an interrupting task session into the interrupted one. Searchers follow the principle of least effort and select the cheapest tactics: sequential tasks execution as a rule or, in the rare case of multitasking, the least expensive form of it. Quantitative characteristics of search behavior in 3 classes of temporal sessions (1-task session, several tasks executed one-by-one, and multitasking session) were compared, and significant differences were revealed.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/11931584_16",
        "year": "2006"
    },
    {
        "title": "AWide Area Log Analyzing System Based on Mobile Agents",
        "abstract": "The Internet is being widely used these days and many users are required to manage their network environments, because damages caused by worms, which spread using security holes of software, are also increasing rapidly. One of the effective means of detecting the damages caused by the worms in early stage is to analyze the network communication logs stored in computers that are spread over a wide area. However, almost all network administrators are not able to install many observation points, though a large number of observation points over a wide area of a network are needed to grasp symptoms of attacks precisely. In this paper, we propose an agent based log analyzing system by integrating the concepts of P2P network and mobile agents to realize detection and protection from the damages which may be caused by the worms in early stage. We also show results of experiments using our prototype system. The results show that our system can collect useful information from a wide area of a network, and provide means of flexible and on-demand analysis of network traffic logs to detect hostile attacks on the network.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4052673/",
        "year": "2006"
    },
    {
        "title": "Axial mode-matching technique for analysis of directional well-logging sensor tools",
        "abstract": "We present an new axial mode-matching technique to analyze directional well-logging tools used for oil and gas exploration. The geometry of this problem consists of a number of coil antennas (each with arbitrary relative tilt with the respect to the axis of symmetry) in a cylindrically layered medium comprised of a metallic mandrel, a borehole, an invasion zone, and a surrouding layered Earth formation. The proposed technique is based on a judicious eigenfunction expansion for the fields in cylindrical coordinates followed by a derivation the generalized scattering matrix (GSM) at each axial discontinuity based on the mode matching technique. Particular tilted coil excitations are represented by a set of modal coefficients, combined with the GSM matrices, and integrated over the receiver coils to compute the transimpedances. We present a series of validation results which show that our technique can efficiently and accurately model directional logging tools.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/7696727/",
        "year": "2016"
    },
    {
        "title": "Axiomatic Analysis for Improving the Log-Logistic Feedback Model",
        "abstract": "Pseudo-relevance feedback (PRF) has been proven to be an effective query expansion strategy to improve retrieval performance. Several PRF methods have so far been proposed for many retrieval models. Recent theoretical studies of PRF methods show that most of the PRF methods do not satisfy all necessary constraints. Among all, the log-logistic model has been shown to be an effective method that satisfies most of the PRF constraints. In this paper, we first introduce two new PRF constraints. We further analyze the log-logistic feedback model and show that it does not satisfy these two constraints as well as the previously proposed \"relevance effect\" constraint. We then modify the log-logistic formulation to satisfy all these constraints. Experiments on three TREC newswire and web collections demonstrate that the proposed modification significantly outperforms the original log-logistic model, in all collections.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=2914768",
        "year": "2016"
    },
    {
        "title": "Backhoe, a Packet Trace and Log Browser",
        "abstract": "We present Backhoe, a tool for browsing packet trace or other event logs that makes it easy to spot ‚Äústatistical novelties‚Äù in the traffic, i.e. changes in the character of frequency distributions of feature values and in mutual relationships between pairs of features. Our visualization uses feature entropy and mutual information displays as either the top-level summary of the dataset or alongside the data. Our tool makes it easy to switch between absolute and conditional metrics, and observe their variations at a glance. We successfully used Backhoe for analysis of proprietary protocols.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-85933-8_15",
        "year": "2008"
    },
    {
        "title": "Beam position monitoring in the 100-MHz to 500-MHz frequency range using the log-ratio technique",
        "abstract": "A logarithmic-ratio beam position monitor (BPM) circuit has been designed that operates directly from radio frequency signals in the 100-MHz to 500-MHz frequency range. The circuit uses four logarithmic amplifiers, a pair for each channel. One amplifier per channel receives its signal input directly from a BPM electrode, while the second amplifier receives the same signal attenuated by 7-dB. The two outputs of each channel are summed together and the composite video outputs are applied to a differencing amplifier. The net result is the logarithmic-ratio position measurement derived from the two input RF signals. Paralleling the pairs of outputs from the amplifiers provides measurement accuracy that is comparable to other circuit techniques used for position measurement.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/309307/",
        "year": "1993"
    },
    {
        "title": "Beat story: life-log system of subjective time using heart beat rate",
        "abstract": "No abstract available.",
        "include": false,
        "url": "https://dl.acm.org/citation.cfm?id=1401015",
        "year": "2008"
    },
    {
        "title": "Behavior mining of female students by analyzing log files",
        "abstract": "In this paper, we attempt to get better insight on how Internet usage behaviors of female students in university can affect on their different academic activities also compare usage patterns of different departments' students with their teachers' patterns. In addition, we attempt to find similarities and dissimilarities of usage patterns of students on various branches and finding relationships between Internet usage pattern of female students and their academic performance CPI (Cumulative Performance Index). The other objective of this investigation is to identify factors for usage behaviors of Internet in terms of average time spent per day on Internet, number of hits(in aspects of opened or visited pages) and number of unique Websites visited per day by students of different departments or branches. This paper presents the results of a study conducted at Motilal Nehru National Institute of Technology, Allahabad (India) for a period of one year, regarding behavior mining of female students related to their Internet usage patterns with analyzing access log files. Results of this paper can help academic researchers and course coordinators for improvement of logistics and teaching methods with respect to Internet usage behavior related to branch and gender differentiation.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/5662243/",
        "year": "2010"
    },
    {
        "title": "Behavioral Sequences: A New Log-Coding Scheme for Effective Prediction of Web User Accesses",
        "abstract": "Mining web site logs for predicting user actions is a central issue in the field of adaptive web site development. In order to match the dynamic nature of today web sites we propose in this paper a new scheme for coding Web server log data into sessions of behavioral sequences. Following the proposed coding scheme the navigation sessions are coded as a sequence of hypothetical actions that may explain the transition from one page to another. The output of a prediction algorithm will now be an action that can be evaluated in the context of the current navigation in order to find pages that to be visited by the user.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/3-540-47952-X_48",
        "year": "2002"
    },
    {
        "title": "Beyond Click Graph: Topic Modeling for Search Engine Query Log Analysis",
        "abstract": "Search engine query log is a valuable information source to analyze the users‚Äô interests and preferences. In existing work, click graph is intensively utilized to analyze the information in query log. However, click graph is usually plagued by low information coverage, failure of capturing the diverse types of co-occurrence and the incapability of discovering the latent semantics in data. In this paper, we go beyond click graph and analyze query log through the new perspective of probabilistic topic modeling. In order to systematically explore the potential assumptions of the latent structure of the log data, we propose three different topic models. The first model, the Meta-word Model (MWM), unifies the co-occurrence of query terms and URLs by the meta-word occurrence. The second model, the Term-URL Model (TUM), captures the characteristics of query terms and URLs separately. The third model, the Clickthrough Model (CTM), captures the clicking behavior explicitly and models the ternary relation between search queries, query terms and URLs. We evaluate the three proposed models against several strong baselines on a real-life query log. The experimental results show that the proposed models demonstrate significantly improved performance with respect to different quantitative metrics and also in applications such as date prediction, community discovery and URL annotation.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-37487-6_18",
        "year": "2013"
    },
    {
        "title": "Big Log Data Stream Processing: Adapting an Anomaly Detection Technique",
        "abstract": "With the continuous increase in data velocity and volume nowadays, preserving system and data security is particularly affected. In order to handle the huge amount of data and to discover security incidents in real-time, analyses of log data streams are required. However, most of the log anomaly detection techniques fall short in considering continuous data processing. Thus, this paper aligns an anomaly detection technique for data stream processing. It thereby provides a conceptual basis for future adaption of other techniques and further delivers proof of concept by prototype implementation.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-98812-2_12",
        "year": "2018"
    },
    {
        "title": "Bilateral anonymity and prevention of abusing logged Web addresses",
        "abstract": "A lot of effort has been taken to hide the content of a message from eavesdroppers. However, often not only the content, but also the address and identity of sender and/or receiver of the message are of interest for attackers. For that reason, several approaches were developed to guarantee anonymity in the case of email. A lot of services offer users to access Web pages unrecognised or without the risk of being backtracked, respectively. This kind of anonymity is called user or \"client anonymity\". However, there are only a few offers that provide an equivalent protection for content providers, although this feature is desirable for many situations in which the identity of a publisher or content provider is to be hidden. This property is called server anonymity. The term \"server anonymity\" is explained in detail with the help of an existing system fulfilling some hundreds of thousand user requests per day. We also describe our experiences in providing such a system with respect to misuse. Furthermore there is another sensitive fact. While browsing Web pages, the used URLs are logged both by the Web client (Web browser) which is used and the Internet service provider (ISP), or any other instance or organisation that is involved in the communication. Hence the ISP can investigate the content a user is interested in afterwards simply by reusing the logged URLs. The same problem results from the behaviour of regular Web browsers to build an address history and local copies (browser cache) of the visited Web pages. We demonstrate a way of preventing the reuse of logged Web addresses by introducing the concept of temporarily valid Web addresses.",
        "include": true,
        "url": "https://ieeexplore.ieee.org/document/904990/",
        "year": "2000"
    },
    {
        "title": "Brain Controlled Interface Log Analysis in Real Time Strategy Game Matches",
        "abstract": "Emotions are an important aspect that affects human interaction with systems and applications. The correlation of emotional and affective state with game interaction data is a relevant issue since it can explain player behavior and the outcome of a digital game match. In this work, we present an initial exploratory study to analyze interaction log data and its correlation with an off-the-shelf Brain Controlled Interface (BCI) that collected excitement in a RTS (Real Time Strategy game). Our results shown moderate correlations with player‚Äôs preferences and amount of interactions. Additionally, we also found in the interaction and game logs that character‚Äôs choice significantly impacts the time spent in data-driven levels of excitement. We did not find statistically significant differences of excitement for other factors such as player ranking and game style, map, and opponent character.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-92049-8_19",
        "year": "2018"
    },
    {
        "title": "Browsing Behavior Analysis from Wi-Fi Logs Based on Community Detection: Case Study on Educational Institution",
        "abstract": "As the growth of technology, the need of internet increases. Wireless internet technology or called Wi-Fi is dominate. Hence, monitoring the usage of Wi-Fi is crucial. Community detection is a technique which can be used to understand browsing behavior analysis. Using this approach, a website is represented as a node in a graph and the transition over websites is represented as an edge where the weight of an edge is calculated based on the frequency of website transition. In this study, we implement two community detection algorithms which support directed and weighted graph, i.e Girvan-Newman and Infomap to know the communities of websites accessed by Wi-Fi users. An evaluation metrics named modularity is employed to access the quality of both algorithms. The results of our experiment show that Infomap performs better to detect communities than Girvan-Newman. Based on browsing behavior analysis, it can be understood that the Wi-Fi network is mostly used for education, entertainment and government purposes. However, we also obtain insight regarding the misbehavior activities which need prevention.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/8471720/",
        "year": "2018"
    },
    {
        "title": "Bug localization using revision log analysis and open bug repository text categorization",
        "abstract": "In this paper, we present a new approach to localize a bug in the software source file hierarchy. The proposed approach uses log files of the revision control system and bug reports information in open bug repository of open source projects to train a Support Vector Machine (SVM) classifier. Our approach employs textual information in summary and description of bugs reported to the bug repository, in order to form machine learning features. The class labels are revision paths of fixed issues, as recorded in the log file of the revision control system. Given an unseen bug instance, the trained classifier can predict which part of the software source file hierarchy (revision path) is more likely to be related to this issue. Experimental results on more than 2000 bug reports of ‚ÄòUI‚Äôcomponent of the Eclipse JDT project from the initiation date of the project until November 24, 2009 (about 8 years) using this approach, show weighted precision and recall values of about 98% on average.",
        "include": true,
        "url": "https://link.springer.com/10.1007/978-3-642-13244-5_15",
        "year": "2010"
    },
    {
        "title": "Building and exploiting ad hoc concept hierarchies for web log analysis",
        "abstract": "Web usage mining aims at the discovery of interesting usage patterns from Web server log files. ‚ÄúInterestingness‚Äù relates to the business goals of the site owner. However, business goals refer to business objects rather than the page hits and script invocations recorded by the site server. Hence, Web usage analysis requires a preparatory mechanism that incorporates the business goals, the concepts reflecting them and the expert‚Äôs background knowledge on them into the mining process. To this purpose, we present a methodology and a mechanism for the establishment and exploitation of application-oriented concept hierarchies in Web usage analysis. We demonstrate our approach on a real data set and show how it can substantially improve both the search for interesting patterns by the mining algorithm and the interpretation of the mining results by the analyst.",
        "include": true,
        "url": "https://link.springer.com/chapter/10.1007/3-540-46145-0_9",
        "year": "2002"
    },
    {
        "title": "Business Monitoring Framework for Process Discovery with Real-Life Logs",
        "abstract": "Business analysis with processes extracted from real-life system logs has recently become important for improving business performance. Since business users desire to see the current situations of business with visualized process models from various perspective, we need an analysis platform that supports changes of viewpoint. We have developed a runtime monitoring framework for log analysis. Our framework can simultaneously extract process instances and derive appropriate metrics in a single pass through the logs. We tested our proposed framework with a real-life system log. The results for twenty days of data show synthesized process models along with an analysis axis. They were synthesized from the metric-annotated process instances generated by our framework.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-319-10172-9_30",
        "year": "2014"
    },
    {
        "title": "Business Process Event Logs and Visualization",
        "abstract": "Business process audit trail; Business process execution log; Business process execution trail; Workflow audit trail; Workflow log",
        "include": false,
        "url": "http://link.springer.com/referenceworkentry/10.1007/978-3-319-63962-8_86-1",
        "year": "2018"
    },
    {
        "title": "Business Process Quality Metrics: Log-Based Complexity of Workflow Patterns",
        "abstract": "We believe that analysis tools for BPM should provide other analytical capabilities besides verification. Namely, they should provide mechanisms to analyze the complexity of workflows. High complexity in workflows may result in poor understandability, errors, defects, and exceptions leading processes to need more time to develop, test, and maintain. Therefore, excessive complexity should be avoided. The major goal of this paper is to describe a quality metric to analyze the complexity of workflow patterns from a log-based perspective.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-540-76848-7_30",
        "year": "2007"
    },
    {
        "title": "Calculating Costs by Analyzing Apache Web Logs",
        "abstract": "At the time of this writing, the Web is less than 20 years old. While that may seem like a lifetime in software terms, in business terms, the Web is still an infant, with countless new ventures being built around it.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-1-4302-0532-6_10",
        "year": "2008"
    },
    {
        "title": "Calculating Website‚Äôs Usability Metrics Using Log File Information",
        "abstract": "Log files are interesting.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-3-642-39473-7_20",
        "year": "2013"
    },
    {
        "title": "Calculation of probability of detection for log-normal target fluctuations",
        "abstract": "An accurate, simple, and reliable method for the computation of the probability of detection for a target whose radar cross section is constant within a scan but fluctuates log-normally scan-to-scan is presented.\u003c \u003e",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/68161/",
        "year": "1991"
    },
    {
        "title": "Calculation of the radiation pattern and impedance of a log-periodic dipole array",
        "abstract": "A computer program is presented for the calculation of the radiation and impedance performance of a 12-element log-periodic dipole array. The analysis uses King's 3-term approximation to the current distribution on the elements, taking their mutual interaction into full account.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/4236344/",
        "year": "1973"
    },
    {
        "title": "Capturing the Semantics of Web Log Data by Navigation Matrices",
        "abstract": "The information left behind by users who have visited a web site is recorded in the related web server log files. From analysing the data contained in such files, a web designer is able to understand the interaction between the users and a web site, and then to improve the web topology. We assume that the information of web usage can be generated from log files via a cleaning process, from which we identify a set of navigation sessions that represent the trails formed by users during the navigation process. The trails are modelled as a weighted directed graph, called a transition graph, and then a corresponding navigation matrix is computed with respect to the underlying web topology. The main contribution in this paper is that we formally define a minimal set of binary operators on navigation matrices, which consists of the sum, union, intersection and difference operators. These operations afford us the ability to analyse users navigation from the contents of two given navigation matrices.",
        "include": false,
        "url": "http://link.springer.com/chapter/10.1007/978-0-387-35658-7_10",
        "year": "2003"
    },
    {
        "title": "Causal Connections Mining Within Security Event Logs",
        "abstract": "Performing both security vulnerability assessment and configuration processes are heavily reliant on expert knowledge. This requirement often results in many systems being left insecure due to a lack of analysis expertise and access to specialist resources. It has long been known that a system's event log provides historical information depicting potential security threats, as well as recording configuration activities. In this paper, a novel technique is developed that can process security event logs on a computer that has been assessed and configured by a security professional, and autonomously establish causality amongst event log entries to learn performed configuration tasks. This extracted knowledge can then be exploited by non-professionals to plan steps that can improve the security of a previously unseen system.",
        "include": true,
        "url": "https://dl.acm.org/citation.cfm?id=3154476",
        "year": "2017"
    },
    {
        "title": "CFAR detection for Weibull and other log-log-linear tail clutter distributions",
        "abstract": "The problem of CFAR (constant false alarm rate) radar target detection in non-Rayleigh, two-parameter, unknown clutter is already a classical one. The authors describe and evaluate a practical way of solving the already old problem of target detection in two-parameter clutter. By reformulating some basic tail extrapolation theory results, they fully develop a complete detection architecture for Weibull clutter. The method can be directly applied to deal with any other type of clutter of the log-log-linear tail family.",
        "include": false,
        "url": "https://ieeexplore.ieee.org/document/591255/",
        "year": "1997"
    },
    {
        "title": "Change Detection in Event Logs by Clustering",
        "abstract": "The detection of changes in event logs recording the behavior of flexible processes is especially challenging and process mining algorithms generate useless ‚Äúspaghetti‚Äù models out of them. Due to this, existing approaches for change detection in event logs recording the behavior of flexible processes can only localize a change point, which is of no avail when it comes to explain when, why and how a process model changed and will change. The aim of this paper is to present a novel clustering technique laying the foundation to determine a variety of changes and to foresee changes. In order to do this, four algorithms have been developed. We report the results of evaluations on synthetic as well as real-life data demonstrating the efficiency of the approach and also its broad scope of application for event and sensor data.",
        "include": true,
        "url": "http://link.springer.com/chapter/10.1007/978-3-030-02610-3_36",
        "year": "2018"
    }
]